# üöÄ Quick Start - Ollama com Modelos Pr√©-instalados

## ‚ö° In√≠cio R√°pido (3 passos)

### 1Ô∏è‚É£ Build da imagem (primeira vez apenas)

```powershell
# Op√ß√£o A: Script interativo (recomendado)
.\manage-ollama.ps1

# Op√ß√£o B: Manual com GPU
docker-compose build ollama

# Op√ß√£o C: Manual CPU-only
docker-compose -f docker-compose-cpu.yaml build ollama
```

**‚è±Ô∏è Tempo:** ~10-15 minutos (download de 1GB de modelos)

### 2Ô∏è‚É£ Iniciar servi√ßos

```powershell
# Com GPU
docker-compose up -d

# CPU-only
docker-compose -f docker-compose-cpu.yaml up -d
```

### 3Ô∏è‚É£ Verificar modelos instalados

```powershell
docker exec ollama-embeddings ollama list
```

**Resultado esperado:**
```
NAME                       ID           SIZE    MODIFIED
all-minilm:latest          abc123       23 MB   2 minutes ago
nomic-embed-text:latest    def456       274 MB  2 minutes ago
mxbai-embed-large:latest   ghi789       670 MB  2 minutes ago
```

## üéØ Modelos Inclu√≠dos

| Modelo | Dimens√µes | Tamanho | Uso |
|--------|-----------|---------|-----|
| `all-minilm:latest` | 384 | 23 MB | R√°pido, prototipagem |
| `nomic-embed-text:latest` | 768 | 274 MB | RAG geral |
| `mxbai-embed-large:latest` | 1024 | 670 MB | M√°xima qualidade |

## üß™ Testar no Notebook

```python
import requests

# Verificar conex√£o
response = requests.get('http://localhost:11434/api/tags')
print(f"Modelos dispon√≠veis: {len(response.json()['models'])}")

# Gerar embedding
payload = {
    "model": "nomic-embed-text",
    "input": "Teste de embedding"
}
response = requests.post('http://localhost:11434/api/embed', json=payload)
embedding = response.json()['embeddings'][0]
print(f"Dimens√µes: {len(embedding)}")
```

## üìÅ Arquivos Criados

- `Dockerfile.ollama` - Imagem customizada com modelos
- `OLLAMA_SETUP.md` - Documenta√ß√£o completa
- `manage-ollama.ps1` - Script de gerenciamento
- `.dockerignore` - Otimiza√ß√£o de build

## üîÑ Mudan√ßas nos Docker Compose

### Antes:
```yaml
ollama:
  image: ollama/ollama:latest
```

### Depois:
```yaml
ollama:
  build:
    context: .
    dockerfile: Dockerfile.ollama
  image: ollama-embeddings-custom:latest
```

## ‚ö†Ô∏è Importante

1. **Primeira build √© demorada** (~15 min) - Downloads de modelos
2. **Imagem fica maior** (~2.1GB vs ~500MB)
3. **Benef√≠cio:** Modelos prontos para usar imediatamente!

## üêõ Problemas Comuns

### "Connection refused"
```powershell
# Verificar se est√° rodando
docker ps | Select-String ollama

# Se n√£o estiver, iniciar
docker-compose up -d ollama
```

### "Model not found"
```powershell
# Rebuild for√ßado
docker-compose build --no-cache ollama
docker-compose up -d ollama
```

### Build travou
```powershell
# Cancelar (Ctrl+C) e tentar novamente
docker-compose build --progress=plain ollama
```

## üìö Pr√≥ximos Passos

1. ‚úÖ Abrir Jupyter: http://localhost:8888
2. ‚úÖ Executar `lab_1.3_comparativos_ollama.ipynb`
3. ‚úÖ Testar os 3 modelos de embeddings
4. ‚úÖ Comparar performance e qualidade

## üÜò Ajuda

- **Documenta√ß√£o completa:** `OLLAMA_SETUP.md`
- **Script interativo:** `.\manage-ollama.ps1`
- **Ollama docs:** https://github.com/ollama/ollama
