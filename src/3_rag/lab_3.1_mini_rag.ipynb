{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d79fb7e6",
   "metadata": {},
   "source": [
    "# üéØ Mini RAG: Persistindo e Reutilizando √çndices Vetoriais\n",
    "\n",
    "## O que vamos aprender?\n",
    "\n",
    "Neste notebook, voc√™ vai aprender a **salvar e carregar** √≠ndices vetoriais do FAISS, um passo essencial para construir aplica√ß√µes **RAG (Retrieval Augmented Generation)** em produ√ß√£o.\n",
    "\n",
    "### O que √© RAG?\n",
    "\n",
    "**RAG = Retrieval Augmented Generation** (Gera√ß√£o Aumentada de Recupera√ß√£o)\n",
    "\n",
    "√â uma t√©cnica que combina:\n",
    "1. üîç **Busca vetorial** (o que fizemos nos labs anteriores)\n",
    "2. ü§ñ **LLM** (ChatGPT, Gemini, etc.) para gerar respostas\n",
    "\n",
    "**Fluxo RAG completo:**\n",
    "\n",
    "```text\n",
    "Pergunta do usu√°rio\n",
    "    ‚Üì\n",
    "Busca vetorial (FAISS) ‚Üí Encontra documentos relevantes\n",
    "    ‚Üì\n",
    "Contexto + Pergunta ‚Üí Enviado para LLM\n",
    "    ‚Üì\n",
    "LLM gera resposta baseada no contexto\n",
    "    ‚Üì\n",
    "Resposta final ao usu√°rio\n",
    "```\n",
    "\n",
    "### Por que persistir √≠ndices?\n",
    "\n",
    "**Problema sem persist√™ncia:**\n",
    "```python\n",
    "# Toda vez que voc√™ reinicia o programa:\n",
    "vector_store = FAISS.from_texts(meus_textos, embeddings)\n",
    "# ‚Üë Precisa recalcular TODOS os embeddings (caro e lento!)\n",
    "```\n",
    "\n",
    "**Solu√ß√£o com persist√™ncia:**\n",
    "```python\n",
    "# Uma vez:\n",
    "vector_store.save_local(\"meu_indice\")\n",
    "\n",
    "# Depois, sempre que precisar:\n",
    "vector_store = FAISS.load_local(\"meu_indice\", embeddings)\n",
    "# ‚Üë Instant√¢neo! N√£o recalcula nada\n",
    "```\n",
    "\n",
    "### Benef√≠cios\n",
    "\n",
    "‚úÖ **Economia:** N√£o paga API repetidamente  \n",
    "‚úÖ **Velocidade:** Carrega em milissegundos  \n",
    "‚úÖ **Escalabilidade:** √çndices podem ter milh√µes de documentos  \n",
    "‚úÖ **Produ√ß√£o:** Essencial para apps reais\n",
    "\n",
    "### O que faremos\n",
    "\n",
    "1. Criar um √≠ndice vetorial (como antes)\n",
    "2. **Salvar no disco** (novidade!)\n",
    "3. **Carregar do disco** (novidade!)\n",
    "4. Usar normalmente para buscas\n",
    "\n",
    "üí° **Analogia:** √â como salvar um jogo - voc√™ n√£o quer recome√ßar do zero toda vez!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd50fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b618f",
   "metadata": {},
   "source": [
    "## üì¶ Imports e Configura√ß√£o\n",
    "\n",
    "Bibliotecas necess√°rias:\n",
    "- `os`, `Path`, `load_dotenv`: Gerenciamento de arquivos e vari√°veis de ambiente\n",
    "- `FAISS`: Banco de dados vetorial (com suporte a salvar/carregar!)\n",
    "- `OpenAIEmbeddings`: API da OpenAI para embeddings\n",
    "\n",
    "### Por que OpenAI?\n",
    "\n",
    "Neste lab usamos OpenAI porque:\n",
    "- ‚úÖ **Qualidade l√≠der de mercado** (1536 dimens√µes)\n",
    "- ‚úÖ **Modelo text-embedding-3-small** eficiente e econ√¥mico\n",
    "- ‚úÖ **Amplamente usado em produ√ß√£o**\n",
    "\n",
    "**Como obter API key:**\n",
    "1. Acesse [platform.openai.com/api-keys](https://platform.openai.com/api-keys)\n",
    "2. Clique \"Create new secret key\"\n",
    "3. Adicione no `.env`: `OPENAI_API_KEY=sk-...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25342ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé .env carregado -> E:\\01-projetos\\11-work\\11.34-engenharia-vetorial\\.env\n"
     ]
    }
   ],
   "source": [
    "# 2) Configura√ß√£o e carregamento do .env (simplificado)\n",
    "env_path = Path.cwd().joinpath('..', '..', '.env').resolve()\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f'üîé .env carregado -> {env_path.resolve()}')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  .env n√£o encontrado. Defina as vari√°veis de ambiente manualmente.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a216489",
   "metadata": {},
   "source": [
    "## üîê Carregando Vari√°veis de Ambiente\n",
    "\n",
    "Carrega a `OPENAI_API_KEY` do arquivo `.env`.\n",
    "\n",
    "**Estrutura do `.env`:**\n",
    "```bash\n",
    "OPENAI_API_KEY=sk-...\n",
    "```\n",
    "\n",
    "Se voc√™ vir ‚ö†Ô∏è, crie o arquivo `.env` na raiz do projeto com sua API key da OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2b68f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meus_textos = [\n",
    "    \"O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.\",    # Tecnologia\n",
    "    \"Para fazer um bolo macio, bata as claras em neve.\",       # Culin√°ria\n",
    "    \"O atacante chutou a bola no √¢ngulo e foi gol.\",           # Esporte\n",
    "    \"A placa de v√≠deo RTX 4090 roda jogos em 4K.\",             # Tecnologia\n",
    "    \"Receita de lasanha √† bolonhesa com muito queijo.\"         # Culin√°ria\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d49c948",
   "metadata": {},
   "source": [
    "## üìÑ Passo 1: Preparando os Documentos\n",
    "\n",
    "Nosso dataset de teste com 5 documentos de categorias diferentes.\n",
    "\n",
    "**Importante:** Em uma aplica√ß√£o RAG real, esses textos viriam de:\n",
    "- üìë PDFs de documenta√ß√£o\n",
    "- üåê Base de conhecimento da empresa\n",
    "- üí¨ FAQs e tickets de suporte\n",
    "- üìä Manuais t√©cnicos\n",
    "\n",
    "üí° **Conceito RAG:** Quanto mais documentos relevantes voc√™ indexar, melhor o LLM conseguir√° responder perguntas espec√≠ficas do seu dom√≠nio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f2a4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb83bcd",
   "metadata": {},
   "source": [
    "## üß† Passo 2: Inicializando o Modelo de Embeddings\n",
    "\n",
    "Criamos uma inst√¢ncia do modelo de embeddings da OpenAI.\n",
    "\n",
    "**Modelo:** `text-embedding-3-small`\n",
    "- **Dimens√µes:** 1536 (alta qualidade sem√¢ntica)\n",
    "- **Qualidade:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê L√≠der de mercado\n",
    "- **Custo:** ~$0.02 por 1M tokens\n",
    "- **Velocidade:** ~100-200ms por request\n",
    "\n",
    "### Como funciona?\n",
    "\n",
    "```\n",
    "Texto ‚Üí üåê API OpenAI ‚Üí Vetor [1536 n√∫meros]\n",
    "```\n",
    "\n",
    "Cada texto ser√° transformado em um vetor de 1536 dimens√µes que captura seu significado sem√¢ntico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aff7c7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_texts(meus_textos, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346983e6",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Passo 3: Criando o √çndice Vetorial\n",
    "\n",
    "Transformamos os textos em vetores e criamos o √≠ndice FAISS **em mem√≥ria**.\n",
    "\n",
    "### O que acontece internamente?\n",
    "\n",
    "```text\n",
    "Para cada texto em meus_textos:\n",
    "  1. Envia para API da OpenAI\n",
    "  2. Recebe vetor de 1536 dimens√µes\n",
    "  3. FAISS armazena o vetor em RAM\n",
    "\n",
    "Resultado:\n",
    "  vector_store = √çndice FAISS com 5 vetores em mem√≥ria\n",
    "```\n",
    "\n",
    "**Importante:** Neste ponto, o √≠ndice existe **apenas na RAM**! Se voc√™ fechar o programa, perde tudo.\n",
    "\n",
    "‚è±Ô∏è **Tempo:** ~1-2 segundos (5 chamadas √† API da OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e8e3152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banco vetorial salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "FAISS_PATH = Path.cwd().joinpath('..', '..', 'data', 'meu_indice_faiss')\n",
    "\n",
    "vector_store.save_local(str(FAISS_PATH))\n",
    "print(\"Banco vetorial salvo com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8baffb",
   "metadata": {},
   "source": [
    "## üíæ Passo 4: Salvando o √çndice no Disco (PERSIST√äNCIA!)\n",
    "\n",
    "Esta √© a **parte mais importante** para aplica√ß√µes RAG em produ√ß√£o!\n",
    "\n",
    "### O que acontece aqui?\n",
    "\n",
    "```python\n",
    "vector_store.save_local(str(FAISS_PATH))\n",
    "```\n",
    "\n",
    "**Internamente:**\n",
    "1. FAISS cria uma pasta: `data/meu_indice_faiss/`\n",
    "2. Salva 2 arquivos:\n",
    "   - `index.faiss` ‚Üí Os vetores e estrutura de √≠ndice\n",
    "   - `index.pkl` ‚Üí Metadados (textos originais, etc.)\n",
    "\n",
    "### Estrutura de arquivos criada:\n",
    "\n",
    "```text\n",
    "data/\n",
    "‚îî‚îÄ‚îÄ meu_indice_faiss/\n",
    "    ‚îú‚îÄ‚îÄ index.faiss  ‚Üê ~100KB (5 vetores √ó 1536 dims √ó 4 bytes)\n",
    "    ‚îî‚îÄ‚îÄ index.pkl    ‚Üê ~2KB (textos originais)\n",
    "```\n",
    "\n",
    "### Por que isso √© importante?\n",
    "\n",
    "**Sem persist√™ncia:**\n",
    "```python\n",
    "# Toda execu√ß√£o:\n",
    "vector_store = FAISS.from_texts(textos, embeddings)\n",
    "# ‚Üë 5 chamadas √† API + 1-2 segundos\n",
    "```\n",
    "\n",
    "**Com persist√™ncia:**\n",
    "```python\n",
    "# Apenas uma vez:\n",
    "vector_store.save_local(\"path\")\n",
    "\n",
    "# Depois, sempre:\n",
    "vector_store = FAISS.load_local(\"path\", embeddings)\n",
    "# ‚Üë 0 chamadas √† API + ~10ms\n",
    "```\n",
    "\n",
    "### Benef√≠cios em produ√ß√£o\n",
    "\n",
    "| M√©trica | Sem Persist√™ncia | Com Persist√™ncia |\n",
    "|---------|------------------|------------------|\n",
    "| **Tempo de startup** | Minutos (muitos docs) | Milissegundos |\n",
    "| **Custo de API** | Toda vez | Uma vez (na cria√ß√£o) |\n",
    "| **Escalabilidade** | Limitada | Milh√µes de docs |\n",
    "| **Confiabilidade** | Dependente de API | Funciona offline |\n",
    "\n",
    "üí° **Analogia:** √â como salvar um documento do Word - voc√™ n√£o quer redigitar tudo cada vez que abrir!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36b94efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARREGAR do disco\n",
    "# O par√¢metro 'allow_dangerous_deserialization' √© necess√°rio em vers√µes recentes\n",
    "# para confirmar que voc√™ confia no arquivo que est√° carregando.\n",
    "novo_db = FAISS.load_local(\n",
    "    str(FAISS_PATH), \n",
    "    embeddings, \n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faff540f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id: 5c23ef21-07bd-4f1b-9498-e961cfad9a87 -> texto: page_content='O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.'\n",
      "doc_id: c925c84f-f8e5-489d-8b54-4bc286bd8c72 -> texto: page_content='Para fazer um bolo macio, bata as claras em neve.'\n",
      "doc_id: 15df16e6-1b97-46ea-85aa-fd3f51931c8a -> texto: page_content='O atacante chutou a bola no √¢ngulo e foi gol.'\n",
      "doc_id: 6b8b34e7-c597-4d42-921d-6ff9252aa0ef -> texto: page_content='A placa de v√≠deo RTX 4090 roda jogos em 4K.'\n",
      "doc_id: 8ca233c3-f687-4516-bbc4-57795c1eb133 -> texto: page_content='Receita de lasanha √† bolonhesa com muito queijo.'\n"
     ]
    }
   ],
   "source": [
    "# Visualizar os textos carregados\n",
    "for doc_id in novo_db.index_to_docstore_id.values():\n",
    "    documento = novo_db.docstore.search(doc_id)\n",
    "    print(f'doc_id: {doc_id} -> texto: {documento}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71522f",
   "metadata": {},
   "source": [
    "## üìÇ Passo 5: Carregando o √çndice do Disco\n",
    "\n",
    "Agora vamos **carregar** o √≠ndice que salvamos anteriormente!\n",
    "\n",
    "### O que acontece aqui?\n",
    "\n",
    "```python\n",
    "novo_db = FAISS.load_local(str(FAISS_PATH), embeddings, allow_dangerous_deserialization=True)\n",
    "```\n",
    "\n",
    "**Internamente:**\n",
    "1. FAISS l√™ os arquivos da pasta `data/meu_indice_faiss/`\n",
    "2. Reconstr√≥i o √≠ndice em mem√≥ria (~10ms)\n",
    "3. Pronto para uso instant√¢neo!\n",
    "\n",
    "### Par√¢metros importantes\n",
    "\n",
    "**`embeddings`**: Necess√°rio para futuras queries\n",
    "- N√£o recalcula nada dos documentos existentes\n",
    "- S√≥ ser√° usado quando voc√™ buscar algo novo\n",
    "\n",
    "**`allow_dangerous_deserialization=True`**: Confirma√ß√£o de seguran√ßa\n",
    "- FAISS usa pickle para serializar\n",
    "- Pickle pode executar c√≥digo malicioso se vier de fonte n√£o confi√°vel\n",
    "- Como voc√™ mesmo criou o √≠ndice, √© seguro ‚úÖ\n",
    "\n",
    "### Fluxo completo de uma aplica√ß√£o RAG\n",
    "\n",
    "```text\n",
    "Primeira execu√ß√£o (setup):\n",
    "  1. Carregar documentos\n",
    "  2. Criar embeddings (via API)\n",
    "  3. Criar √≠ndice FAISS\n",
    "  4. Salvar no disco ‚Üê Voc√™ est√° aqui!\n",
    "\n",
    "Execu√ß√µes subsequentes (produ√ß√£o):\n",
    "  5. Carregar √≠ndice do disco ‚Üê Estamos fazendo isso agora!\n",
    "  6. Receber query do usu√°rio\n",
    "  7. Buscar documentos relevantes\n",
    "  8. Enviar contexto + query para LLM\n",
    "  9. Retornar resposta\n",
    "```\n",
    "\n",
    "### Performance\n",
    "\n",
    "| Opera√ß√£o | Tempo (5 docs) | Tempo (10k docs) | Tempo (1M docs) |\n",
    "|----------|----------------|------------------|-----------------|\n",
    "| **Criar √≠ndice** | ~2s | ~30s | ~30min |\n",
    "| **Salvar** | ~50ms | ~200ms | ~5s |\n",
    "| **Carregar** | ~10ms | ~100ms | ~2s |\n",
    "| **Buscar** | <1ms | ~5ms | ~50ms |\n",
    "\n",
    "üí° **Observe:** Carregar √© **muito mais r√°pido** que criar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97fc2403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usar normalmente\n",
    "consulta = \"Sugest√£o de celular\"\n",
    "resultados = novo_db.similarity_search(consulta, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc578291",
   "metadata": {},
   "source": [
    "## üîç Passo 6: Usando o √çndice Carregado\n",
    "\n",
    "Agora vamos fazer uma busca usando o √≠ndice que **carregamos do disco**!\n",
    "\n",
    "### A Query\n",
    "\n",
    "```python\n",
    "consulta = \"Sugest√£o de celular\"\n",
    "```\n",
    "\n",
    "**Desafio sem√¢ntico:**\n",
    "- A palavra \"iPhone\" n√£o aparece na query\n",
    "- A palavra \"celular\" sim\n",
    "- O modelo precisa entender que iPhone √© um celular\n",
    "\n",
    "### O que acontece internamente?\n",
    "\n",
    "1. **Query vira vetor (via API OpenAI):**\n",
    "   ```text\n",
    "   \"Sugest√£o de celular\" ‚Üí üåê API call ‚Üí [0.15, -0.32, ..., 0.47]\n",
    "   ```\n",
    "\n",
    "2. **FAISS busca (local, sem API):**\n",
    "   ```text\n",
    "   Compara o vetor da query com os 5 vetores armazenados\n",
    "   Calcula dist√¢ncias:\n",
    "     - iPhone: 0.3 ‚Üê Mais pr√≥ximo!\n",
    "     - RTX 4090: 1.2\n",
    "     - Bolo: 2.5\n",
    "     - Gol: 2.8\n",
    "     - Lasanha: 2.6\n",
    "   ```\n",
    "\n",
    "3. **Retorna top-k (k=2):**\n",
    "   ```\n",
    "   [iPhone, RTX 4090]\n",
    "   ```\n",
    "\n",
    "### Par√¢metro k\n",
    "\n",
    "- `k=1`: Retorna apenas o mais similar\n",
    "- `k=2`: Retorna os 2 mais similares\n",
    "- `k=5`: Retorna todos (nosso dataset tem 5)\n",
    "\n",
    "üí° **Dica:** Em RAG, geralmente usa-se `k=3` ou `k=5` para dar contexto suficiente ao LLM sem sobrecarregar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b97274e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.\n"
     ]
    }
   ],
   "source": [
    "print(resultados[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecbe31",
   "metadata": {},
   "source": [
    "## üìä Passo 7: Visualizando o Resultado\n",
    "\n",
    "Vamos ver o documento mais relevante (√≠ndice 0 = primeiro resultado).\n",
    "\n",
    "**Esperado:** Deve retornar algo sobre iPhone, pois √© o documento sobre celular!\n",
    "\n",
    "### Interpretando o resultado\n",
    "\n",
    "Se voc√™ ver:\n",
    "- ‚úÖ `\"O novo iPhone 15...\"` ‚Üí Perfeito! A busca funcionou\n",
    "- ‚ùå Qualquer outro texto ‚Üí Algo deu errado (improv√°vel com OpenAI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01794d7",
   "metadata": {},
   "source": [
    "\n",
    "### Pr√≥ximo passo: RAG completo\n",
    "\n",
    "Em um RAG de verdade, voc√™ faria:\n",
    "\n",
    "```python\n",
    "# 1. Buscar documentos relevantes (j√° fizemos!)\n",
    "resultados = novo_db.similarity_search(consulta, k=3)\n",
    "\n",
    "# 2. Montar o contexto\n",
    "contexto = \"\\n\".join([doc.page_content for doc in resultados])\n",
    "\n",
    "# 3. Criar prompt para LLM\n",
    "prompt = f\"\"\"\n",
    "Baseado no seguinte contexto:\n",
    "{contexto}\n",
    "\n",
    "Responda a pergunta: {consulta}\n",
    "\"\"\"\n",
    "\n",
    "# 4. Enviar para LLM (GPT, etc.)\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "resposta = llm.invoke(prompt)\n",
    "\n",
    "print(resposta.content)\n",
    "# ‚Üí \"Eu recomendo o iPhone 15, que possui uma lente perisc√≥pica incr√≠vel...\"\n",
    "```\n",
    "\n",
    "üí° **Esse √© o poder do RAG:** O LLM responde com base nos **seus** documentos, n√£o apenas no conhecimento geral!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f9bcdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def rag_query(pergunta, k=2):\n",
    "    \"\"\"\n",
    "    Realiza uma busca RAG completa:\n",
    "    1. Carrega o √≠ndice do disco\n",
    "    2. Busca documentos relevantes\n",
    "    3. Envia para o LLM gerar a resposta\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Carregar √≠ndice do disco (garantindo que usamos a vers√£o persistida)\n",
    "    # allow_dangerous_deserialization=True √© necess√°rio pois criamos o √≠ndice n√≥s mesmos\n",
    "    db = FAISS.load_local(str(FAISS_PATH), embeddings, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    # 2. Buscar documentos relevantes (Retrieval)\n",
    "    docs = db.similarity_search(pergunta, k=k)\n",
    "    \n",
    "    # 3. Montar o contexto (Augmentation)\n",
    "    contexto = \"\\n\".join([f\"- {doc.page_content}\" for doc in docs])\n",
    "    \n",
    "    # 4. Criar prompt\n",
    "    prompt = f\"\"\"\n",
    "    Voc√™ √© um assistente inteligente. Use APENAS o contexto abaixo para responder a pergunta do usu√°rio.\n",
    "    Se a resposta n√£o estiver no contexto, diga educadamente que n√£o possui essa informa√ß√£o.\n",
    "    \n",
    "    Contexto:\n",
    "    {contexto}\n",
    "    \n",
    "    Pergunta: {pergunta}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 5. Gerar resposta (Generation)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "    resposta = llm.invoke(prompt)\n",
    "    \n",
    "    # Retorna a resposta (texto) e os documentos usados (fonte)\n",
    "    return resposta.content, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65701263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Pergunta: Qual celular tem uma c√¢mera boa?\n",
      "\n",
      "‚è≥ Processando RAG...\n",
      "\n",
      "üìù Resposta do GPT:\n",
      "----------------------------------------\n",
      "O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel, o que indica que ele possui uma c√¢mera boa.\n",
      "----------------------------------------\n",
      "\n",
      "üìö Documentos usados no contexto:\n",
      "   1. O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.\n",
      "   2. A placa de v√≠deo RTX 4090 roda jogos em 4K.\n",
      "üìù Resposta do GPT:\n",
      "----------------------------------------\n",
      "O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel, o que indica que ele possui uma c√¢mera boa.\n",
      "----------------------------------------\n",
      "\n",
      "üìö Documentos usados no contexto:\n",
      "   1. O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.\n",
      "   2. A placa de v√≠deo RTX 4090 roda jogos em 4K.\n"
     ]
    }
   ],
   "source": [
    "# Vamos testar nosso sistema RAG!\n",
    "pergunta = \"Qual celular tem uma c√¢mera boa?\"\n",
    "\n",
    "print(f\"ü§ñ Pergunta: {pergunta}\\n\")\n",
    "print(\"‚è≥ Processando RAG...\\n\")\n",
    "\n",
    "resposta_final, docs_usados = rag_query(pergunta)\n",
    "\n",
    "print(\"üìù Resposta do GPT:\")\n",
    "print(\"-\" * 40)\n",
    "print(resposta_final)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nüìö Documentos usados no contexto:\")\n",
    "for i, doc in enumerate(docs_usados):\n",
    "    print(f\"   {i+1}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637abf5",
   "metadata": {},
   "source": [
    "### üß† Entendendo a \"M√°gica\"\n",
    "\n",
    "Observe o que aconteceu acima:\n",
    "\n",
    "1.  **Recupera√ß√£o (Retrieval):** O sistema buscou no FAISS os trechos mais parecidos com \"c√¢mera boa\". Ele encontrou reviews de celulares (ex: \"O Galaxy S23 tem uma c√¢mera incr√≠vel...\").\n",
    "2.  **Augmenta√ß√£o (Augmentation):** Pegamos esses trechos e colamos no prompt do GPT.\n",
    "3.  **Gera√ß√£o (Generation):** O GPT leu os trechos e respondeu a pergunta **baseado apenas neles**.\n",
    "\n",
    "√â como se voc√™ desse um livro para algu√©m e dissesse: \"Responda a pergunta usando APENAS este livro\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1efb7c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üß™ Experimento: Perguntas fora do contexto\n",
    "\n",
    "O que acontece se perguntarmos algo que **n√£o** est√° nos documentos que indexamos?\n",
    "Vamos testar com uma pergunta sobre futebol (nossos documentos s√£o sobre tecnologia/celulares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe1b52f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Pergunta: Quem ganhou a copa de 1994?\n",
      "\n",
      "üìù Resposta do GPT:\n",
      "----------------------------------------\n",
      "Desculpe, mas n√£o possuo essa informa√ß√£o.\n",
      "----------------------------------------\n",
      "\n",
      "üìö Documentos recuperados (provavelmente irrelevantes):\n",
      "   1. O atacante chutou a bola no √¢ngulo e foi gol.\n",
      "   2. Receita de lasanha √† bolonhesa com muito queijo.\n",
      "üìù Resposta do GPT:\n",
      "----------------------------------------\n",
      "Desculpe, mas n√£o possuo essa informa√ß√£o.\n",
      "----------------------------------------\n",
      "\n",
      "üìö Documentos recuperados (provavelmente irrelevantes):\n",
      "   1. O atacante chutou a bola no √¢ngulo e foi gol.\n",
      "   2. Receita de lasanha √† bolonhesa com muito queijo.\n"
     ]
    }
   ],
   "source": [
    "pergunta_fora = \"Quem ganhou a copa de 1994?\"\n",
    "\n",
    "print(f\"ü§ñ Pergunta: {pergunta_fora}\\n\")\n",
    "resposta_fora, docs_fora = rag_query(pergunta_fora)\n",
    "\n",
    "print(\"üìù Resposta do GPT:\")\n",
    "print(\"-\" * 40)\n",
    "print(resposta_fora)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nüìö Documentos recuperados (provavelmente irrelevantes):\")\n",
    "for i, doc in enumerate(docs_fora):\n",
    "    print(f\"   {i+1}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac75c7",
   "metadata": {},
   "source": [
    "### üí° An√°lise do Erro (ou Sucesso)\n",
    "\n",
    "Se o modelo respondeu \"N√£o sei\" ou deu uma resposta estranha, isso √© **bom**!\n",
    "Significa que ele est√° respeitando o contexto. Como n√£o indexamos nada sobre futebol, ele n√£o encontrou informa√ß√µes para responder.\n",
    "\n",
    "**Li√ß√£o:** O RAG depende 100% da qualidade da busca.\n",
    "*   **Busca Ruim** (Garbage In) ‚Üí **Resposta Ruim** (Garbage Out).\n",
    "\n",
    "Por isso, gastamos tanto tempo nos labs anteriores aprendendo sobre Embeddings e Busca Vetorial! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb55d61",
   "metadata": {},
   "source": [
    "### üîç Analisando o Resultado\n",
    "\n",
    "Observe que o modelo n√£o apenas \"copiou\" o texto, mas **interpretou** a informa√ß√£o.\n",
    "\n",
    "- Voc√™ perguntou sobre \"c√¢mera boa\"\n",
    "- O documento falava sobre \"lente perisc√≥pica incr√≠vel\"\n",
    "- O LLM conectou os pontos e recomendou o iPhone 15!\n",
    "\n",
    "Isso √© muito mais poderoso que uma busca simples por palavras-chave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e46967",
   "metadata": {},
   "source": [
    "## üéì Resumo e Conceitos-Chave\n",
    "\n",
    "### O que aprendemos\n",
    "\n",
    "‚úÖ **Persist√™ncia de √≠ndices** - Salvar e carregar FAISS do disco  \n",
    "‚úÖ **Economia de API** - N√£o recalcular embeddings toda vez  \n",
    "‚úÖ **Performance** - Carregar √© 100x mais r√°pido que criar  \n",
    "‚úÖ **Fundamentos de RAG** - Base para aplica√ß√µes com LLM\n",
    "\n",
    "### Fluxo completo que implementamos\n",
    "\n",
    "```text\n",
    "1. Criar embeddings (via API OpenAI)\n",
    "   ‚Üì\n",
    "2. Criar √≠ndice FAISS (em RAM)\n",
    "   ‚Üì\n",
    "3. Salvar no disco (.save_local) ‚Üê PERSIST√äNCIA!\n",
    "   ‚Üì\n",
    "4. Carregar do disco (.load_local) ‚Üê REUTILIZA√á√ÉO!\n",
    "   ‚Üì\n",
    "5. Fazer buscas normalmente\n",
    "```\n",
    "\n",
    "### Diferen√ßas: Labs anteriores vs. Este\n",
    "\n",
    "| Aspecto | Labs 1.4-1.6 | Este Lab (1.7) |\n",
    "|---------|--------------|----------------|\n",
    "| **√çndice** | S√≥ em RAM | Salvo em disco |\n",
    "| **Reiniciar programa** | Perde tudo | Mant√©m tudo |\n",
    "| **Startup** | Lento (recria) | R√°pido (carrega) |\n",
    "| **Produ√ß√£o** | ‚ùå Invi√°vel | ‚úÖ Pronto |\n",
    "\n",
    "### Aplica√ß√µes reais de RAG\n",
    "\n",
    "ü§ñ **Chatbots corporativos**\n",
    "- Indexa documenta√ß√£o interna\n",
    "- LLM responde baseado nos docs da empresa\n",
    "\n",
    "üìö **Assistentes de estudo**\n",
    "- Indexa livros, apostilas, anota√ß√µes\n",
    "- LLM explica conceitos baseado no material\n",
    "\n",
    "üîß **Suporte t√©cnico**\n",
    "- Indexa manuais, FAQs, tickets antigos\n",
    "- LLM sugere solu√ß√µes baseadas em casos similares\n",
    "\n",
    "üè• **Assist√™ncia m√©dica**\n",
    "- Indexa prontu√°rios, estudos cient√≠ficos\n",
    "- LLM auxilia diagn√≥sticos (com supervis√£o humana!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc2c64",
   "metadata": {},
   "source": [
    "\n",
    "### üß™ Experimentos para tentar\n",
    "\n",
    "#### Experimento 1: Adicionar mais documentos\n",
    "```python\n",
    "novos_textos = [\n",
    "    \"Samsung Galaxy S23 tem c√¢mera de 200MP\",\n",
    "    \"Xiaomi Redmi Note 12 √© um bom custo-benef√≠cio\",\n",
    "]\n",
    "\n",
    "# Criar novo √≠ndice com todos os documentos\n",
    "todos_textos = meus_textos + novos_textos\n",
    "vector_store = FAISS.from_texts(todos_textos, embeddings)\n",
    "vector_store.save_local(str(FAISS_PATH))\n",
    "\n",
    "# Agora tem 7 documentos!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870205e",
   "metadata": {},
   "source": [
    "\n",
    "#### Experimento 2: Atualizar √≠ndice existente\n",
    "\n",
    "```python\n",
    "# Carregar √≠ndice existente\n",
    "db = FAISS.load_local(str(FAISS_PATH), embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Adicionar novos documentos\n",
    "novos_docs = [\"Novo documento...\"]\n",
    "db.add_texts(novos_docs)\n",
    "\n",
    "# Salvar novamente (agora com os novos docs)\n",
    "db.save_local(str(FAISS_PATH))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e0a09",
   "metadata": {},
   "source": [
    "\n",
    "#### Experimento 3: M√∫ltiplos √≠ndices\n",
    "```python\n",
    "# √çndice para tecnologia\n",
    "tech_db = FAISS.from_texts(textos_tech, embeddings)\n",
    "tech_db.save_local(\"indices/tecnologia\")\n",
    "\n",
    "# √çndice para culin√°ria\n",
    "food_db = FAISS.from_texts(textos_culinaria, embeddings)\n",
    "food_db.save_local(\"indices/culinaria\")\n",
    "\n",
    "# Carregar conforme necess√°rio\n",
    "tech = FAISS.load_local(\"indices/tecnologia\", embeddings, allow_dangerous_deserialization=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8717bb",
   "metadata": {},
   "source": [
    "\n",
    "#### Experimento 4: RAG completo (desafio!)\n",
    "```python\n",
    "# Combine tudo que aprendeu:\n",
    "# 1. Carregar √≠ndice\n",
    "# 2. Fazer busca\n",
    "# 3. Enviar contexto para LLM (GPT)\n",
    "# 4. Retornar resposta gerada\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def rag_query(pergunta, k=3):\n",
    "    # Buscar documentos relevantes\n",
    "    db = FAISS.load_local(str(FAISS_PATH), embeddings, allow_dangerous_deserialization=True)\n",
    "    docs = db.similarity_search(pergunta, k=k)\n",
    "    \n",
    "    # Montar contexto\n",
    "    contexto = \"\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    # Prompt para LLM\n",
    "    prompt = f\"Contexto:\\n{contexto}\\n\\nPergunta: {pergunta}\\nResposta:\"\n",
    "    \n",
    "    # Gerar resposta\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    resposta = llm.invoke(prompt)\n",
    "    \n",
    "    return resposta.content\n",
    "\n",
    "# Testar\n",
    "print(rag_query(\"Qual √© o melhor celular?\"))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d562b64c",
   "metadata": {},
   "source": [
    "\n",
    "### üí° Boas pr√°ticas em produ√ß√£o\n",
    "\n",
    "1. **Versionamento de √≠ndices**\n",
    "   ```python\n",
    "   vector_store.save_local(\"indices/v1.0.0\")\n",
    "   vector_store.save_local(\"indices/v1.1.0\")  # Ap√≥s adicionar docs\n",
    "   ```\n",
    "\n",
    "2. **Backup autom√°tico**\n",
    "   ```python\n",
    "   import shutil\n",
    "   from datetime import datetime\n",
    "   \n",
    "   backup_path = f\"backups/index_{datetime.now().strftime('%Y%m%d')}\"\n",
    "   shutil.copytree(FAISS_PATH, backup_path)\n",
    "   ```\n",
    "\n",
    "3. **Metadados**\n",
    "   ```python\n",
    "   # Salvar informa√ß√µes sobre o √≠ndice\n",
    "   metadata = {\n",
    "       \"created_at\": datetime.now(),\n",
    "       \"num_docs\": len(meus_textos),\n",
    "       \"model\": \"text-embedding-3-small\",\n",
    "       \"dimensions\": 1536\n",
    "   }\n",
    "   \n",
    "   import json\n",
    "   with open(FAISS_PATH / \"metadata.json\", \"w\") as f:\n",
    "       json.dump(metadata, f)\n",
    "   ```\n",
    "\n",
    "4. **Monitoramento**\n",
    "   ```python\n",
    "   import os\n",
    "   \n",
    "   # Verificar tamanho do √≠ndice\n",
    "   index_size = os.path.getsize(FAISS_PATH / \"index.faiss\")\n",
    "   print(f\"Tamanho do √≠ndice: {index_size / 1024:.2f} KB\")\n",
    "   ```\n",
    "\n",
    "üí° **Dica final:** O RAG √© a t√©cnica mais popular para fazer LLMs responderem com informa√ß√µes espec√≠ficas do seu neg√≥cio. Domine isso e voc√™ ter√° um skill muito valorizado no mercado!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "11-34-engenharia-vetorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
