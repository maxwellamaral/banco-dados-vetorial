{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b325f0f",
   "metadata": {},
   "source": [
    "# üîç Laborat√≥rio: Busca Sem√¢ntica Local com Ollama\n",
    "\n",
    "## O que vamos aprender?\n",
    "\n",
    "Neste notebook, voc√™ vai construir um **sistema de busca inteligente** que entende o **significado** das palavras, n√£o apenas correspond√™ncias exatas de texto.\n",
    "\n",
    "### Diferen√ßa entre Busca Tradicional vs. Busca Sem√¢ntica\n",
    "\n",
    "**Busca Tradicional (palavras-chave):**\n",
    "```text\n",
    "Query: \"hardware celular\"\n",
    "Resultado: S√≥ encontra documentos com as palavras exatas \"hardware\" e \"celular\"\n",
    "```\n",
    "\n",
    "**Busca Sem√¢ntica (significado):**\n",
    "```text\n",
    "Query: \"hardware celular\"\n",
    "Resultado: Encontra \"iPhone\", \"smartphone\", \"processador mobile\" \n",
    "           mesmo sem as palavras exatas!\n",
    "```\n",
    "\n",
    "### Como isso funciona?\n",
    "\n",
    "Usando **bibliotecas reais** de produ√ß√£o que empresas usam no dia a dia:\n",
    "- **LangChain** ‚Üí Framework para construir apps com IA\n",
    "- **Ollama** ‚Üí Roda modelos de IA localmente (gr√°tis!)\n",
    "- **FAISS** ‚Üí Banco de dados vetorial ultrarr√°pido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b34f0e4",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Ferramentas que Vamos Usar\n",
    "\n",
    "### 1. LangChain ü¶úüîó\n",
    "**O que √©:** Framework Python que simplifica a cria√ß√£o de aplica√ß√µes com IA  \n",
    "**Por que usar:** Em vez de escrever centenas de linhas de c√≥digo, usamos algumas linhas  \n",
    "**Analogia:** √â como usar um framework web (Django/Flask) em vez de programar HTTP do zero\n",
    "\n",
    "### 2. Ollama ü¶ô\n",
    "**O que √©:** Plataforma para rodar modelos de IA **localmente** no seu computador  \n",
    "**Por que usar:** \n",
    "- ‚úÖ Gratuito (sem pagar por API)\n",
    "- ‚úÖ Privado (seus dados n√£o v√£o para a nuvem)\n",
    "- ‚úÖ R√°pido (sem lat√™ncia de internet)\n",
    "\n",
    "**Modelos de embedding dispon√≠veis:**\n",
    "- `all-minilm`: 384 dimens√µes, r√°pido e leve\n",
    "- `nomic-embed-text`: 768 dimens√µes, balanceado\n",
    "- `mxbai-embed-large`: 1024 dimens√µes, mais preciso\n",
    "\n",
    "### 3. FAISS üöÄ\n",
    "**O que √©:** Biblioteca do Facebook/Meta para busca vetorial eficiente  \n",
    "**Por que usar:** Consegue buscar em **milh√µes de vetores** em milissegundos  \n",
    "**Analogia:** √â como um √≠ndice de banco de dados, mas para vetores matem√°ticos\n",
    "\n",
    "**Como funciona:**\n",
    "```text\n",
    "Texto ‚Üí [0.2, -0.5, 0.8, ...] ‚Üê Vetor com 384/768/1024 n√∫meros\n",
    "         ‚Üì\n",
    "      Armazenado no FAISS\n",
    "         ‚Üì\n",
    "Query ‚Üí [0.3, -0.4, 0.7, ...] ‚Üê Tamb√©m vira vetor\n",
    "         ‚Üì\n",
    "      FAISS compara dist√¢ncias\n",
    "         ‚Üì\n",
    "    Retorna os mais pr√≥ximos!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618018e7",
   "metadata": {},
   "source": [
    "## üéØ Cen√°rio do Experimento\n",
    "\n",
    "Vamos criar um **\"mini Google\"** com documentos de 3 categorias completamente diferentes:\n",
    "\n",
    "| Categoria | Documentos |\n",
    "|-----------|------------|\n",
    "| üñ•Ô∏è **Tecnologia** | iPhone 15, RTX 4090 |\n",
    "| üç∞ **Culin√°ria** | Receita de bolo, Lasanha |\n",
    "| ‚öΩ **Esportes** | Gol de futebol |\n",
    "\n",
    "### O Desafio\n",
    "\n",
    "Faremos uma pergunta que **n√£o cont√©m palavras exatas** dos documentos:\n",
    "\n",
    "**Pergunta:** \"Quero sugest√µes de hardware para computador ou celular\"\n",
    "\n",
    "**Expectativa:**\n",
    "- ‚úÖ Deve retornar: iPhone e RTX 4090 (s√£o hardware!)\n",
    "- ‚ùå N√ÉO deve retornar: Bolo, Lasanha, Gol (n√£o s√£o hardware)\n",
    "\n",
    "### Por que isso √© dif√≠cil?\n",
    "\n",
    "Um sistema de busca tradicional (Ctrl+F) falharia porque:\n",
    "- A palavra \"iPhone\" n√£o aparece na pergunta\n",
    "- A palavra \"RTX 4090\" n√£o aparece na pergunta\n",
    "- Mas nossa IA precisa **entender** que ambos s√£o hardware!\n",
    "\n",
    "üí° **Isso √© Intelig√™ncia Artificial em a√ß√£o!** O modelo aprendeu que \"iPhone = celular = hardware\" e \"RTX 4090 = placa de v√≠deo = hardware\" durante seu treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda77408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu>=1.13.1 langchain-openai==1.1.0  langchain-ollama==1.0.0 ollama==0.6.1 langchain-community>=0.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3bd061",
   "metadata": {},
   "source": [
    "## üì¶ Instala√ß√£o de Depend√™ncias\n",
    "\n",
    "Antes de come√ßar, precisamos instalar as bibliotecas necess√°rias:\n",
    "\n",
    "- `faiss-cpu`: Busca vetorial (vers√£o CPU)\n",
    "- `langchain-ollama`: Integra√ß√£o Ollama + LangChain\n",
    "- `langchain-community`: Utilit√°rios do LangChain\n",
    "- `ollama`: Cliente Python do Ollama\n",
    "\n",
    "‚ö†Ô∏è **Nota:** Se voc√™ j√° instalou essas bibliotecas, pode pular esta c√©lula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5da2c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Se preferir usar Ollama Embeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e005c1dc",
   "metadata": {},
   "source": [
    "## üîå Imports e Configura√ß√£o Inicial\n",
    "\n",
    "Importando as bibliotecas que vamos usar:\n",
    "\n",
    "- `os`: Para ler vari√°veis de ambiente\n",
    "- `requests`: Para verificar se o Ollama est√° rodando\n",
    "- `Path` e `load_dotenv`: Para carregar configura√ß√µes do arquivo `.env`\n",
    "- `FAISS`: O banco de dados vetorial\n",
    "- `OllamaEmbeddings`: O modelo que transforma texto em vetores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf197ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  .env n√£o encontrado. Defina as vari√°veis de ambiente manualmente.\n"
     ]
    }
   ],
   "source": [
    "# 2) Configura√ß√£o e carregamento do .env (simplificado)\n",
    "env_path = Path.cwd().joinpath('..', '..', '.env').resolve()\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f'üîé .env carregado -> {env_path.resolve()}')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  .env n√£o encontrado. Defina as vari√°veis de ambiente manualmente.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d96b7b2",
   "metadata": {},
   "source": [
    "## üîê Carregando Vari√°veis de Ambiente\n",
    "\n",
    "O arquivo `.env` cont√©m configura√ß√µes como:\n",
    "- URL do Ollama (`OLLAMA_API_URL`)\n",
    "- Chaves de API (se necess√°rio)\n",
    "\n",
    "**Por que usar `.env`?**\n",
    "- ‚úÖ N√£o expor senhas no c√≥digo\n",
    "- ‚úÖ F√°cil mudar entre ambientes (dev/prod)\n",
    "- ‚úÖ Compartilhar c√≥digo sem vazar credenciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc61b89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama est√° online! Modelos dispon√≠veis: 3\n",
      "   - all-minilm:latest (0.04 GB)\n",
      "   - mxbai-embed-large:latest (0.62 GB)\n",
      "   - nomic-embed-text:latest (0.26 GB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configura√ß√£o do Ollama (ajuste conforme necess√°rio)\n",
    "# - Dentro do Docker: http://ollama:11434\n",
    "# - Fora do Docker: http://localhost:11434\n",
    "OLLAMA_API_URL = os.getenv('OLLAMA_API_URL', 'http://localhost:11434')\n",
    "\n",
    "# Verificar se Ollama est√° online\n",
    "def check_ollama_health() -> bool:\n",
    "    try:\n",
    "        response = requests.get(f'{OLLAMA_API_URL}/api/tags', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            print(f'‚úÖ Ollama est√° online! Modelos dispon√≠veis: {len(models)}')\n",
    "            for model in models:\n",
    "                name = model.get('name', 'unknown')\n",
    "                size = model.get('size', 0) / (1024**3)  # Convert to GB\n",
    "                print(f'   - {name} ({size:.2f} GB)')\n",
    "            return True\n",
    "        else:\n",
    "            print(f'‚ùå Ollama retornou status {response.status_code}')\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Erro ao conectar com Ollama: {e}')\n",
    "        return False\n",
    "\n",
    "# Testar conex√£o\n",
    "check_ollama_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0113a6",
   "metadata": {},
   "source": [
    "## üè• Health Check do Ollama\n",
    "\n",
    "Antes de usar o Ollama, precisamos verificar se ele est√° rodando corretamente.\n",
    "\n",
    "**O que esta c√©lula faz:**\n",
    "1. Tenta conectar na URL do Ollama (padr√£o: `http://localhost:11434`)\n",
    "2. Lista todos os modelos dispon√≠veis\n",
    "3. Mostra o tamanho de cada modelo em GB\n",
    "\n",
    "**Poss√≠veis URLs:**\n",
    "- `http://localhost:11434` ‚Üí Se Ollama est√° rodando no seu computador\n",
    "- `http://ollama:11434` ‚Üí Se Ollama est√° rodando em Docker Compose\n",
    "\n",
    "üí° **Dica:** Se voc√™ ver ‚ùå, o Ollama n√£o est√° rodando. Inicie-o com `ollama serve` no terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbaf26e",
   "metadata": {},
   "source": [
    "## üìÑ Passo 1: Preparando os Documentos\n",
    "\n",
    "Aqui temos nosso **dataset de teste** ‚Äî 5 documentos de categorias bem distintas.\n",
    "\n",
    "**Por que categorias diferentes?**\n",
    "Queremos testar se a IA consegue distinguir contextos. Um bom modelo de embedding deve:\n",
    "- Colocar \"iPhone\" e \"RTX 4090\" pr√≥ximos (ambos s√£o tecnologia/hardware)\n",
    "- Colocar \"bolo\" e \"lasanha\" pr√≥ximos (ambos s√£o culin√°ria)\n",
    "- Manter \"gol\" distante de receitas e hardware (esporte √© outro contexto)\n",
    "\n",
    "### Estrutura dos Dados\n",
    "\n",
    "```python\n",
    "meus_textos = [\n",
    "    \"Documento sobre tecnologia...\",  # Categoria: Tech\n",
    "    \"Documento sobre culin√°ria...\",   # Categoria: Food\n",
    "    \"Documento sobre esporte...\",     # Categoria: Sport\n",
    "]\n",
    "```\n",
    "\n",
    "üí° **Conceito importante:** Esses textos s√£o chamados de **corpus** (conjunto de documentos que queremos buscar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da63cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meus_textos = [\n",
    "    \"O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.\",    # Tecnologia\n",
    "    \"Para fazer um bolo macio, bata as claras em neve.\",       # Culin√°ria\n",
    "    \"O atacante chutou a bola no √¢ngulo e foi gol.\",           # Esporte\n",
    "    \"A placa de v√≠deo RTX 4090 roda jogos em 4K.\",             # Tecnologia\n",
    "    \"Receita de lasanha √† bolonhesa com muito queijo.\"         # Culin√°ria\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aecab4d",
   "metadata": {},
   "source": [
    "## üß† Passo 2: Inicializando o Modelo de Embeddings\n",
    "\n",
    "Este √© o **\"c√©rebro\"** que transforma texto em vetores num√©ricos!\n",
    "\n",
    "### O que s√£o Embeddings?\n",
    "\n",
    "**Embedding** = Representa√ß√£o num√©rica de um texto\n",
    "\n",
    "Exemplo:\n",
    "```text\n",
    "Texto: \"gato\"\n",
    "Embedding: [0.12, -0.34, 0.56, 0.89, ..., 0.23]\n",
    "           ‚Üë vetor com 384 n√∫meros (all-minilm)\n",
    "```\n",
    "\n",
    "### Como funciona a magia?\n",
    "\n",
    "1. O modelo foi **treinado** em milh√µes de textos\n",
    "2. Aprendeu que \"gato\" e \"felino\" t√™m significados parecidos\n",
    "3. Ent√£o gera vetores **pr√≥ximos** para palavras com significados similares\n",
    "\n",
    "**Visualiza√ß√£o conceitual:**\n",
    "```text\n",
    "Espa√ßo vetorial (imagine em 3D, mas √© 384D!)\n",
    "\n",
    "  gato ‚Ä¢ ‚Üê pr√≥ximo de ‚Üí ‚Ä¢ felino\n",
    "  \n",
    "  carro ‚Ä¢ ‚Üê DISTANTE de ‚Üí ‚Ä¢ felino\n",
    "```\n",
    "\n",
    "### Modelos Dispon√≠veis\n",
    "\n",
    "| Modelo | Dimens√µes | Velocidade | Qualidade |\n",
    "|--------|-----------|------------|-----------|\n",
    "| `all-minilm` | 384 | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê |\n",
    "| `nomic-embed-text` | 768 | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê |\n",
    "| `mxbai-embed-large` | 1024 | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "\n",
    "**Qual escolher?**\n",
    "- Teste r√°pido/prot√≥tipo: `all-minilm`\n",
    "- Produ√ß√£o balanceada: `nomic-embed-text`\n",
    "- M√°xima precis√£o: `mxbai-embed-large`\n",
    "\n",
    "üí° **Nota:** Todos esses modelos rodam **localmente** e s√£o **gratuitos**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f41cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se preferir usar Ollama Embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    # model=\"mxbai-embed-large\",\n",
    "    # model=\"nomic-embed-text\",\n",
    "    model=\"all-minilm\",\n",
    "    base_url=OLLAMA_API_URL,\n",
    "    # Em vez de headers, use client kwargs (se necess√°rio para auth)\n",
    "    client_kwargs={\"headers\": {\"Authorization\": \"Bearer YOUR_TOKEN\"}} if os.getenv(\"OLLAMA_API_KEY\") else {},\n",
    "    # opcional: validar se o modelo existe localmente\n",
    "    validate_model_on_init=False,\n",
    "    # opcional: tempo de keep-alive\n",
    "    keep_alive=5 * 60,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce7ba2f",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Passo 3: Criando o Banco de Dados Vetorial (Indexa√ß√£o)\n",
    "\n",
    "Aqui acontece a **m√°gica**! Vamos transformar todos os textos em vetores e armazen√°-los no FAISS.\n",
    "\n",
    "### O que acontece nesta linha de c√≥digo?\n",
    "\n",
    "```python\n",
    "vector_store = FAISS.from_texts(meus_textos, embeddings)\n",
    "```\n",
    "\n",
    "**Passo a passo interno:**\n",
    "\n",
    "1. **Para cada texto** em `meus_textos`:\n",
    "   ```text\n",
    "   \"O novo iPhone 15...\" ‚Üí embeddings.embed_query() ‚Üí [0.2, -0.5, 0.8, ...]\n",
    "   \"Para fazer um bolo...\" ‚Üí embeddings.embed_query() ‚Üí [0.1, 0.3, -0.4, ...]\n",
    "   ...\n",
    "   ```\n",
    "\n",
    "2. **FAISS armazena** todos esses vetores em uma estrutura otimizada:\n",
    "   ```text\n",
    "   √çndice FAISS:\n",
    "   [0] ‚Üí [0.2, -0.5, 0.8, ...] (iPhone)\n",
    "   [1] ‚Üí [0.1, 0.3, -0.4, ...] (bolo)\n",
    "   [2] ‚Üí [-0.3, 0.7, 0.2, ...] (gol)\n",
    "   [3] ‚Üí [0.25, -0.48, 0.82, ...] (RTX 4090)\n",
    "   [4] ‚Üí [0.12, 0.28, -0.38, ...] (lasanha)\n",
    "   ```\n",
    "\n",
    "3. **Pronto!** Agora podemos fazer buscas ultrarr√°pidas\n",
    "\n",
    "### Por que FAISS √© r√°pido?\n",
    "\n",
    "FAISS usa algoritmos avan√ßados como:\n",
    "- **IVF** (Inverted File Index): Agrupa vetores similares\n",
    "- **HNSW** (Hierarchical Navigable Small World): Grafo para busca eficiente\n",
    "\n",
    "**Resultado:** Consegue buscar em milh√µes de vetores em milissegundos! ‚ö°\n",
    "\n",
    "### Analogia\n",
    "\n",
    "Imagine que voc√™ tem uma biblioteca com milh√µes de livros:\n",
    "- **Sem √≠ndice:** Voc√™ precisa ler cada livro para achar o que quer üò∞\n",
    "- **Com FAISS:** √â como ter um sistema Dewey Decimal que te leva direto ao livro certo! üìö‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "231e1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_texts(meus_textos, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846679db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Hora de Testar! Vamos Fazer Buscas\n",
    "\n",
    "Configura√ß√£o conclu√≠da! ‚úÖ  \n",
    "Agora vamos usar nosso sistema de busca sem√¢ntica.\n",
    "\n",
    "**O que fizemos at√© agora:**\n",
    "1. ‚úÖ Instalamos as bibliotecas\n",
    "2. ‚úÖ Conectamos ao Ollama\n",
    "3. ‚úÖ Criamos um modelo de embeddings\n",
    "4. ‚úÖ Indexamos nossos documentos no FAISS\n",
    "\n",
    "**Pr√≥ximo passo:** Fazer perguntas e ver a IA encontrar as respostas! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910e95e",
   "metadata": {},
   "source": [
    "## üîç Passo 4: Definindo a Pergunta (Query)\n",
    "\n",
    "Aqui est√° o **teste de fogo** para nosso sistema!\n",
    "\n",
    "### A Pergunta\n",
    "\n",
    "```python\n",
    "\"Quero sugest√µes de hardware para computador ou celular\"\n",
    "```\n",
    "\n",
    "### Por que esta pergunta √© desafiadora?\n",
    "\n",
    "**Palavras que N√ÉO aparecem na pergunta:**\n",
    "- ‚ùå \"iPhone\"\n",
    "- ‚ùå \"RTX\"\n",
    "- ‚ùå \"4090\"\n",
    "- ‚ùå \"placa de v√≠deo\"\n",
    "- ‚ùå \"lente perisc√≥pica\"\n",
    "\n",
    "**Palavras que SIM aparecem:**\n",
    "- ‚úÖ \"hardware\"\n",
    "- ‚úÖ \"computador\"\n",
    "- ‚úÖ \"celular\"\n",
    "\n",
    "### O Desafio\n",
    "\n",
    "Um sistema de busca tradicional (Ctrl+F) **falharia** porque:\n",
    "```text\n",
    "Busca por \"hardware\" ‚Üí N√£o encontra nada (palavra n√£o est√° nos docs)\n",
    "Busca por \"celular\" ‚Üí N√£o encontra \"iPhone\" (palavra diferente)\n",
    "Busca por \"computador\" ‚Üí N√£o encontra \"RTX 4090\" (palavra diferente)\n",
    "```\n",
    "\n",
    "### A Intelig√™ncia Artificial\n",
    "\n",
    "Nossa IA vai **entender** que:\n",
    "```text\n",
    "\"celular\" ‚âà \"iPhone\" ‚âà \"smartphone\"\n",
    "\"computador\" ‚âà \"PC\" ‚âà \"placa de v√≠deo\"\n",
    "\"hardware\" ‚âà \"equipamento\" ‚âà \"dispositivo\"\n",
    "```\n",
    "\n",
    "üí° **Isso √© aprendizado sem√¢ntico!** O modelo foi treinado para entender rela√ß√µes entre conceitos, n√£o apenas correspond√™ncia exata de strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "946d3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta = \"Quero sugest√µes de hardware para computador ou celular\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb8374",
   "metadata": {},
   "source": [
    "## üéØ Passo 5: Executando a Busca Sem√¢ntica\n",
    "\n",
    "Agora vamos **executar a busca** e ver os resultados!\n",
    "\n",
    "### O que significa `k=2`?\n",
    "\n",
    "```python\n",
    "vector_store.similarity_search(pergunta, k=2)\n",
    "```\n",
    "\n",
    "O par√¢metro `k` define quantos resultados queremos:\n",
    "- `k=1` ‚Üí Retorna apenas o documento mais similar\n",
    "- `k=2` ‚Üí Retorna os 2 documentos mais similares\n",
    "- `k=5` ‚Üí Retorna os 5 documentos mais similares\n",
    "\n",
    "### Como funciona internamente?\n",
    "\n",
    "1. **Sua pergunta vira um vetor:**\n",
    "   ```text\n",
    "   \"Quero sugest√µes de hardware...\" ‚Üí embeddings.embed_query() \n",
    "   ‚Üí [0.22, -0.48, 0.79, ...]\n",
    "   ```\n",
    "\n",
    "2. **FAISS calcula dist√¢ncias:**\n",
    "   ```text\n",
    "   Dist√¢ncia(query, iPhone) = 0.85\n",
    "   Dist√¢ncia(query, bolo) = 2.34\n",
    "   Dist√¢ncia(query, gol) = 2.67\n",
    "   Dist√¢ncia(query, RTX 4090) = 0.92\n",
    "   Dist√¢ncia(query, lasanha) = 2.41\n",
    "   ```\n",
    "\n",
    "3. **Ordena por proximidade:**\n",
    "   ```text\n",
    "   1¬∫ lugar: iPhone (0.85) ‚Üê Mais pr√≥ximo!\n",
    "   2¬∫ lugar: RTX 4090 (0.92)\n",
    "   3¬∫ lugar: lasanha (2.41)\n",
    "   ...\n",
    "   ```\n",
    "\n",
    "4. **Retorna os top-k:**\n",
    "   ```text\n",
    "   Como k=2, retorna: [iPhone, RTX 4090]\n",
    "   ```\n",
    "\n",
    "### M√©trica de Similaridade\n",
    "\n",
    "FAISS usa **similaridade cosseno** por padr√£o:\n",
    "- Mede o **√¢ngulo** entre dois vetores\n",
    "- Quanto menor o √¢ngulo, mais similares s√£o os textos\n",
    "- Funciona bem mesmo com vetores de dimens√µes diferentes\n",
    "\n",
    "**Visualiza√ß√£o conceitual (2D, mas √© 384D!):**\n",
    "\n",
    "![Dist√¢ncia Euclidiana](distancia_euclidiana.png)\n",
    "\n",
    "### O que esperar?\n",
    "\n",
    "‚úÖ **Esperado:** iPhone e RTX 4090 (hardware!)  \n",
    "‚ùå **N√£o esperado:** Bolo, Lasanha, Gol (n√£o s√£o hardware)\n",
    "\n",
    "Vamos ver se a IA acerta! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2110a5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: 'Quero sugest√µes de hardware para computador ou celular'\n",
      "\n",
      "--- Documentos Encontrados ---\n",
      "1. A placa de v√≠deo RTX 4090 roda jogos em 4K.\n",
      "2. Receita de lasanha √† bolonhesa com muito queijo.\n"
     ]
    }
   ],
   "source": [
    "resultados = vector_store.similarity_search(pergunta, k=2)\n",
    "\n",
    "print(f\"Pergunta: '{pergunta}'\\n\")\n",
    "print(\"--- Documentos Encontrados ---\")\n",
    "for i, doc in enumerate(resultados):\n",
    "    print(f\"{i+1}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94620ce4",
   "metadata": {},
   "source": [
    "## üìä An√°lise dos Resultados\n",
    "\n",
    "### O que observar?\n",
    "\n",
    "Quando voc√™ executar a c√©lula acima, observe:\n",
    "\n",
    "1. **Os documentos retornados s√£o relevantes?**\n",
    "   - ‚úÖ Se retornou iPhone e RTX 4090: O modelo entendeu!\n",
    "   - ‚ùå Se retornou receitas ou gol: O modelo falhou\n",
    "\n",
    "2. **A ordem faz sentido?**\n",
    "   - O primeiro resultado deve ser o mais relevante\n",
    "   - O segundo resultado deve ser um pouco menos relevante\n",
    "\n",
    "### Por que pode falhar?\n",
    "\n",
    "Modelos diferentes podem interpretar de formas diferentes:\n",
    "- `all-minilm` √© mais r√°pido, mas menos preciso\n",
    "- `nomic-embed-text` pode ter vi√©s de treinamento\n",
    "- `mxbai-embed-large` geralmente √© mais confi√°vel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230eb46",
   "metadata": {},
   "source": [
    "\n",
    "### üß™ Experimentos para Tentar\n",
    "\n",
    "#### Experimento 1: Mudar o modelo\n",
    "```python\n",
    "# Tente trocar na c√©lula de inicializa√ß√£o:\n",
    "model=\"nomic-embed-text\"  # ou \"mxbai-embed-large\"\n",
    "\n",
    "# E recriar o vector_store:\n",
    "vector_store = FAISS.from_texts(meus_textos, embeddings)\n",
    "\n",
    "# E refazer a busca\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905344b",
   "metadata": {},
   "source": [
    "\n",
    "#### Experimento 2: Testar outras queries\n",
    "```python\n",
    "# Query espec√≠fica\n",
    "pergunta = \"receitas de massas italianas\"\n",
    "# Deve retornar: lasanha\n",
    "\n",
    "# Query amb√≠gua\n",
    "pergunta = \"como melhorar performance\"\n",
    "# Vai retornar hardware ou esporte? ü§î\n",
    "\n",
    "# Query fora do dom√≠nio\n",
    "pergunta = \"viagens para a Europa\"\n",
    "# Vai retornar o que est√° \"menos distante\" (mas nada relevante)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd3972",
   "metadata": {},
   "source": [
    "\n",
    "#### Experimento 3: Ajustar k\n",
    "```python\n",
    "# Ver mais resultados\n",
    "resultados = vector_store.similarity_search(pergunta, k=5)\n",
    "# Agora voc√™ v√™ TODOS os 5 documentos ordenados por relev√¢ncia\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b499929",
   "metadata": {},
   "source": [
    "\n",
    "#### Experimento 4: Ver os scores\n",
    "```python\n",
    "# Usar similarity_search_with_score para ver as dist√¢ncias\n",
    "resultados = vector_store.similarity_search_with_score(pergunta, k=2)\n",
    "for doc, score in resultados:\n",
    "    print(f\"Score: {score:.4f} | Texto: {doc.page_content}\")\n",
    "\n",
    "# Score menor = mais similar!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a316ba",
   "metadata": {},
   "source": [
    "\n",
    "### üí° Conceitos-chave para lembrar\n",
    "\n",
    "1. **Embedding transforma texto em vetor** (lista de n√∫meros)\n",
    "2. **FAISS armazena vetores** e faz buscas r√°pidas\n",
    "3. **Similaridade = proximidade no espa√ßo vetorial**\n",
    "4. **Modelos diferentes = interpreta√ß√µes diferentes**\n",
    "5. **Sempre teste com dados do SEU dom√≠nio!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
