{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d79fb7e6",
   "metadata": {},
   "source": [
    "# üéØ Mini RAG: Persistindo e Reutilizando √çndices Vetoriais\n",
    "\n",
    "## O que vamos aprender?\n",
    "\n",
    "Neste notebook, voc√™ vai aprender a **salvar e carregar** √≠ndices vetoriais do FAISS, um passo essencial para construir aplica√ß√µes **RAG (Retrieval Augmented Generation)** em produ√ß√£o.\n",
    "\n",
    "### O que √© RAG?\n",
    "\n",
    "**RAG = Retrieval Augmented Generation**\n",
    "\n",
    "√â uma t√©cnica que combina:\n",
    "1. üîç **Busca vetorial** (o que fizemos nos labs anteriores)\n",
    "2. ü§ñ **LLM** (ChatGPT, Gemini, etc.) para gerar respostas\n",
    "\n",
    "**Fluxo RAG completo:**\n",
    "\n",
    "```text\n",
    "Pergunta do usu√°rio\n",
    "    ‚Üì\n",
    "Busca vetorial (FAISS) ‚Üí Encontra documentos relevantes\n",
    "    ‚Üì\n",
    "Contexto + Pergunta ‚Üí Enviado para LLM\n",
    "    ‚Üì\n",
    "LLM gera resposta baseada no contexto\n",
    "    ‚Üì\n",
    "Resposta final ao usu√°rio\n",
    "```\n",
    "\n",
    "### Por que persistir √≠ndices?\n",
    "\n",
    "**Problema sem persist√™ncia:**\n",
    "```python\n",
    "# Toda vez que voc√™ reinicia o programa:\n",
    "vector_store = FAISS.from_texts(meus_textos, embeddings)\n",
    "# ‚Üë Precisa recalcular TODOS os embeddings (caro e lento!)\n",
    "```\n",
    "\n",
    "**Solu√ß√£o com persist√™ncia:**\n",
    "```python\n",
    "# Uma vez:\n",
    "vector_store.save_local(\"meu_indice\")\n",
    "\n",
    "# Depois, sempre que precisar:\n",
    "vector_store = FAISS.load_local(\"meu_indice\", embeddings)\n",
    "# ‚Üë Instant√¢neo! N√£o recalcula nada\n",
    "```\n",
    "\n",
    "### Benef√≠cios\n",
    "\n",
    "‚úÖ **Economia:** N√£o paga API repetidamente  \n",
    "‚úÖ **Velocidade:** Carrega em milissegundos  \n",
    "‚úÖ **Escalabilidade:** √çndices podem ter milh√µes de documentos  \n",
    "‚úÖ **Produ√ß√£o:** Essencial para apps reais\n",
    "\n",
    "### O que faremos\n",
    "\n",
    "1. Criar um √≠ndice vetorial (como antes)\n",
    "2. **Salvar no disco** (novidade!)\n",
    "3. **Carregar do disco** (novidade!)\n",
    "4. Usar normalmente para buscas\n",
    "\n",
    "üí° **Analogia:** √â como salvar um jogo - voc√™ n√£o quer recome√ßar do zero toda vez!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd50fda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\01-projetos\\11-work\\11.34-engenharia-vetorial\\.venv\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.5) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b618f",
   "metadata": {},
   "source": [
    "## üì¶ Imports e Configura√ß√£o\n",
    "\n",
    "Bibliotecas necess√°rias:\n",
    "- `os`, `Path`, `load_dotenv`: Gerenciamento de arquivos e vari√°veis de ambiente\n",
    "- `FAISS`: Banco de dados vetorial (com suporte a salvar/carregar!)\n",
    "- `GoogleGenerativeAIEmbeddings`: API do Google para embeddings (gratuita!)\n",
    "\n",
    "### Por que Google Generative AI?\n",
    "\n",
    "Neste lab usamos Google em vez de OpenAI porque:\n",
    "- ‚úÖ **Tier gratuito generoso** (1500 requests/dia)\n",
    "- ‚úÖ **Qualidade excelente** (768 dimens√µes)\n",
    "- ‚úÖ **Sem cart√£o de cr√©dito** para come√ßar\n",
    "\n",
    "**Como obter API key:**\n",
    "1. Acesse [aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey)\n",
    "2. Clique \"Create API Key\"\n",
    "3. Adicione no `.env`: `GOOGLE_API_KEY=AIza...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25342ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé .env carregado -> E:\\01-projetos\\11-work\\11.34-engenharia-vetorial\\.env\n"
     ]
    }
   ],
   "source": [
    "# 2) Configura√ß√£o e carregamento do .env (simplificado)\n",
    "env_path = Path.cwd().joinpath('..', '..', '.env').resolve()\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f'üîé .env carregado -> {env_path.resolve()}')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  .env n√£o encontrado. Defina as vari√°veis de ambiente manualmente.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a216489",
   "metadata": {},
   "source": [
    "## üîê Carregando Vari√°veis de Ambiente\n",
    "\n",
    "Carrega a `GOOGLE_API_KEY` do arquivo `.env`.\n",
    "\n",
    "**Estrutura do `.env`:**\n",
    "```bash\n",
    "GOOGLE_API_KEY=AIzaSy...\n",
    "```\n",
    "\n",
    "Se voc√™ vir ‚ö†Ô∏è, crie o arquivo `.env` na raiz do projeto com sua API key do Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b68f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meus_textos = [\n",
    "    \"O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.\",    # Tecnologia\n",
    "    \"Para fazer um bolo macio, bata as claras em neve.\",       # Culin√°ria\n",
    "    \"O atacante chutou a bola no √¢ngulo e foi gol.\",           # Esporte\n",
    "    \"A placa de v√≠deo RTX 4090 roda jogos em 4K.\",             # Tecnologia\n",
    "    \"Receita de lasanha √† bolonhesa com muito queijo.\"         # Culin√°ria\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d49c948",
   "metadata": {},
   "source": [
    "## üìÑ Passo 1: Preparando os Documentos\n",
    "\n",
    "Nosso dataset de teste com 5 documentos de categorias diferentes.\n",
    "\n",
    "**Importante:** Em uma aplica√ß√£o RAG real, esses textos viriam de:\n",
    "- üìë PDFs de documenta√ß√£o\n",
    "- üåê Base de conhecimento da empresa\n",
    "- üí¨ FAQs e tickets de suporte\n",
    "- üìä Manuais t√©cnicos\n",
    "\n",
    "üí° **Conceito RAG:** Quanto mais documentos relevantes voc√™ indexar, melhor o LLM conseguir√° responder perguntas espec√≠ficas do seu dom√≠nio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2a4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb83bcd",
   "metadata": {},
   "source": [
    "## üß† Passo 2: Inicializando o Modelo de Embeddings\n",
    "\n",
    "Criamos uma inst√¢ncia do modelo de embeddings do Google.\n",
    "\n",
    "**Modelo:** `text-embedding-004`\n",
    "- **Dimens√µes:** 768 (menor que OpenAI, mas ainda excelente)\n",
    "- **Qualidade:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Estado da arte\n",
    "- **Custo:** Gratuito at√© 1500 requests/dia\n",
    "- **Velocidade:** ~100-200ms por request\n",
    "\n",
    "### Como funciona?\n",
    "\n",
    "```\n",
    "Texto ‚Üí üåê API Google ‚Üí Vetor [768 n√∫meros]\n",
    "```\n",
    "\n",
    "Cada texto ser√° transformado em um vetor de 768 dimens√µes que captura seu significado sem√¢ntico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aff7c7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_texts(meus_textos, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346983e6",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Passo 3: Criando o √çndice Vetorial\n",
    "\n",
    "Transformamos os textos em vetores e criamos o √≠ndice FAISS **em mem√≥ria**.\n",
    "\n",
    "### O que acontece internamente?\n",
    "\n",
    "```text\n",
    "Para cada texto em meus_textos:\n",
    "  1. Envia para API do Google\n",
    "  2. Recebe vetor de 768 dimens√µes\n",
    "  3. FAISS armazena o vetor em RAM\n",
    "\n",
    "Resultado:\n",
    "  vector_store = √çndice FAISS com 5 vetores em mem√≥ria\n",
    "```\n",
    "\n",
    "**Importante:** Neste ponto, o √≠ndice existe **apenas na RAM**! Se voc√™ fechar o programa, perde tudo.\n",
    "\n",
    "‚è±Ô∏è **Tempo:** ~1-2 segundos (5 chamadas √† API do Google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e8e3152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banco vetorial salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "FAISS_PATH = Path.cwd().joinpath('..', '..', 'data', 'meu_indice_faiss')\n",
    "\n",
    "vector_store.save_local(str(FAISS_PATH))\n",
    "print(\"Banco vetorial salvo com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8baffb",
   "metadata": {},
   "source": [
    "## üíæ Passo 4: Salvando o √çndice no Disco (PERSIST√äNCIA!)\n",
    "\n",
    "Esta √© a **parte mais importante** para aplica√ß√µes RAG em produ√ß√£o!\n",
    "\n",
    "### O que acontece aqui?\n",
    "\n",
    "```python\n",
    "vector_store.save_local(str(FAISS_PATH))\n",
    "```\n",
    "\n",
    "**Internamente:**\n",
    "1. FAISS cria uma pasta: `data/meu_indice_faiss/`\n",
    "2. Salva 2 arquivos:\n",
    "   - `index.faiss` ‚Üí Os vetores e estrutura de √≠ndice\n",
    "   - `index.pkl` ‚Üí Metadados (textos originais, etc.)\n",
    "\n",
    "### Estrutura de arquivos criada:\n",
    "\n",
    "```text\n",
    "data/\n",
    "‚îî‚îÄ‚îÄ meu_indice_faiss/\n",
    "    ‚îú‚îÄ‚îÄ index.faiss  ‚Üê ~50KB (5 vetores √ó 768 dims √ó 4 bytes)\n",
    "    ‚îî‚îÄ‚îÄ index.pkl    ‚Üê ~2KB (textos originais)\n",
    "```\n",
    "\n",
    "### Por que isso √© importante?\n",
    "\n",
    "**Sem persist√™ncia:**\n",
    "```python\n",
    "# Toda execu√ß√£o:\n",
    "vector_store = FAISS.from_texts(textos, embeddings)\n",
    "# ‚Üë 5 chamadas √† API + 1-2 segundos\n",
    "```\n",
    "\n",
    "**Com persist√™ncia:**\n",
    "```python\n",
    "# Apenas uma vez:\n",
    "vector_store.save_local(\"path\")\n",
    "\n",
    "# Depois, sempre:\n",
    "vector_store = FAISS.load_local(\"path\", embeddings)\n",
    "# ‚Üë 0 chamadas √† API + ~10ms\n",
    "```\n",
    "\n",
    "### Benef√≠cios em produ√ß√£o\n",
    "\n",
    "| M√©trica | Sem Persist√™ncia | Com Persist√™ncia |\n",
    "|---------|------------------|------------------|\n",
    "| **Tempo de startup** | Minutos (muitos docs) | Milissegundos |\n",
    "| **Custo de API** | Toda vez | Uma vez (na cria√ß√£o) |\n",
    "| **Escalabilidade** | Limitada | Milh√µes de docs |\n",
    "| **Confiabilidade** | Dependente de API | Funciona offline |\n",
    "\n",
    "üí° **Analogia:** √â como salvar um documento do Word - voc√™ n√£o quer redigitar tudo cada vez que abrir!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36b94efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARREGAR do disco\n",
    "# O par√¢metro 'allow_dangerous_deserialization' √© necess√°rio em vers√µes recentes\n",
    "# para confirmar que voc√™ confia no arquivo que est√° carregando.\n",
    "novo_db = FAISS.load_local(\n",
    "    str(FAISS_PATH), \n",
    "    embeddings, \n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71522f",
   "metadata": {},
   "source": [
    "## üìÇ Passo 5: Carregando o √çndice do Disco\n",
    "\n",
    "Agora vamos **carregar** o √≠ndice que salvamos anteriormente!\n",
    "\n",
    "### O que acontece aqui?\n",
    "\n",
    "```python\n",
    "novo_db = FAISS.load_local(str(FAISS_PATH), embeddings, allow_dangerous_deserialization=True)\n",
    "```\n",
    "\n",
    "**Internamente:**\n",
    "1. FAISS l√™ os arquivos da pasta `data/meu_indice_faiss/`\n",
    "2. Reconstr√≥i o √≠ndice em mem√≥ria (~10ms)\n",
    "3. Pronto para uso instant√¢neo!\n",
    "\n",
    "### Par√¢metros importantes\n",
    "\n",
    "**`embeddings`**: Necess√°rio para futuras queries\n",
    "- N√£o recalcula nada dos documentos existentes\n",
    "- S√≥ ser√° usado quando voc√™ buscar algo novo\n",
    "\n",
    "**`allow_dangerous_deserialization=True`**: Confirma√ß√£o de seguran√ßa\n",
    "- FAISS usa pickle para serializar\n",
    "- Pickle pode executar c√≥digo malicioso se vier de fonte n√£o confi√°vel\n",
    "- Como voc√™ mesmo criou o √≠ndice, √© seguro ‚úÖ\n",
    "\n",
    "### Fluxo completo de uma aplica√ß√£o RAG\n",
    "\n",
    "```text\n",
    "Primeira execu√ß√£o (setup):\n",
    "  1. Carregar documentos\n",
    "  2. Criar embeddings (via API)\n",
    "  3. Criar √≠ndice FAISS\n",
    "  4. Salvar no disco ‚Üê Voc√™ est√° aqui!\n",
    "\n",
    "Execu√ß√µes subsequentes (produ√ß√£o):\n",
    "  5. Carregar √≠ndice do disco ‚Üê Estamos fazendo isso agora!\n",
    "  6. Receber query do usu√°rio\n",
    "  7. Buscar documentos relevantes\n",
    "  8. Enviar contexto + query para LLM\n",
    "  9. Retornar resposta\n",
    "```\n",
    "\n",
    "### Performance\n",
    "\n",
    "| Opera√ß√£o | Tempo (5 docs) | Tempo (10k docs) | Tempo (1M docs) |\n",
    "|----------|----------------|------------------|-----------------|\n",
    "| **Criar √≠ndice** | ~2s | ~30s | ~30min |\n",
    "| **Salvar** | ~50ms | ~200ms | ~5s |\n",
    "| **Carregar** | ~10ms | ~100ms | ~2s |\n",
    "| **Buscar** | <1ms | ~5ms | ~50ms |\n",
    "\n",
    "üí° **Observe:** Carregar √© **muito mais r√°pido** que criar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97fc2403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Usar normalmente\n",
    "consulta = \"Sugest√£o de celular\"\n",
    "resultados = novo_db.similarity_search(consulta, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc578291",
   "metadata": {},
   "source": [
    "## üîç Passo 6: Usando o √çndice Carregado\n",
    "\n",
    "Agora vamos fazer uma busca usando o √≠ndice que **carregamos do disco**!\n",
    "\n",
    "### A Query\n",
    "\n",
    "```python\n",
    "consulta = \"Sugest√£o de celular\"\n",
    "```\n",
    "\n",
    "**Desafio sem√¢ntico:**\n",
    "- A palavra \"iPhone\" n√£o aparece na query\n",
    "- A palavra \"celular\" sim\n",
    "- O modelo precisa entender que iPhone √© um celular\n",
    "\n",
    "### O que acontece internamente?\n",
    "\n",
    "1. **Query vira vetor (via API Google):**\n",
    "   ```text\n",
    "   \"Sugest√£o de celular\" ‚Üí üåê API call ‚Üí [0.15, -0.32, ..., 0.47]\n",
    "   ```\n",
    "\n",
    "2. **FAISS busca (local, sem API):**\n",
    "   ```text\n",
    "   Compara o vetor da query com os 5 vetores armazenados\n",
    "   Calcula dist√¢ncias:\n",
    "     - iPhone: 0.3 ‚Üê Mais pr√≥ximo!\n",
    "     - RTX 4090: 1.2\n",
    "     - Bolo: 2.5\n",
    "     - Gol: 2.8\n",
    "     - Lasanha: 2.6\n",
    "   ```\n",
    "\n",
    "3. **Retorna top-k (k=2):**\n",
    "   ```\n",
    "   [iPhone, RTX 4090]\n",
    "   ```\n",
    "\n",
    "### Par√¢metro k\n",
    "\n",
    "- `k=1`: Retorna apenas o mais similar\n",
    "- `k=2`: Retorna os 2 mais similares\n",
    "- `k=5`: Retorna todos (nosso dataset tem 5)\n",
    "\n",
    "üí° **Dica:** Em RAG, geralmente usa-se `k=3` ou `k=5` para dar contexto suficiente ao LLM sem sobrecarregar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b97274e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.\n"
     ]
    }
   ],
   "source": [
    "print(resultados[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecbe31",
   "metadata": {},
   "source": [
    "## üìä Passo 7: Visualizando o Resultado\n",
    "\n",
    "Vamos ver o documento mais relevante (√≠ndice 0 = primeiro resultado).\n",
    "\n",
    "**Esperado:** Deve retornar algo sobre iPhone, pois √© o documento sobre celular!\n",
    "\n",
    "### Interpretando o resultado\n",
    "\n",
    "Se voc√™ ver:\n",
    "- ‚úÖ `\"O novo iPhone 15...\"` ‚Üí Perfeito! A busca funcionou\n",
    "- ‚ùå Qualquer outro texto ‚Üí Algo deu errado (improv√°vel com Google)\n",
    "\n",
    "### Pr√≥ximo passo: RAG completo\n",
    "\n",
    "Em um RAG de verdade, voc√™ faria:\n",
    "\n",
    "```python\n",
    "# 1. Buscar documentos relevantes (j√° fizemos!)\n",
    "resultados = novo_db.similarity_search(consulta, k=3)\n",
    "\n",
    "# 2. Montar o contexto\n",
    "contexto = \"\\n\".join([doc.page_content for doc in resultados])\n",
    "\n",
    "# 3. Criar prompt para LLM\n",
    "prompt = f\"\"\"\n",
    "Baseado no seguinte contexto:\n",
    "{contexto}\n",
    "\n",
    "Responda a pergunta: {consulta}\n",
    "\"\"\"\n",
    "\n",
    "# 4. Enviar para LLM (Gemini, GPT, etc.)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "resposta = llm.invoke(prompt)\n",
    "\n",
    "print(resposta.content)\n",
    "# ‚Üí \"Eu recomendo o iPhone 15, que possui uma lente perisc√≥pica incr√≠vel...\"\n",
    "```\n",
    "\n",
    "üí° **Esse √© o poder do RAG:** O LLM responde com base nos **seus** documentos, n√£o apenas no conhecimento geral!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e46967",
   "metadata": {},
   "source": [
    "## üéì Resumo e Conceitos-Chave\n",
    "\n",
    "### O que aprendemos\n",
    "\n",
    "‚úÖ **Persist√™ncia de √≠ndices** - Salvar e carregar FAISS do disco  \n",
    "‚úÖ **Economia de API** - N√£o recalcular embeddings toda vez  \n",
    "‚úÖ **Performance** - Carregar √© 100x mais r√°pido que criar  \n",
    "‚úÖ **Fundamentos de RAG** - Base para aplica√ß√µes com LLM\n",
    "\n",
    "### Fluxo completo que implementamos\n",
    "\n",
    "```text\n",
    "1. Criar embeddings (via API Google)\n",
    "   ‚Üì\n",
    "2. Criar √≠ndice FAISS (em RAM)\n",
    "   ‚Üì\n",
    "3. Salvar no disco (.save_local) ‚Üê PERSIST√äNCIA!\n",
    "   ‚Üì\n",
    "4. Carregar do disco (.load_local) ‚Üê REUTILIZA√á√ÉO!\n",
    "   ‚Üì\n",
    "5. Fazer buscas normalmente\n",
    "```\n",
    "\n",
    "### Diferen√ßas: Labs anteriores vs. Este\n",
    "\n",
    "| Aspecto | Labs 1.4-1.6 | Este Lab (1.7) |\n",
    "|---------|--------------|----------------|\n",
    "| **√çndice** | S√≥ em RAM | Salvo em disco |\n",
    "| **Reiniciar programa** | Perde tudo | Mant√©m tudo |\n",
    "| **Startup** | Lento (recria) | R√°pido (carrega) |\n",
    "| **Produ√ß√£o** | ‚ùå Invi√°vel | ‚úÖ Pronto |\n",
    "\n",
    "### Aplica√ß√µes reais de RAG\n",
    "\n",
    "ü§ñ **Chatbots corporativos**\n",
    "- Indexa documenta√ß√£o interna\n",
    "- LLM responde baseado nos docs da empresa\n",
    "\n",
    "üìö **Assistentes de estudo**\n",
    "- Indexa livros, apostilas, anota√ß√µes\n",
    "- LLM explica conceitos baseado no material\n",
    "\n",
    "üîß **Suporte t√©cnico**\n",
    "- Indexa manuais, FAQs, tickets antigos\n",
    "- LLM sugere solu√ß√µes baseadas em casos similares\n",
    "\n",
    "üè• **Assist√™ncia m√©dica**\n",
    "- Indexa prontu√°rios, estudos cient√≠ficos\n",
    "- LLM auxilia diagn√≥sticos (com supervis√£o humana!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc2c64",
   "metadata": {},
   "source": [
    "\n",
    "### üß™ Experimentos para tentar\n",
    "\n",
    "#### Experimento 1: Adicionar mais documentos\n",
    "```python\n",
    "novos_textos = [\n",
    "    \"Samsung Galaxy S23 tem c√¢mera de 200MP\",\n",
    "    \"Xiaomi Redmi Note 12 √© um bom custo-benef√≠cio\",\n",
    "]\n",
    "\n",
    "# Criar novo √≠ndice com todos os documentos\n",
    "todos_textos = meus_textos + novos_textos\n",
    "vector_store = FAISS.from_texts(todos_textos, embeddings)\n",
    "vector_store.save_local(str(FAISS_PATH))\n",
    "\n",
    "# Agora tem 7 documentos!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870205e",
   "metadata": {},
   "source": [
    "\n",
    "#### Experimento 2: Atualizar √≠ndice existente\n",
    "```python\n",
    "# Carregar √≠ndice existente\n",
    "db = FAISS.load_local(str(FAISS_PATH), embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Adicionar novos documentos\n",
    "novos_docs = [\"Novo documento...\"]\n",
    "db.add_texts(novos_docs)\n",
    "\n",
    "# Salvar novamente (agora com os novos docs)\n",
    "db.save_local(str(FAISS_PATH))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e0a09",
   "metadata": {},
   "source": [
    "\n",
    "#### Experimento 3: M√∫ltiplos √≠ndices\n",
    "```python\n",
    "# √çndice para tecnologia\n",
    "tech_db = FAISS.from_texts(textos_tech, embeddings)\n",
    "tech_db.save_local(\"indices/tecnologia\")\n",
    "\n",
    "# √çndice para culin√°ria\n",
    "food_db = FAISS.from_texts(textos_culinaria, embeddings)\n",
    "food_db.save_local(\"indices/culinaria\")\n",
    "\n",
    "# Carregar conforme necess√°rio\n",
    "tech = FAISS.load_local(\"indices/tecnologia\", embeddings, allow_dangerous_deserialization=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8717bb",
   "metadata": {},
   "source": [
    "\n",
    "#### Experimento 4: RAG completo (desafio!)\n",
    "```python\n",
    "# Combine tudo que aprendeu:\n",
    "# 1. Carregar √≠ndice\n",
    "# 2. Fazer busca\n",
    "# 3. Enviar contexto para LLM (Gemini)\n",
    "# 4. Retornar resposta gerada\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def rag_query(pergunta, k=3):\n",
    "    # Buscar documentos relevantes\n",
    "    db = FAISS.load_local(str(FAISS_PATH), embeddings, allow_dangerous_deserialization=True)\n",
    "    docs = db.similarity_search(pergunta, k=k)\n",
    "    \n",
    "    # Montar contexto\n",
    "    contexto = \"\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    # Prompt para LLM\n",
    "    prompt = f\"Contexto:\\n{contexto}\\n\\nPergunta: {pergunta}\\nResposta:\"\n",
    "    \n",
    "    # Gerar resposta\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "    resposta = llm.invoke(prompt)\n",
    "    \n",
    "    return resposta.content\n",
    "\n",
    "# Testar\n",
    "print(rag_query(\"Qual √© o melhor celular?\"))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d562b64c",
   "metadata": {},
   "source": [
    "\n",
    "### üí° Boas pr√°ticas em produ√ß√£o\n",
    "\n",
    "1. **Versionamento de √≠ndices**\n",
    "   ```python\n",
    "   vector_store.save_local(\"indices/v1.0.0\")\n",
    "   vector_store.save_local(\"indices/v1.1.0\")  # Ap√≥s adicionar docs\n",
    "   ```\n",
    "\n",
    "2. **Backup autom√°tico**\n",
    "   ```python\n",
    "   import shutil\n",
    "   from datetime import datetime\n",
    "   \n",
    "   backup_path = f\"backups/index_{datetime.now().strftime('%Y%m%d')}\"\n",
    "   shutil.copytree(FAISS_PATH, backup_path)\n",
    "   ```\n",
    "\n",
    "3. **Metadados**\n",
    "   ```python\n",
    "   # Salvar informa√ß√µes sobre o √≠ndice\n",
    "   metadata = {\n",
    "       \"created_at\": datetime.now(),\n",
    "       \"num_docs\": len(meus_textos),\n",
    "       \"model\": \"text-embedding-004\",\n",
    "       \"dimensions\": 768\n",
    "   }\n",
    "   \n",
    "   import json\n",
    "   with open(FAISS_PATH / \"metadata.json\", \"w\") as f:\n",
    "       json.dump(metadata, f)\n",
    "   ```\n",
    "\n",
    "4. **Monitoramento**\n",
    "   ```python\n",
    "   import os\n",
    "   \n",
    "   # Verificar tamanho do √≠ndice\n",
    "   index_size = os.path.getsize(FAISS_PATH / \"index.faiss\")\n",
    "   print(f\"Tamanho do √≠ndice: {index_size / 1024:.2f} KB\")\n",
    "   ```\n",
    "\n",
    "üí° **Dica final:** O RAG √© a t√©cnica mais popular para fazer LLMs responderem com informa√ß√µes espec√≠ficas do seu neg√≥cio. Domine isso e voc√™ ter√° um skill muito valorizado no mercado!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "11-34-engenharia-vetorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
