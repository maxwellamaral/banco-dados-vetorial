{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b325f0f",
   "metadata": {},
   "source": [
    "# üî¨ Laborat√≥rio: Compara√ß√£o de Modelos de Embeddings do Ollama\n",
    "\n",
    "## Objetivo do Notebook\n",
    "\n",
    "Neste laborat√≥rio, voc√™ aprender√° a **comparar diferentes modelos de embeddings** dispon√≠veis via Ollama. O foco √© entender:\n",
    "\n",
    "- **O que s√£o embeddings** e por que s√£o fundamentais para busca sem√¢ntica\n",
    "- **Como diferentes modelos** (all-minilm, nomic-embed-text, mxbai-embed-large) produzem resultados distintos\n",
    "- **Impacto da dimensionalidade** dos vetores no desempenho e precis√£o\n",
    "- **Como interpretar scores de similaridade** e escolher o modelo adequado\n",
    "\n",
    "### Por que comparar modelos?\n",
    "\n",
    "Cada modelo de embedding tem caracter√≠sticas √∫nicas:\n",
    "- **Dimensionalidade**: modelos com mais dimens√µes podem capturar nuances, mas consomem mais recursos\n",
    "- **Velocidade**: modelos menores s√£o mais r√°pidos\n",
    "- **Qualidade sem√¢ntica**: alguns modelos entendem melhor contextos espec√≠ficos (t√©cnico, casual, multil√≠ngue)\n",
    "\n",
    "üí° **Exemplo pr√°tico**: Se voc√™ est√° construindo um chatbot t√©cnico, precisa saber qual modelo melhor entende jarg√µes da sua √°rea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b34f0e4",
   "metadata": {},
   "source": [
    "## Pr√©-requisitos e Ferramentas\n",
    "\n",
    "Para realizar as compara√ß√µes, utilizaremos:\n",
    "\n",
    "1. **LangChain**: Framework que facilita a integra√ß√£o com diferentes modelos de embeddings\n",
    "2. **Ollama**: Plataforma para rodar modelos localmente (sem custos de API externa)\n",
    "3. **FAISS**: Biblioteca de busca vetorial eficiente (criada pelo Facebook/Meta)\n",
    "\n",
    "### Por que Ollama?\n",
    "\n",
    "‚úÖ **Gratuito** - N√£o h√° custos por requisi√ß√£o  \n",
    "‚úÖ **Privacidade** - Seus dados n√£o saem do seu computador  \n",
    "‚úÖ **Variedade** - Diversos modelos dispon√≠veis para teste  \n",
    "‚úÖ **Rapidez** - Lat√™ncia baixa (sem lat√™ncia de rede para APIs externas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618018e7",
   "metadata": {},
   "source": [
    "## üìä Dataset de Teste\n",
    "\n",
    "Vamos usar documentos de **tr√™s categorias distintas** para testar a capacidade dos modelos de distinguir contextos:\n",
    "\n",
    "- **Tecnologia** (hardware, gadgets)\n",
    "- **Culin√°ria** (receitas, t√©cnicas)\n",
    "- **Esportes** (futebol, competi√ß√µes)\n",
    "\n",
    "**Por que isso importa?** Modelos de qualidade devem conseguir identificar que \"iPhone\" e \"RTX 4090\" est√£o mais pr√≥ximos semanticamente entre si do que com \"bolo\" ou \"gol\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda77408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu>=1.13.1 langchain-openai==1.1.0  langchain-ollama==1.0.0 ollama==0.6.1 langchain-community>=0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5da2c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Se preferir usar Ollama Embeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf197ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  .env n√£o encontrado. Defina as vari√°veis de ambiente manualmente.\n"
     ]
    }
   ],
   "source": [
    "# 2) Configura√ß√£o e carregamento do .env (simplificado)\n",
    "env_path = Path.cwd().joinpath('..', '..', '.env').resolve()\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f'üîé .env carregado -> {env_path.resolve()}')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  .env n√£o encontrado. Defina as vari√°veis de ambiente manualmente.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc61b89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama est√° online! Modelos dispon√≠veis: 3\n",
      "   - all-minilm:latest (0.04 GB)\n",
      "   - mxbai-embed-large:latest (0.62 GB)\n",
      "   - nomic-embed-text:latest (0.26 GB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configura√ß√£o do Ollama (ajuste conforme necess√°rio)\n",
    "# - Dentro do Docker: http://ollama:11434\n",
    "# - Fora do Docker: http://localhost:11434\n",
    "OLLAMA_API_URL = os.getenv('OLLAMA_API_URL', 'http://localhost:11434')\n",
    "\n",
    "# Verificar se Ollama est√° online\n",
    "def check_ollama_health() -> bool:\n",
    "    try:\n",
    "        response = requests.get(f'{OLLAMA_API_URL}/api/tags', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            print(f'‚úÖ Ollama est√° online! Modelos dispon√≠veis: {len(models)}')\n",
    "            for model in models:\n",
    "                name = model.get('name', 'unknown')\n",
    "                size = model.get('size', 0) / (1024**3)  # Convert to GB\n",
    "                print(f'   - {name} ({size:.2f} GB)')\n",
    "            return True\n",
    "        else:\n",
    "            print(f'‚ùå Ollama retornou status {response.status_code}')\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Erro ao conectar com Ollama: {e}')\n",
    "        return False\n",
    "\n",
    "# Testar conex√£o\n",
    "check_ollama_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbaf26e",
   "metadata": {},
   "source": [
    "## Nossos \"Documentos\" (Textos crus)\n",
    "\n",
    "Note que s√£o t√≥picos bem diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da63cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meus_textos = [\n",
    "    \"O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.\",    # Tecnologia\n",
    "    \"Para fazer um bolo macio, bata as claras em neve.\",       # Culin√°ria\n",
    "    \"O atacante chutou a bola no √¢ngulo e foi gol.\",           # Esporte\n",
    "    \"A placa de v√≠deo RTX 4090 roda jogos em 4K.\",             # Tecnologia\n",
    "    \"Receita de lasanha √† bolonhesa com muito queijo.\"         # Culin√°ria\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2598ab1",
   "metadata": {},
   "source": [
    "## üß™ Compara√ß√£o de Modelos do Ollama\n",
    "\n",
    "Agora vem a parte central do laborat√≥rio! Vamos testar **3 modelos populares** e comparar seus resultados.\n",
    "\n",
    "### üìñ Conceitos Fundamentais\n",
    "\n",
    "#### O que √© um Embedding?\n",
    "\n",
    "Um **embedding** √© uma representa√ß√£o num√©rica (vetor) de um texto. Por exemplo:\n",
    "\n",
    "```\n",
    "Texto: \"gato\"\n",
    "Embedding: [0.12, -0.34, 0.56, ..., 0.78]  ‚Üê vetor com 768 n√∫meros\n",
    "```\n",
    "\n",
    "**Por que isso √© √∫til?** Textos semanticamente similares geram vetores pr√≥ximos no espa√ßo matem√°tico.\n",
    "\n",
    "**Exemplo intuitivo:**\n",
    "- `\"gato\"` e `\"felino\"` ‚Üí vetores pr√≥ximos (conceitos similares)\n",
    "- `\"gato\"` e `\"carro\"` ‚Üí vetores distantes (conceitos diferentes)\n",
    "\n",
    "#### O que √© Dimensionalidade?\n",
    "\n",
    "√â o **n√∫mero de componentes** do vetor:\n",
    "\n",
    "- **all-minilm**: 384 dimens√µes (r√°pido, leve)\n",
    "- **nomic-embed-text**: 768 dimens√µes (balanceado)\n",
    "- **mxbai-embed-large**: 1024 dimens√µes (mais rico, mais lento)\n",
    "\n",
    "**Analogia**: Pense em dimens√µes como \"caracter√≠sticas\" que descrevem o texto:\n",
    "- 384 dimens√µes = descrever uma pessoa com 384 adjetivos\n",
    "- 1024 dimens√µes = descrever com 1024 adjetivos (mais detalhado, mas mais pesado)\n",
    "\n",
    "#### Como funciona a Busca?\n",
    "\n",
    "1. **Indexa√ß√£o**: Todos os textos viram vetores e s√£o armazenados no FAISS\n",
    "2. **Query**: Sua pergunta tamb√©m vira um vetor\n",
    "3. **Compara√ß√£o**: FAISS calcula a dist√¢ncia entre o vetor da pergunta e todos os vetores armazenados\n",
    "4. **Ranking**: Retorna os k documentos mais pr√≥ximos (menor dist√¢ncia = mais similar)\n",
    "\n",
    "**M√©tricas de similaridade:**\n",
    "- **Dist√¢ncia Euclidiana**: dist√¢ncia \"em linha reta\" entre vetores\n",
    "- **Similaridade Cosseno**: √¢ngulo entre vetores (usado pelo FAISS por padr√£o)\n",
    "\n",
    "**Score:** Quanto **menor** o score, **mais similar** o documento (pense em \"dist√¢ncia\")\n",
    "\n",
    "### üéØ O que vamos testar?\n",
    "\n",
    "Faremos a **mesma pergunta** para os 3 modelos e compararemos:\n",
    "- Quais documentos cada modelo considera mais relevantes\n",
    "- Os scores de similaridade\n",
    "- A dimensionalidade dos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3a8bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√µes para comparar modelos Ollama (langchain_ollama x langchain_community)\n",
    "from typing import List, Dict, Any\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Tentamos importar ambas as implementa√ß√µes (preferir langchain_ollama)\n",
    "try:\n",
    "    from langchain_ollama import OllamaEmbeddings as LC_OllamaEmbeddings\n",
    "except Exception:\n",
    "    LC_OllamaEmbeddings = None\n",
    "\n",
    "try:\n",
    "    from langchain_community.embeddings import OllamaEmbeddings as CommunityOllamaEmbeddings\n",
    "except Exception:\n",
    "    CommunityOllamaEmbeddings = None\n",
    "\n",
    "def build_embeddings_for_model(model_name: str, base_url: str = OLLAMA_API_URL):\n",
    "    \"\"\"Tenta criar uma inst√¢ncia de embeddings para o modelo especificado.\n",
    "    Retorna (embeddings_instance, implementation_name).\n",
    "    \"\"\"\n",
    "    client_kwargs = {\"headers\": {\"Authorization\": f\"Bearer {os.getenv('OLLAMA_API_KEY')}\"}} if os.getenv('OLLAMA_API_KEY') else {}\n",
    "    # 1) Preferir langchain_ollama\n",
    "    if LC_OllamaEmbeddings is not None:\n",
    "        try:\n",
    "            emb = LC_OllamaEmbeddings(\n",
    "                model=model_name,\n",
    "                base_url=base_url,\n",
    "                client_kwargs=client_kwargs,\n",
    "                validate_model_on_init=False,\n",
    "                keep_alive=5 * 60,\n",
    "            )\n",
    "            return emb, 'langchain_ollama'\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    else:\n",
    "        last_err = None\n",
    "    # 2) Tentar community implementation\n",
    "    if CommunityOllamaEmbeddings is not None:\n",
    "        try:\n",
    "            emb = CommunityOllamaEmbeddings(\n",
    "                model=model_name,\n",
    "                base_url=base_url,\n",
    "                embed_instruction=\"\",\n",
    "                query_instruction=\"\",\n",
    "                headers=client_kwargs.get(\"headers\") if client_kwargs else None,\n",
    "                show_progress=False,\n",
    "            )\n",
    "            return emb, 'langchain_community'\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise Exception(f'N√£o foi poss√≠vel instanciar embeddings para {model_name}. Erro: {last_err}')\n",
    "\n",
    "def compare_models(models: List[str], texts: List[str], query: str, k: int = 2, base_url: str = OLLAMA_API_URL) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Compara uma lista de modelos: para cada modelo, instancia embeddings, constr√≥i um FAISS index e executa uma busca.\n",
    "    Retorna lista de dicion√°rios com model, implementacao, dim e hits (texto + score) ou error.\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    for m in models:\n",
    "        try:\n",
    "            emb, impl = build_embeddings_for_model(m, base_url=base_url)\n",
    "            # conferindo a dimens√£o do embedding (usa embed_query como exemplo)\n",
    "            emb_example = emb.embed_query(\"teste de dimens√£o\")\n",
    "            dim = len(emb_example)\n",
    "            # construir √≠ndice FAISS e buscar\n",
    "            vstore = FAISS.from_texts(texts, emb)\n",
    "            hits = vstore.similarity_search_with_score(query, k=k)\n",
    "            formatted_hits = [{\"text\": doc.page_content, \"score\": score} for doc, score in hits]\n",
    "            report.append({\"model\": m, \"impl\": impl, \"dim\": dim, \"hits\": formatted_hits})\n",
    "        except Exception as e:\n",
    "            report.append({\"model\": m, \"error\": str(e)})\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f017c716",
   "metadata": {},
   "source": [
    "### üîß Fun√ß√µes Auxiliares\n",
    "\n",
    "As fun√ß√µes abaixo automatizam o processo de compara√ß√£o:\n",
    "\n",
    "- **`build_embeddings_for_model()`**: Tenta instanciar um modelo de embeddings\n",
    "  - Prioriza `langchain_ollama` (mais recente)\n",
    "  - Faz fallback para `langchain_community` se necess√°rio\n",
    "  \n",
    "- **`compare_models()`**: Para cada modelo:\n",
    "  1. Cria uma inst√¢ncia de embeddings\n",
    "  2. Verifica a dimensionalidade (com um embed de teste)\n",
    "  3. Constr√≥i um √≠ndice FAISS com os textos\n",
    "  4. Executa a busca e retorna os top-k resultados\n",
    "\n",
    "**Por que duas implementa√ß√µes?** O LangChain est√° em transi√ß√£o: `langchain_ollama` √© o pacote moderno, mas `langchain_community` ainda √© amplamente usado. Nossa fun√ß√£o suporta ambos para m√°xima compatibilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b66ca",
   "metadata": {},
   "source": [
    "### üîç Definindo a Query de Teste\n",
    "\n",
    "Vamos usar uma pergunta que **n√£o cont√©m palavras-chave exatas** dos documentos. Isso for√ßa os modelos a entender o **significado sem√¢ntico**.\n",
    "\n",
    "**Pergunta:** \"Quero sugest√µes de hardware para computador ou celular\"\n",
    "\n",
    "**Desafio para o modelo:**\n",
    "- A palavra \"iPhone\" n√£o aparece na pergunta\n",
    "- A palavra \"RTX 4090\" n√£o aparece na pergunta\n",
    "- Mas ambos s√£o **hardware** para **celular** e **computador**\n",
    "\n",
    "Um modelo de qualidade deve:\n",
    "‚úÖ Retornar os documentos sobre iPhone e RTX 4090  \n",
    "‚ùå N√ÉO retornar documentos sobre receitas ou esportes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e33ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query de teste: 'Quero sugest√µes de hardware para computador ou celular'\n"
     ]
    }
   ],
   "source": [
    "pergunta = \"Quero sugest√µes de hardware para computador ou celular\"\n",
    "print(f\"Query de teste: '{pergunta}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b76a10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ INICIANDO COMPARA√á√ÉO DE MODELOS\n",
      "============================================================\n",
      "Modelos a comparar: all-minilm, nomic-embed-text, mxbai-embed-large\n",
      "Query: \"Quero sugest√µes de hardware para computador ou celular\"\n",
      "Documentos no √≠ndice: 5\n",
      "Top-K resultados por modelo: 2\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ‚úÖ all-minilm"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Implementa√ß√£o:** `langchain_ollama`  |  **Dimens√µes:** `384`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Resultados:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ü•á **Score: 1.2114** ‚Üí *A placa de v√≠deo RTX 4090 roda jogos em 4K.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ü•à **Score: 1.4282** ‚Üí *Receita de lasanha √† bolonhesa com muito queijo.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ‚úÖ nomic-embed-text"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Implementa√ß√£o:** `langchain_ollama`  |  **Dimens√µes:** `768`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Resultados:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ü•á **Score: 0.7295** ‚Üí *Para fazer um bolo macio, bata as claras em neve.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ü•à **Score: 0.7368** ‚Üí *O atacante chutou a bola no √¢ngulo e foi gol.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ‚úÖ mxbai-embed-large"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Implementa√ß√£o:** `langchain_ollama`  |  **Dimens√µes:** `1024`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Resultados:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ü•á **Score: 0.7448** ‚Üí *A placa de v√≠deo RTX 4090 roda jogos em 4K.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ü•à **Score: 0.8314** ‚Üí *O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Executar compara√ß√£o entre alguns modelos comuns do Ollama\n",
    "models_to_compare = [\"all-minilm\", \"nomic-embed-text\", \"mxbai-embed-large\"]\n",
    "\n",
    "print('='*60)\n",
    "print('üöÄ INICIANDO COMPARA√á√ÉO DE MODELOS')\n",
    "print('='*60)\n",
    "print(f'Modelos a comparar: {\", \".join(models_to_compare)}')\n",
    "print(f'Query: \"{pergunta}\"')\n",
    "print(f'Documentos no √≠ndice: {len(meus_textos)}')\n",
    "print(f'Top-K resultados por modelo: 2')\n",
    "print('='*60 + '\\n')\n",
    "\n",
    "comp_results = compare_models(models_to_compare, meus_textos, pergunta, k=2)\n",
    "\n",
    "# Exibir resultados de forma leg√≠vel\n",
    "for res in comp_results:\n",
    "    if 'error' in res:\n",
    "        display(Markdown(f\"### ‚ùå {res['model']}\"))\n",
    "        display(Markdown(f\"**Erro:** {res['error']}\"))\n",
    "        display(Markdown(\"---\"))\n",
    "        continue\n",
    "    \n",
    "    display(Markdown(f\"### ‚úÖ {res['model']}\"))\n",
    "    display(Markdown(f\"**Implementa√ß√£o:** `{res['impl']}`  |  **Dimens√µes:** `{res['dim']}`\"))\n",
    "    display(Markdown(\"**Resultados:**\"))\n",
    "    \n",
    "    for i, h in enumerate(res['hits'], 1):\n",
    "        score_emoji = \"ü•á\" if i == 1 else \"ü•à\"\n",
    "        display(Markdown(f\"{score_emoji} **Score: {h['score']:.4f}** ‚Üí *{h['text']}*\"))\n",
    "    \n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76dcba1",
   "metadata": {},
   "source": [
    "## üìä Como Interpretar os Resultados\n",
    "\n",
    "### An√°lise dos Scores\n",
    "\n",
    "**O que significa o score?**\n",
    "- √â a **dist√¢ncia** entre o vetor da query e o vetor do documento\n",
    "- **Menor score = maior similaridade**\n",
    "- Scores n√£o t√™m unidade fixa (dependem da m√©trica usada)\n",
    "\n",
    "**Exemplo de interpreta√ß√£o real dos resultados acima:**\n",
    "\n",
    "```text\n",
    "Query: \"Quero sugest√µes de hardware para computador ou celular\"\n",
    "\n",
    "‚úÖ all-minilm (384 dims)\n",
    "ü•á Score: 1.2114 ‚Üí RTX 4090\n",
    "ü•à Score: 1.4282 ‚Üí Lasanha\n",
    "\n",
    "‚úÖ nomic-embed-text (768 dims)\n",
    "ü•á Score: 0.7295 ‚Üí Bolo\n",
    "ü•à Score: 0.7368 ‚Üí Gol\n",
    "\n",
    "‚úÖ mxbai-embed-large (1024 dims)\n",
    "ü•á Score: 0.7448 ‚Üí RTX 4090\n",
    "ü•à Score: 0.8314 ‚Üí iPhone 15\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ef96d",
   "metadata": {},
   "source": [
    "\n",
    "### An√°lise Cr√≠tica dos Resultados\n",
    "\n",
    "**üéØ Modelo `mxbai-embed-large` - O MELHOR**\n",
    "- ‚úÖ **Top-1:** RTX 4090 (score: 0.7448) - CORRETO! Hardware para computador\n",
    "- ‚úÖ **Top-2:** iPhone 15 (score: 0.8314) - CORRETO! Hardware para celular\n",
    "- **Conclus√£o:** Entendeu perfeitamente a query, retornando exatamente os 2 documentos de hardware\n",
    "\n",
    "**‚ö†Ô∏è Modelo `all-minilm` - PARCIAL**\n",
    "- ‚úÖ **Top-1:** RTX 4090 (score: 1.2114) - CORRETO! \n",
    "- ‚ùå **Top-2:** Lasanha (score: 1.4282) - INCORRETO! N√£o √© hardware\n",
    "- **Conclus√£o:** Acertou o primeiro resultado, mas trouxe um documento irrelevante em segundo lugar\n",
    "\n",
    "**‚ùå Modelo `nomic-embed-text` - FALHOU**\n",
    "- ‚ùå **Top-1:** Bolo (score: 0.7295) - INCORRETO! Receita, n√£o hardware\n",
    "- ‚ùå **Top-2:** Gol (score: 0.7368) - INCORRETO! Esporte, n√£o hardware\n",
    "- **Conclus√£o:** N√£o entendeu a query, retornou documentos completamente irrelevantes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc93be",
   "metadata": {},
   "source": [
    "\n",
    "### üîç Por que esses resultados?\n",
    "\n",
    "**Observa√ß√µes importantes:**\n",
    "\n",
    "1. **Scores n√£o s√£o compar√°veis entre modelos diferentes**\n",
    "   - `all-minilm` tem scores maiores (~1.2) \n",
    "   - `nomic-embed-text` e `mxbai-embed-large` t√™m scores menores (~0.7-0.8)\n",
    "   - Isso √© normal! Cada modelo usa um espa√ßo vetorial diferente\n",
    "\n",
    "2. **Mais dimens√µes ‚Üí Melhor compreens√£o sem√¢ntica?**\n",
    "   - ‚úÖ `mxbai-embed-large` (1024 dims): 2/2 acertos\n",
    "   - ‚ö†Ô∏è `all-minilm` (384 dims): 1/2 acertos\n",
    "   - ‚ùå `nomic-embed-text` (768 dims): 0/2 acertos\n",
    "   - **Conclus√£o:** Neste caso, o modelo maior teve melhor desempenho, mas dimensionalidade sozinha n√£o garante qualidade!\n",
    "\n",
    "3. **`nomic-embed-text` falhou inesperadamente**\n",
    "   - √â um modelo popular e geralmente bom\n",
    "   - Pode ter sido treinado em dados diferentes\n",
    "   - Pode ter interpreta√ß√£o diferente de \"hardware\" e \"sugest√µes\"\n",
    "   - **Li√ß√£o:** Sempre teste com SEU dom√≠nio espec√≠fico!\n",
    "\n",
    "### üìä Tabela Comparativa Real\n",
    "\n",
    "| Modelo | Dims | Top-1 (Score) | Top-2 (Score) | Acertos | Qualidade |\n",
    "|--------|------|---------------|---------------|---------|-----------|\n",
    "| **mxbai-embed-large** | 1024 | RTX 4090 (0.74) | iPhone 15 (0.83) | ‚úÖ‚úÖ 2/2 | üèÜ Excelente |\n",
    "| **all-minilm** | 384 | RTX 4090 (1.21) | ‚ùå Lasanha (1.42) | ‚úÖ 1/2 | ‚ö†Ô∏è Parcial |\n",
    "| **nomic-embed-text** | 768 | ‚ùå Bolo (0.73) | ‚ùå Gol (0.74) | ‚ùå 0/2 | üí• Falhou |\n",
    "\n",
    "### üéØ Conclus√£o desta Compara√ß√£o\n",
    "\n",
    "**Para esta query espec√≠fica e este dataset:**\n",
    "\n",
    "ü•á **Vencedor: `mxbai-embed-large`**\n",
    "- √önica compreens√£o correta da query\n",
    "- Trouxe exatamente os documentos relevantes\n",
    "- Ordem dos resultados faz sentido (RTX mais pr√≥ximo que iPhone)\n",
    "\n",
    "ü•à **Segundo lugar: `all-minilm`**\n",
    "- Acertou o documento mais relevante\n",
    "- R√°pido e leve (384 dims)\n",
    "- Mas trouxe um falso positivo\n",
    "\n",
    "ü•â **Terceiro lugar: `nomic-embed-text`**\n",
    "- Falhou completamente nesta query\n",
    "- Pode ter boa performance em outros dom√≠nios\n",
    "- N√£o recomendado para este tipo de busca\n",
    "\n",
    "### üí° Li√ß√µes Aprendidas\n",
    "\n",
    "1. **N√£o confie cegamente em benchmarks p√∫blicos** - teste com seus dados!\n",
    "2. **Modelo maior NEM SEMPRE √© melhor** - mas neste caso foi\n",
    "3. **Um modelo popular pode falhar** - como o `nomic-embed-text` falhou aqui\n",
    "4. **Contexto importa** - estes modelos podem ter sido treinados em dom√≠nios diferentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c4fd99",
   "metadata": {},
   "source": [
    "## üß† Exerc√≠cios Pr√°ticos\n",
    "\n",
    "### Exerc√≠cio 1: Testando Outras Queries\n",
    "\n",
    "Modifique a vari√°vel `pergunta` e reexecute a compara√ß√£o. Tente:\n",
    "\n",
    "```python\n",
    "# Teste 1: Query muito espec√≠fica\n",
    "pergunta = \"processador Intel i9 13¬™ gera√ß√£o\"\n",
    "\n",
    "# Teste 2: Query amb√≠gua\n",
    "pergunta = \"como melhorar performance\"\n",
    "\n",
    "# Teste 3: Query de outro dom√≠nio\n",
    "pergunta = \"receitas r√°pidas para o jantar\"\n",
    "```\n",
    "\n",
    "**Perguntas para reflex√£o:**\n",
    "- Os modelos concordam nos resultados?\n",
    "- Algum modelo se saiu melhor em um tipo espec√≠fico de query?\n",
    "- Queries mais espec√≠ficas geram scores menores (mais confian√ßa)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae67ec",
   "metadata": {},
   "source": [
    "\n",
    "### Exerc√≠cio 2: Expandindo o Dataset\n",
    "\n",
    "Adicione mais documentos em `meus_textos`:\n",
    "\n",
    "```python\n",
    "meus_textos = [\n",
    "    # ... documentos existentes ...\n",
    "    \"O SSD NVMe de 2TB tem velocidades de leitura de 7000 MB/s\",\n",
    "    \"Mem√≥ria RAM DDR5 de 32GB para multitarefas pesadas\",\n",
    "    \"Tutorial de massa de pizza caseira com fermenta√ß√£o natural\",\n",
    "]\n",
    "```\n",
    "\n",
    "**Observe:** Como a adi√ß√£o de documentos similares afeta os scores?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f956a550",
   "metadata": {},
   "source": [
    "\n",
    "### Exerc√≠cio 3: Ajustando k (Top-K)\n",
    "\n",
    "Altere o par√¢metro `k` na fun√ß√£o `compare_models()`:\n",
    "\n",
    "```python\n",
    "comp_results = compare_models(models_to_compare, meus_textos, pergunta, k=3)  # Top-3\n",
    "```\n",
    "\n",
    "**An√°lise:** \n",
    "- O 3¬∫ resultado √© relevante?\n",
    "- H√° um \"gap\" grande entre o score do 2¬∫ e 3¬∫ resultados?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c56e497",
   "metadata": {},
   "source": [
    "\n",
    "### Exerc√≠cio 4: Testando Modelos Adicionais\n",
    "\n",
    "Se voc√™ tem outros modelos instalados no Ollama, teste-os:\n",
    "\n",
    "```python\n",
    "models_to_compare = [\"all-minilm\", \"nomic-embed-text\", \"mxbai-embed-large\", \"llama2\"]\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è **Aten√ß√£o:** Nem todos os modelos do Ollama s√£o embeddings! Modelos como `llama2` s√£o LLMs (gera√ß√£o de texto), n√£o embeddings. A compara√ß√£o falhar√° graciosamente com mensagem de erro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e44f7",
   "metadata": {},
   "source": [
    "\n",
    "### üèÜ Desafio Final\n",
    "\n",
    "**Crie um benchmark completo:**\n",
    "\n",
    "1. Defina 5 queries diferentes (variando especificidade e dom√≠nio)\n",
    "2. Execute todas contra os 3 modelos\n",
    "3. Calcule m√©tricas agregadas:\n",
    "   - M√©dia dos scores por modelo\n",
    "   - Taxa de concord√¢ncia (quantas vezes o top-1 coincide?)\n",
    "   - Tempo de execu√ß√£o (use `time.time()`)\n",
    "\n",
    "4. **Escolha o vencedor** baseado no seu caso de uso:\n",
    "   - Se precisar de velocidade: `all-minilm`\n",
    "   - Se precisar de balanceamento: `nomic-embed-text`\n",
    "   - Se precisar de m√°xima qualidade: `mxbai-embed-large`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b9f4b4",
   "metadata": {},
   "source": [
    "## üìö Resumo e Pr√≥ximos Passos\n",
    "\n",
    "### O que voc√™ aprendeu\n",
    "\n",
    "‚úÖ **Embeddings** s√£o representa√ß√µes num√©ricas de textos que capturam significado sem√¢ntico  \n",
    "‚úÖ **Dimensionalidade** afeta qualidade, velocidade e consumo de recursos  \n",
    "‚úÖ **Modelos diferentes** podem ter interpreta√ß√µes distintas do mesmo texto  \n",
    "‚úÖ **Scores menores** indicam maior similaridade (menor dist√¢ncia vetorial)  \n",
    "‚úÖ **Compara√ß√µes sistem√°ticas** s√£o essenciais para escolher o modelo ideal  \n",
    "\n",
    "### Conceitos-chave para lembrar\n",
    "\n",
    "1. **N√£o existe modelo perfeito** - tudo √© trade-off (velocidade vs. qualidade)\n",
    "2. **Sempre teste com dados reais** do seu dom√≠nio\n",
    "3. **FAISS √© local e r√°pido** - ideal para prototipagem\n",
    "4. **Ollama √© gratuito** - aproveite para experimentar!\n",
    "\n",
    "\n",
    "### üìñ Recursos Adicionais\n",
    "\n",
    "- [Ollama Models](https://ollama.ai/library) - Cat√°logo completo de modelos\n",
    "- [FAISS Documentation](https://github.com/facebookresearch/faiss/wiki) - Guias e tutoriais\n",
    "- [LangChain Embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/) - Refer√™ncia oficial\n",
    "- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) - Benchmark de embeddings\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Parab√©ns por completar o laborat√≥rio!** Agora voc√™ tem as ferramentas para tomar decis√µes informadas sobre modelos de embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
