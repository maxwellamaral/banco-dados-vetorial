{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2349cee0",
   "metadata": {},
   "source": [
    "### 1) Instalar depend√™ncias (opcional - rode no terminal ou em uma c√©lula do notebook)\n",
    "Em um notebook voc√™ pode executar (c√©lula de bash):\n",
    "```bash\n",
    "pip install -U openai numpy scikit-learn python-dotenv langchain-google-genai requests\n",
    "```\n",
    "*Na pr√°tica, instale apenas as libs que vai usar no ambiente do Jupyter.*\n",
    "\n",
    "**Nota:** N√£o √© necess√°rio instalar `google-generativeai` pois h√° conflito de depend√™ncias com `langchain-google-genai`. Usamos REST API direta para obter metadados completos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "834d2801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\01-projetos\\11-work\\11.34-engenharia-vetorial\\.venv\\lib\\site-packages\\google\\api_core\\_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.5) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Tuple, Optional\n",
    "from openai import OpenAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580f850",
   "metadata": {},
   "source": [
    "### 2) Testar carregamento das vari√°veis de ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b5308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé .env carregado -> E:\\01-projetos\\11-work\\11.34-engenharia-vetorial\\.env\n",
      "OPENAI_API_KEY set? -> True\n",
      "GOOGLE_API_KEY set? -> True\n"
     ]
    }
   ],
   "source": [
    "# 2) Configura√ß√£o e carregamento do .env (simplificado)\n",
    "env_path = Path.cwd().joinpath('..', '..', '.env').resolve()\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f'üîé .env carregado -> {env_path.resolve()}')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  .env n√£o encontrado. Defina as vari√°veis de ambiente manualmente.')\n",
    "\n",
    "\n",
    "# Checar chaves (r√≥tulos simples)\n",
    "print('OPENAI_API_KEY set? ->', bool(os.getenv('OPENAI_API_KEY')))\n",
    "print('GOOGLE_API_KEY set? ->', bool(os.getenv('GOOGLE_API_KEY')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1de86d",
   "metadata": {},
   "source": [
    "### 3) Fun√ß√µes para gerar embeddings\n",
    "\n",
    "- Vamos criar fun√ß√µes pequenas e claras para OpenAI e para Google Gemini (com fallback pra LangChain).\n",
    "- Foque no fluxo: obter texto, chamar API, receber vetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85e3874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OpenAI embedding wrapper\n",
    "def openai_embedding(text: str, model: str = 'text-embedding-3-small', return_usage: bool = False) -> Tuple[List[float], Optional[int]]:\n",
    "    client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "    resp = client.embeddings.create(input=text, model=model)\n",
    "    emb = resp.data[0].embedding\n",
    "    if return_usage:\n",
    "        usage = getattr(resp, 'usage', None)\n",
    "        total_tokens = getattr(usage, 'total_tokens', None) if usage is not None else None\n",
    "        return emb, total_tokens\n",
    "    return emb, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef441742",
   "metadata": {},
   "source": [
    "#### Sobre a implementa√ß√£o REST da API Google\n",
    "\n",
    "A fun√ß√£o `google_embedding` usa duas abordagens diferentes:\n",
    "\n",
    "**Quando `return_usage=True` (precisa de token count exato):**\n",
    "- Faz chamada REST direta para `https://generativelanguage.googleapis.com/v1beta/models/{model}:embedContent`\n",
    "- Retorna embedding + metadados incluindo `tokenCount` exato\n",
    "- Se o token count n√£o vier na resposta, faz uma segunda chamada para `:countTokens` endpoint\n",
    "- Fallback para LangChain + estimativa se houver erro na API\n",
    "\n",
    "**Quando `return_usage=False` (modo simples):**\n",
    "- Usa `GoogleGenerativeAIEmbeddings` do LangChain (mais conveniente)\n",
    "- N√£o faz chamadas extras para contar tokens\n",
    "\n",
    "**Vantagens da abordagem REST:**\n",
    "- ‚úÖ Token count **exato** retornado pela API\n",
    "- ‚úÖ Sem conflitos de depend√™ncias\n",
    "- ‚úÖ Acesso a todos os metadados da resposta\n",
    "- ‚úÖ Fallback robusto em caso de erro\n",
    "\n",
    "**Documenta√ß√£o oficial:**\n",
    "- API Reference: https://ai.google.dev/api/rest/v1beta/models/embedContent\n",
    "- Count Tokens: https://ai.google.dev/api/rest/v1beta/models/countTokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e48934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Google embedding wrapper (retorna embedding e tokens exatos via REST API)\n",
    "def google_embedding(text: str, model: str = 'gemini-embedding-001', return_usage: bool = True) -> Tuple[List[float], Optional[int]]:        \n",
    "    \"\"\"\n",
    "    Gera embeddings usando Google Gemini.\n",
    "    \n",
    "    Quando return_usage=True, usa REST API direta para obter token count exato.\n",
    "    Quando return_usage=False, usa LangChain para simplicidade.\n",
    "    \"\"\"\n",
    "    if return_usage:\n",
    "        # Usar REST API direta para obter metadados completos incluindo token count\n",
    "        import requests\n",
    "        \n",
    "        api_key = os.getenv('GOOGLE_API_KEY')\n",
    "        if not api_key:\n",
    "            raise ValueError('GOOGLE_API_KEY n√£o encontrada nas vari√°veis de ambiente')\n",
    "        \n",
    "        url = f'https://generativelanguage.googleapis.com/v1beta/models/{model}:embedContent?key={api_key}'\n",
    "        \n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            'model': f'models/{model}',\n",
    "            'content': {\n",
    "                'parts': [{'text': text}]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            # Extrair embedding\n",
    "            embedding = data.get('embedding', {}).get('values', [])\n",
    "            \n",
    "            # Extrair token count (pode estar em diferentes locais dependendo da vers√£o da API)\n",
    "            token_count = None\n",
    "            \n",
    "            # Tentar extrair de metadata\n",
    "            if 'metadata' in data:\n",
    "                token_count = data['metadata'].get('tokenCount')\n",
    "            \n",
    "            # Fallback: usar a API de count tokens se n√£o vier na resposta\n",
    "            if token_count is None:\n",
    "                count_url = f'https://generativelanguage.googleapis.com/v1beta/models/{model}:countTokens?key={api_key}'\n",
    "                count_payload = {\n",
    "                    'contents': [{'parts': [{'text': text}]}]\n",
    "                }\n",
    "                count_response = requests.post(count_url, headers=headers, json=count_payload)\n",
    "                if count_response.status_code == 200:\n",
    "                    count_data = count_response.json()\n",
    "                    token_count = count_data.get('totalTokens')\n",
    "            \n",
    "            return list(embedding), token_count\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Erro na chamada REST API do Google: {e}')\n",
    "            # Fallback para LangChain com estimativa\n",
    "            emb = GoogleGenerativeAIEmbeddings(model=model).embed_query(text)\n",
    "            estimated_tokens = len(text) // 4\n",
    "            return list(emb), estimated_tokens\n",
    "    else:\n",
    "        # Usar LangChain quando n√£o precisar de usage (mais simples)\n",
    "        emb = GoogleGenerativeAIEmbeddings(model=model).embed_query(text)\n",
    "        return list(emb), None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9d81c",
   "metadata": {},
   "source": [
    "### 4) Exemplo pr√°tico: gerar embeddings e comparar similaridade\n",
    "\n",
    "Vamos gerar embeddings para 3 frases e calcular a similaridade (cosine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca89e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo e compara√ß√£o: gerar embeddings e calcular similaridade\n",
    "\n",
    "def cosine_sim(v1, v2):\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "\n",
    "texts = [\n",
    "    'O gato √© um animal dom√©stico',\n",
    "    'O gato √© um felino de estima√ß√£o',\n",
    "    'A programa√ß√£o √© importante para engenheiros de software'\n",
    "]\n",
    "\n",
    "emb1 = emb2 = emb3 = None\n",
    "backend = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2147ae",
   "metadata": {},
   "source": [
    "### Como a similaridade √© calculada (cosine similarity)\n",
    "\n",
    "A similaridade coseno mede o qu√£o \"parecidos\" s√£o dois vetores de embeddings, comparando a **dire√ß√£o** deles no espa√ßo vetorial (ignorando o tamanho/comprimento).\n",
    "\n",
    "**Analogia intuitiva:**\n",
    "Imagine dois vetores como setas no espa√ßo. A similaridade coseno mede o √¢ngulo entre essas setas:\n",
    "- Se apontam na **mesma dire√ß√£o** (√¢ngulo pequeno) ‚Üí similaridade pr√≥xima de **1** (muito similares)\n",
    "- Se apontam em **dire√ß√µes perpendiculares** (√¢ngulo de 90¬∞) ‚Üí similaridade de **0** (sem rela√ß√£o)\n",
    "- Se apontam em **dire√ß√µes opostas** (√¢ngulo de 180¬∞) ‚Üí similaridade de **-1** (completamente opostos)\n",
    "\n",
    "**F√≥rmula matem√°tica:**\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_sim}(v_1, v_2) = \\frac{v_1 \\cdot v_2}{\\|v_1\\| \\times \\|v_2\\|}\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "- $v_1 \\cdot v_2$ √© o **produto escalar** entre os vetores (soma dos produtos elemento a elemento)\n",
    "- $\\|v\\|$ √© a **norma L2** do vetor: $\\sqrt{\\sum x_i^2}$ (comprimento da \"seta\")\n",
    "- O resultado est√° sempre entre **-1** e **+1**\n",
    "\n",
    "**Por que √© √∫til para embeddings?**\n",
    "\n",
    "1. **Independente do tamanho**: Dividir pelo produto das normas \"normaliza\" os vetores, fazendo com que apenas a dire√ß√£o importe, n√£o a magnitude\n",
    "2. **Eficiente para comparar textos**: Textos semanticamente similares ter√£o embeddings apontando em dire√ß√µes parecidas\n",
    "3. **Normaliza√ß√£o L2**: Muitos bancos vetoriais (como Qdrant) normalizam embeddings antes de armazenar, tornando a compara√ß√£o ainda mais r√°pida (vira apenas produto escalar)\n",
    "\n",
    "**Exemplo pr√°tico deste notebook:**\n",
    "- **Sim 1-2 alta** (ex: 0.85): \"gato dom√©stico\" e \"felino de estima√ß√£o\" t√™m significados pr√≥ximos\n",
    "- **Sim 1-3 baixa** (ex: 0.35): \"gato dom√©stico\" e \"programa√ß√£o\" s√£o t√≥picos diferentes\n",
    "- A diferen√ßa entre essas similaridades mostra que o modelo consegue distinguir bem os conceitos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471a8130",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tentar OpenAI (pequeno) -> OpenAI (large) -> Google -> Erro amig√°vel\n",
    "openai_small_available = bool(os.getenv('OPENAI_API_KEY'))\n",
    "openai_large_available = bool(os.getenv('OPENAI_API_KEY'))\n",
    "google_available = bool(os.getenv('GOOGLE_API_KEY'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06abbe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fun√ß√£o helper para rodar OpenAI e obter uso\n",
    "\n",
    "def get_openai_embeddings(texts_list, model_name):\n",
    "    embeddings = []\n",
    "    total_tokens = 0\n",
    "    for t in texts_list:\n",
    "        emb, usage = openai_embedding(t, model=model_name, return_usage=True)\n",
    "        embeddings.append(emb)\n",
    "        if usage is not None:\n",
    "            try:\n",
    "                total_tokens += int(usage)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return embeddings, total_tokens if total_tokens else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20f4a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# OpenAI small\n",
    "if openai_small_available:\n",
    "    try:\n",
    "        emb_small, tokens_small = get_openai_embeddings(texts, 'text-embedding-3-small')\n",
    "        results['openai_small'] = {'embeddings': emb_small, 'tokens': tokens_small, 'dim': len(emb_small[0])}\n",
    "    except Exception as e:\n",
    "        print('Falha ao gerar embeddings OpenAI small:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e8d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI large\n",
    "if openai_large_available:\n",
    "    try:\n",
    "        emb_large, tokens_large = get_openai_embeddings(texts, 'text-embedding-3-large')\n",
    "        results['openai_large'] = {'embeddings': emb_large, 'tokens': tokens_large, 'dim': len(emb_large[0])}\n",
    "    except Exception as e:\n",
    "        print('Falha ao gerar embeddings OpenAI large:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "401f49b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google embeddings\n",
    "if google_available:\n",
    "    try:\n",
    "        emb_google = [google_embedding(t, return_usage=True) for t in texts]\n",
    "        results['google'] = {'embeddings': [e[0] for e in emb_google], 'tokens': [e[1] for e in emb_google], 'dim': len(emb_google[0][0])}\n",
    "    except Exception as e:\n",
    "        print('Falha ao gerar embeddings Google:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ebd670f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backend: openai_small\n",
      "Dimens√£o: 1536\n",
      "Tokens (se dispon√≠vel): 32\n",
      " Embedding 1 primeiros 5: [0.001428517745807767, -0.004510881379246712, -0.04022705927491188, 0.04195206239819527, 0.015516397543251514]\n",
      " Embedding 2 primeiros 5: [-0.012964113615453243, 0.009385946206748486, -0.061422210186719894, 0.0561358705163002, 0.017261510714888573]\n",
      " Embedding 3 primeiros 5: [0.01946329139173031, 0.02798319421708584, -0.0038614224176853895, -0.009704718366265297, 0.048555903136730194]\n",
      " Similaridade 1-2: 0.8135\n",
      " Similaridade 1-3: 0.1421\n",
      "\n",
      "Backend: openai_large\n",
      "Dimens√£o: 3072\n",
      "Tokens (se dispon√≠vel): 32\n",
      " Embedding 1 primeiros 5: [-0.021825000643730164, 0.03996645659208298, -0.0028245302382856607, 0.0035684334579855204, 0.010693789459764957]\n",
      " Embedding 2 primeiros 5: [-0.016434069722890854, 0.02808525785803795, -0.002878795610740781, -0.02375573106110096, 0.024798443540930748]\n",
      " Embedding 3 primeiros 5: [0.00042473155190236866, 0.0036773430183529854, -0.023540019989013672, 0.039040759205818176, -0.002575082238763571]\n",
      " Similaridade 1-2: 0.8490\n",
      " Similaridade 1-3: 0.2333\n",
      "\n",
      "Backend: google\n",
      "Dimens√£o: 3072\n",
      "Tokens (se dispon√≠vel): 24 (total de 3 chamadas)\n",
      " Embedding 1 primeiros 5: [-0.01546041, 0.013743455, 0.037031002, -0.057048496, 0.0026470574]\n",
      " Embedding 2 primeiros 5: [-0.03552813, 0.0063926117, 0.033068012, -0.064242266, -0.0032477966]\n",
      " Embedding 3 primeiros 5: [0.008538279, -0.0011638638, 0.02327945, -0.056141205, -0.014525335]\n",
      " Similaridade 1-2: 0.8846\n",
      " Similaridade 1-3: 0.5313\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mostrar resultados: dimens√£o, primeiros 5 valores e similaridades por backend\n",
    "for k, v in results.items():\n",
    "    print('\\nBackend:', k)\n",
    "    print('Dimens√£o:', v['dim'])\n",
    "    \n",
    "    # Exibir tokens (somar se for lista)\n",
    "    tokens_value = v['tokens']\n",
    "    if isinstance(tokens_value, list):\n",
    "        total_tokens = sum(t for t in tokens_value if t is not None)\n",
    "        print(f'Tokens (se dispon√≠vel): {total_tokens} (total de {len(tokens_value)} chamadas)')\n",
    "    else:\n",
    "        print('Tokens (se dispon√≠vel):', tokens_value)\n",
    "    \n",
    "    for i, e in enumerate(v['embeddings']):\n",
    "        print(f' Embedding {i+1} primeiros 5:', e[:5])\n",
    "    # calcular similaridade\n",
    "    sim_12 = cosine_sim(v['embeddings'][0], v['embeddings'][1])\n",
    "    sim_13 = cosine_sim(v['embeddings'][0], v['embeddings'][2])\n",
    "    print(f' Similaridade 1-2: {sim_12:.4f}')\n",
    "    print(f' Similaridade 1-3: {sim_13:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b540a82",
   "metadata": {},
   "source": [
    "### Como interpretar os resultados e o que estamos comparando\n",
    "\n",
    "Nesta se√ß√£o comparamos **tr√™s modelos** de embeddings: OpenAI (text-embedding-3-small), OpenAI (text-embedding-3-large) e Google (gemini-embedding-001). Para cada um comparamos:\n",
    "\n",
    "- **Dimens√£o (dim)**: n√∫mero de componentes no vetor de embedding. Modelos maiores normalmente representam mais informa√ß√µes, mas s√£o mais caros e custam mais em armazenamento/consulta.\n",
    "- **Tokens (contagem exata)**: n√∫mero total de tokens usados nas chamadas, obtido diretamente das APIs:\n",
    "  - OpenAI: via `response.usage.total_tokens`\n",
    "  - Google: via REST API endpoints `:embedContent` ou `:countTokens`\n",
    "  - Essencial para c√°lculo preciso de custos\n",
    "- **Primeiros 5 valores do embedding**: r√°pido \"check\" para ver distribui√ß√£o/escala dos vetores.\n",
    "- **Similaridade 1-2 vs 1-3**: comparamos a similaridade entre a frase 1 e 2 (sem√¢ntica pr√≥xima) e entre 1 e 3 (sem√¢ntica diferente). O **esperado** √© que 1-2 tenha similaridade maior que 1-3.\n",
    "\n",
    "Interpreta√ß√£o:\n",
    "- Se a similaridade 1-2 > 1-3, o modelo est√° capturando corretamente sem√¢ntica local entre as frases; quanto maior a diferen√ßa, maior a separa√ß√£o sem√¢ntica observada.\n",
    "- Uma similaridade muito alta entre 1 e 3 sugere que o modelo n√£o distingue bem os dois conceitos ou que as frases compartilham termos/estruturas que influenciam a representa√ß√£o.\n",
    "- Use a dimens√£o e tokens para equilibrar custo vs qualidade: modelos com maior dimens√£o costumam retornar maior qualidade sem√¢ntica, mas com custo e lat√™ncia maiores.\n",
    "- Compare os **tokens reais** (n√£o estimados) entre modelos para entender diferen√ßas na tokeniza√ß√£o e no custo efetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970b0d4",
   "metadata": {},
   "source": [
    "### Pre√ßos padr√£o \n",
    "Baseados em informa√ß√µes p√∫blicas (revisar e atualizar conforme o site oficial):\n",
    "\n",
    "- OpenAI text-embedding-3-small: $0.02 por 1M tokens = $0.00002 por 1K tokens\n",
    "- OpenAI text-embedding-3-large: $0.13 por 1M tokens = $0.00013 por 1K tokens\n",
    "- Google gemini-embedding-001: $0.15 por 1M tokens = $0.00015 por 1K tokens\n",
    "\n",
    "Refer√™ncias:\n",
    "- OpenAI Pricing: https://platform.openai.com/docs/models \n",
    "  - Selecione o embedding model na lista\n",
    "  - Ver exemplo em: https://platform.openai.com/docs/models/text-embedding-3-small\n",
    "- Google Gemini Pricing: https://ai.google.dev/gemini-api/docs/pricing#gemini-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8da26514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimativa de custos (usa pre√ßos p√∫blicos quando poss√≠vel)\n",
    "from math import ceil\n",
    "\n",
    "PRICING = {\n",
    "    'openai_text-embedding-3-small': {'per_1k_tokens_usd': 0.00002},\n",
    "    'openai_text-embedding-3-large': {'per_1k_tokens_usd': 0.00013},\n",
    "    'google_gemini-embedding-001': {'per_1k_tokens_usd': 0.00015}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed53ebb",
   "metadata": {},
   "source": [
    "### Metodologia de estimativa de custos\n",
    "\n",
    "Explica√ß√£o de como calculamos o custo estimado por execu√ß√£o:\n",
    "\n",
    "- Para modelos **OpenAI** (text-embedding-3-small e -large) usamos *tokens exatos* retornados no campo `usage.total_tokens` e aplicamos o custo por 1K tokens: `custo = (tokens / 1000) * price_per_1k_tokens`.\n",
    "- Para modelo **Google** (gemini-embedding-001) usamos *tokens exatos* obtidos via REST API (endpoint `:embedContent` ou `:countTokens`) e aplicamos o custo por 1K tokens.\n",
    "- `PRICING` √© um dicion√°rio edit√°vel que cont√©m tarifas por 1k tokens. Esse dicion√°rio pode ser atualizado manualmente com valores oficiais.\n",
    "- Se houver falha ao obter tokens da API, usamos estimativa como fallback (1 token ‚âà 4 caracteres).\n",
    "\n",
    "**Fontes de token count:**\n",
    "- OpenAI: `response.usage.total_tokens` (oficial)\n",
    "- Google: REST API `embedContent` ou `countTokens` endpoint (oficial)\n",
    "\n",
    "Limita√ß√µes e recomenda√ß√µes:\n",
    "- Os valores de **tokens s√£o exatos** quando retornados pelas APIs oficiais.\n",
    "- Os **pre√ßos** em `PRICING` s√£o baseados em documenta√ß√£o p√∫blica e devem ser verificados periodicamente.\n",
    "- Para custos de produ√ß√£o, valide com a documenta√ß√£o de pre√ßos atualizada (links: [OpenAI pricing](https://openai.com/pricing) e [Google AI Studio pricing](https://ai.google.dev/pricing)) e/ou use as APIs oficiais de faturamento, quando dispon√≠veis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bd57a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí∞ Custos por modelo (esta execu√ß√£o):\n",
      "==================================================\n",
      "openai_small         ‚Üí $0.00000064 USD\n",
      "openai_large         ‚Üí $0.00000416 USD\n",
      "google               ‚Üí $0.00000360 USD\n",
      "\n",
      "üìå Observa√ß√£o: Valores baseados em tokens REAIS das APIs.\n",
      "   Revise PRICING periodicamente para manter pre√ßos atualizados.\n"
     ]
    }
   ],
   "source": [
    "# Fun√ß√£o para calcular custo baseado em tokens exatos\n",
    "\n",
    "def estimate_tokens_from_text(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Heur√≠stica para estimar tokens quando n√£o dispon√≠veis da API.\n",
    "    Assume ~1.3 tokens por palavra (aproxima√ß√£o).\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return max(1, int(ceil(len(words) * 1.3)))\n",
    "\n",
    "\n",
    "def calculate_cost(model_key: str, tokens: int) -> float:\n",
    "    \"\"\"\n",
    "    Calcula o custo baseado em tokens exatos e pre√ßos definidos em PRICING.\n",
    "    \n",
    "    Args:\n",
    "        model_key: Chave do modelo no dicion√°rio PRICING\n",
    "        tokens: N√∫mero exato de tokens processados\n",
    "    \n",
    "    Returns:\n",
    "        Custo em USD\n",
    "    \"\"\"\n",
    "    pricing = PRICING.get(model_key)\n",
    "    if pricing is None:\n",
    "        raise KeyError(f'Modelo {model_key} sem pre√ßo definido no PRICING dict')\n",
    "\n",
    "    if 'per_1k_tokens_usd' not in pricing:\n",
    "        raise KeyError(f'Modelo {model_key} sem estrutura de pre√ßo v√°lida')\n",
    "    \n",
    "    return (tokens / 1000.0) * pricing['per_1k_tokens_usd']\n",
    "\n",
    "\n",
    "# Calcular custo para os resultados do notebook (results dict)\n",
    "results_cost = {}\n",
    "\n",
    "for k, v in results.items():\n",
    "    # Mapear nome do resultado para chave do modelo\n",
    "    model_key: str | None = None\n",
    "    if k == 'openai_small':\n",
    "        model_key = 'openai_text-embedding-3-small'\n",
    "    elif k == 'openai_large':\n",
    "        model_key = 'openai_text-embedding-3-large'\n",
    "    elif k == 'google':\n",
    "        model_key = 'google_gemini-embedding-001'\n",
    "    \n",
    "    # Validar que temos o model_key antes de continuar\n",
    "    if model_key is None:\n",
    "        print(f'‚ö†Ô∏è  Modelo desconhecido: {k} - pulando c√°lculo de custo')\n",
    "        results_cost[k] = None\n",
    "        continue\n",
    "\n",
    "    tokens = v.get('tokens')\n",
    "    cost = None\n",
    "    \n",
    "    try:\n",
    "        if tokens is not None:\n",
    "            # Para Google, tokens √© uma lista (um por chamada); somar todos\n",
    "            if isinstance(tokens, list):\n",
    "                total_tokens = sum(t for t in tokens if t is not None)\n",
    "            else:\n",
    "                total_tokens = tokens\n",
    "            \n",
    "            # Calcular custo com tokens reais\n",
    "            if total_tokens > 0:\n",
    "                cost = calculate_cost(model_key, total_tokens)\n",
    "        else:\n",
    "            # Fallback APENAS se tokens n√£o dispon√≠vel (n√£o deveria acontecer)\n",
    "            print(f'‚ö†Ô∏è  Aviso: tokens n√£o dispon√≠vel para {k}, usando estimativa')\n",
    "            total_tokens = sum(estimate_tokens_from_text(t) for t in texts)\n",
    "            cost = calculate_cost(model_key, total_tokens)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Erro ao calcular custo para {k}: {e}')\n",
    "        cost = None\n",
    "    \n",
    "    results_cost[k] = cost\n",
    "\n",
    "# Mostrar custos calculados\n",
    "print('\\nüí∞ Custos por modelo (esta execu√ß√£o):')\n",
    "print('=' * 50)\n",
    "for k, c in results_cost.items():\n",
    "    if c is not None:\n",
    "        print(f'{k:20} ‚Üí ${c:.8f} USD')\n",
    "    else:\n",
    "        print(f'{k:20} ‚Üí [erro no c√°lculo]')\n",
    "\n",
    "print('\\nüìå Observa√ß√£o: Valores baseados em tokens REAIS das APIs.')\n",
    "print('   Revise PRICING periodicamente para manter pre√ßos atualizados.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18772b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV report to E:\\01-projetos\\11-work\\11.34-engenharia-vetorial\\data\\embeddings\\comparative_results.csv\n",
      "\n",
      "DataFrame summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dim</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sim_12</th>\n",
       "      <th>sim_13</th>\n",
       "      <th>est_cost_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openai_small</td>\n",
       "      <td>1536</td>\n",
       "      <td>32</td>\n",
       "      <td>0.81346465</td>\n",
       "      <td>0.14211046</td>\n",
       "      <td>0.00000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openai_large</td>\n",
       "      <td>3072</td>\n",
       "      <td>32</td>\n",
       "      <td>0.84898137</td>\n",
       "      <td>0.23328540</td>\n",
       "      <td>0.00000416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google</td>\n",
       "      <td>3072</td>\n",
       "      <td>24</td>\n",
       "      <td>0.88463019</td>\n",
       "      <td>0.53127521</td>\n",
       "      <td>0.00000360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model   dim  tokens     sim_12     sim_13  est_cost_usd\n",
       "0  openai_small  1536      32 0.81346465 0.14211046    0.00000064\n",
       "1  openai_large  3072      32 0.84898137 0.23328540    0.00000416\n",
       "2        google  3072      24 0.88463019 0.53127521    0.00000360"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to E:\\01-projetos\\11-work\\11.34-engenharia-vetorial\\data\\embeddings\\similarity_comparison.png\n",
      "Saved plot to E:\\01-projetos\\11-work\\11.34-engenharia-vetorial\\data\\embeddings\\cost_comparison.png\n",
      "\n",
      "Instru√ß√µes para exportar o notebook como slides (Reveal.js) ou PDF:\n",
      "1) Para gerar slides HTML:\n",
      "   jupyter nbconvert --to slides notebooks/quick_test_classroom.ipynb --reveal-prefix 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/'\n",
      "2) Para gerar PDF (pode exigir LaTeX):\n",
      "   jupyter nbconvert --to pdf notebooks/quick_test_classroom.ipynb\n"
     ]
    }
   ],
   "source": [
    "# 6) Gerar relat√≥rio: CSV + gr√°ficos + instru√ß√µes para exportar slides/PDF\n",
    "\n",
    "\n",
    "# tentativas de import para pandas/matplotlib; se faltar, gerar CSV somente\n",
    "have_pandas = True\n",
    "have_matplotlib = True\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    have_pandas = False\n",
    "    pd = None\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except Exception:\n",
    "    have_matplotlib = False\n",
    "    plt = None\n",
    "\n",
    "# Criar pasta para salvar resultados\n",
    "out_dir = Path('../../data') / 'embeddings'\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Montar linhas para CSV\n",
    "rows = []\n",
    "for k, v in results.items():\n",
    "    try:\n",
    "        sim_12 = cosine_sim(v['embeddings'][0], v['embeddings'][1])\n",
    "        sim_13 = cosine_sim(v['embeddings'][0], v['embeddings'][2])\n",
    "    except Exception:\n",
    "        sim_12 = None\n",
    "        sim_13 = None\n",
    "    \n",
    "    # Normalizar tokens (somar se for lista)\n",
    "    tokens_value = v.get('tokens')\n",
    "    if isinstance(tokens_value, list):\n",
    "        tokens_value = sum(t for t in tokens_value if t is not None)\n",
    "    \n",
    "    rows.append({\n",
    "        'model': k,\n",
    "        'dim': v.get('dim'),\n",
    "        'tokens': tokens_value,\n",
    "        'sim_12': sim_12,\n",
    "        'sim_13': sim_13,\n",
    "        'est_cost_usd': results_cost.get(k)\n",
    "    })\n",
    "\n",
    "csv_path = out_dir / 'comparative_results.csv'\n",
    "with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['model', 'dim', 'tokens', 'sim_12', 'sim_13', 'est_cost_usd'])\n",
    "    writer.writeheader()\n",
    "    for r in rows:\n",
    "        writer.writerow(r)\n",
    "print(f'Saved CSV report to {csv_path.resolve()}')\n",
    "\n",
    "# If pandas available, show DataFrame\n",
    "if have_pandas:\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Configurar pandas para exibir n√∫meros com mais casas decimais sem nota√ß√£o cient√≠fica\n",
    "    pd.set_option('display.float_format', lambda x: f'{x:.8f}')\n",
    "    \n",
    "    print('\\nDataFrame summary:')\n",
    "    display(df)\n",
    "else:\n",
    "    print('\\nCSV content preview:')\n",
    "    for r in rows:\n",
    "        print(r)\n",
    "\n",
    "# Tentar criar plots se matplotlib estiver dispon√≠vel\n",
    "if have_matplotlib and plt is not None:\n",
    "    # Make arrays for plotting\n",
    "    models = [r['model'] for r in rows]\n",
    "    sim12 = [r['sim_12'] if r['sim_12'] is not None else 0 for r in rows]\n",
    "    sim13 = [r['sim_13'] if r['sim_13'] is not None else 0 for r in rows]\n",
    "    costs = [r['est_cost_usd'] if r['est_cost_usd'] is not None else 0 for r in rows]\n",
    "\n",
    "    # Similarity bar chart\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    x = range(len(models))\n",
    "    plt.bar(x, sim12, width=0.4, label='Sim 1-2')\n",
    "    plt.bar([i + 0.4 for i in x], sim13, width=0.4, label='Sim 1-3')\n",
    "    plt.title('Similaridade (cosine) por modelo')\n",
    "    plt.xticks([i + 0.2 for i in x], models)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plot1 = out_dir / 'similarity_comparison.png'\n",
    "    plt.savefig(plot1)\n",
    "    print(f'Saved plot to {plot1.resolve()}')\n",
    "    plt.close()\n",
    "\n",
    "    # Cost chart\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(models, costs, color='orange')\n",
    "    plt.title('Estimated cost per run (USD)')\n",
    "    plt.ylabel('USD')\n",
    "    plt.tight_layout()\n",
    "    plot2 = out_dir / 'cost_comparison.png'\n",
    "    plt.savefig(plot2)\n",
    "    print(f'Saved plot to {plot2.resolve()}')\n",
    "    plt.close()\n",
    "else:\n",
    "    print('\\nMatplotlib or pandas not installed; plots skipped. To enable plots: pip install pandas matplotlib')\n",
    "\n",
    "# Instru√ß√µes para exportar o notebook em slides/PDF\n",
    "print('\\nInstru√ß√µes para exportar o notebook como slides (Reveal.js) ou PDF:')\n",
    "print('1) Para gerar slides HTML:')\n",
    "print(\"   jupyter nbconvert --to slides notebooks/quick_test_classroom.ipynb --reveal-prefix 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/'\")\n",
    "print('2) Para gerar PDF (pode exigir LaTeX):')\n",
    "print('   jupyter nbconvert --to pdf notebooks/quick_test_classroom.ipynb')\n",
    "\n",
    "# Tamb√©m posso gerar esses arquivos automaticamente se preferir (recomendo usar o terminal)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "11-34-engenharia-vetorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
