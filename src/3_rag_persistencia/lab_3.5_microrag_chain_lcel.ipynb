{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d59e65",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Passo 1: Importar Bibliotecas\n",
    "\n",
    "**Novidades neste notebook:**\n",
    "- `RunnablePassthrough`: Permite passar dados atravÃ©s da chain\n",
    "- `StrOutputParser`: Formata a saÃ­da do LLM como string limpa\n",
    "- Usaremos o operador `|` para criar chains!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cfa8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Imports para LCEL (Chains)\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd336a",
   "metadata": {},
   "source": [
    "## âš™ï¸ Passo 2: Configurar Ambiente\n",
    "\n",
    "Mesma configuraÃ§Ã£o do notebook anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2021f828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ DiretÃ³rio de PDFs: e:\\01-projetos\\11-work\\11.34-engenharia-vetorial\\data\\pdfs\n",
      "ğŸ¤– Ollama URL: http://localhost:11434\n"
     ]
    }
   ],
   "source": [
    "# ConfiguraÃ§Ãµes iniciais\n",
    "OLLAMA_BASE_URL = 'http://localhost:11434'\n",
    "BASE_DIR = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
    "PDF_DIR = BASE_DIR.parent.parent / \"data\" / \"pdfs\"\n",
    "\n",
    "print(f\"ğŸ“ DiretÃ³rio de PDFs: {PDF_DIR}\")\n",
    "print(f\"ğŸ¤– Ollama URL: {OLLAMA_BASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc9f011",
   "metadata": {},
   "source": [
    "## ğŸ“„ Passo 3: Carregar Documentos\n",
    "\n",
    "Carregamos todos os PDFs da pasta `data/pdfs/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9208963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n",
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n",
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n",
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Encontrados 4 PDFs\n",
      "\n",
      "  âœ“ api_documentation.pdf: 3 pÃ¡ginas\n",
      "  âœ“ livro_receitas.pdf: 5 pÃ¡ginas\n",
      "  âœ“ manual_futebol.pdf: 4 pÃ¡ginas\n",
      "  âœ“ manual_iphone15.pdf: 3 pÃ¡ginas\n",
      "\n",
      "âœ… Total de pÃ¡ginas carregadas: 15\n"
     ]
    }
   ],
   "source": [
    "# Carrega os documentos\n",
    "documents = []\n",
    "\n",
    "pdf_paths = list(PDF_DIR.glob(\"*.pdf\"))\n",
    "print(f\"ğŸ“š Encontrados {len(pdf_paths)} PDFs\\n\")\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    loader = PyPDFLoader(str(pdf_path))\n",
    "    docs = loader.load()\n",
    "    documents.extend(docs)\n",
    "    print(f\"  âœ“ {pdf_path.name}: {len(docs)} pÃ¡ginas\")\n",
    "\n",
    "print(f\"\\nâœ… Total de pÃ¡ginas carregadas: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403bfb20",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Passo 4: Dividir em Chunks\n",
    "\n",
    "**Chunking strategy:**\n",
    "- `chunk_size=1000`: Chunks de ~1000 caracteres\n",
    "- `chunk_overlap=200`: Overlap para nÃ£o perder contexto nas bordas\n",
    "- `separators`: Prioriza quebras naturais (parÃ¡grafos â†’ linhas â†’ espaÃ§os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045bf48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸  Total de chunks criados: 33\n",
      "ğŸ“Š ExpansÃ£o de chunking: 2.2x\n"
     ]
    }
   ],
   "source": [
    "# Chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents=documents)\n",
    "\n",
    "print(f\"âœ‚ï¸  Total de chunks criados: {len(chunks)}\")\n",
    "print(f\"ğŸ“Š ExpansÃ£o de chunking: {len(chunks)/len(documents):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6803eeb9",
   "metadata": {},
   "source": [
    "## ğŸ§® Passo 5: Criar Embeddings\n",
    "\n",
    "Usamos o modelo `embeddinggemma` rodando localmente via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf05acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Modelo de embeddings: embeddinggemma\n",
      "âœ… Embeddings configurados!\n"
     ]
    }
   ],
   "source": [
    "# Cria embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model='embeddinggemma', \n",
    "    base_url=OLLAMA_BASE_URL\n",
    "    )\n",
    "\n",
    "print(\"ğŸ§  Modelo de embeddings: embeddinggemma\")\n",
    "print(\"âœ… Embeddings configurados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79007c1f",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Passo 6: Criar Vectorstore FAISS\n",
    "\n",
    "Indexamos todos os chunks no FAISS para busca eficiente.\n",
    "\n",
    "**AtenÃ§Ã£o:** Este passo pode demorar alguns minutos dependendo da quantidade de chunks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec98bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Criando Ã­ndice FAISS (isso pode demorar alguns minutos...)\n",
      "\n",
      "\n",
      "âœ… Vectorstore criado!\n",
      "ğŸ“Š Total de vetores indexados: 33\n",
      "ğŸ“ DimensÃµes dos embeddings: 768\n"
     ]
    }
   ],
   "source": [
    "# Cria vectorstore\n",
    "print(\"â³ Criando Ã­ndice FAISS (isso pode demorar alguns minutos...)\\n\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=chunks, \n",
    "    embedding=embeddings\n",
    "    )\n",
    "\n",
    "print(f\"\\nâœ… Vectorstore criado!\")\n",
    "print(f\"ğŸ“Š Total de vetores indexados: {vectorstore.index.ntotal}\")\n",
    "print(f\"ğŸ“ DimensÃµes dos embeddings: {vectorstore.index.d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ff2ce",
   "metadata": {},
   "source": [
    "## ğŸ” Passo 7: Criar Retriever\n",
    "\n",
    "### O que Ã© um Retriever?\n",
    "\n",
    "Um **retriever** Ã© um componente do LangChain que encapsula a busca no vectorstore.\n",
    "\n",
    "**DiferenÃ§a:**\n",
    "\n",
    "```python\n",
    "# Abordagem manual:\n",
    "docs = vectorstore.similarity_search(query, k=4)\n",
    "\n",
    "# Usando Retriever:\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "docs = retriever.invoke(query)  # Mesmo resultado!\n",
    "```\n",
    "\n",
    "### Por que usar Retriever?\n",
    "\n",
    "âœ… **CompatÃ­vel com Chains:** Pode ser encadeado com `|`  \n",
    "âœ… **Interface padronizada:** Funciona com qualquer vectorstore  \n",
    "âœ… **ConfigurÃ¡vel:** FÃ¡cil trocar estratÃ©gias de busca  \n",
    "âœ… **Runnable:** Suporta invoke, stream, batch, etc.\n",
    "\n",
    "**ConfiguraÃ§Ãµes:**\n",
    "- `search_kwargs={\"k\": 4}`: Retorna os 4 chunks mais similares\n",
    "- Pode adicionar `score_threshold`, `fetch_k`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93d19b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Retriever criado!\n",
      "ğŸ“‹ ConfiguraÃ§Ã£o: busca os top-4 chunks mais similares\n"
     ]
    }
   ],
   "source": [
    "# Cria o retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # Busca por similaridade\n",
    "    search_kwargs={\"k\": 4}      # Retorna top 4 chunks\n",
    ")\n",
    "\n",
    "print(\"ğŸ” Retriever criado!\")\n",
    "print(f\"ğŸ“‹ ConfiguraÃ§Ã£o: busca os top-4 chunks mais similares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18701c",
   "metadata": {},
   "source": [
    "## ğŸ’¬ Passo 8: Criar o Prompt Template\n",
    "\n",
    "O mesmo template do notebook anterior, mas agora serÃ¡ usado dentro da chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5f09c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Prompt template criado!\n"
     ]
    }
   ],
   "source": [
    "# Template de prompt\n",
    "template = \"\"\"Use o seguinte contexto para responder Ã  pergunta.\n",
    "Se vocÃª nÃ£o souber a resposta baseado no contexto, diga \"NÃ£o encontrei essa informaÃ§Ã£o nos documentos fornecidos.\"\n",
    "\n",
    "Importante: Cite sempre a fonte (nome do arquivo e pÃ¡gina) quando possÃ­vel.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pergunta: {question}\n",
    "\n",
    "Resposta detalhada:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['context', 'question']\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ Prompt template criado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e62153e",
   "metadata": {},
   "source": [
    "## ğŸ¤– Passo 9: Criar o LLM\n",
    "\n",
    "Modelo Ollama que gerarÃ¡ as respostas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f65ac6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– LLM configurado: llama3.2:1b\n"
     ]
    }
   ],
   "source": [
    "# Cria o LLM\n",
    "llm = OllamaLLM(\n",
    "    model='llama3.2:1b', \n",
    "    base_url=OLLAMA_BASE_URL\n",
    "    )\n",
    "\n",
    "print(\"ğŸ¤– LLM configurado: llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a460d0",
   "metadata": {},
   "source": [
    "## ğŸ”— Passo 10: Criar a Chain RAG com LCEL\n",
    "\n",
    "### ğŸ¯ Este Ã© o coraÃ§Ã£o do notebook!\n",
    "\n",
    "Vamos criar uma **chain** que automatiza todo o processo RAG usando o operador `|` (pipe).\n",
    "\n",
    "### Anatomia da Chain:\n",
    "\n",
    "```python\n",
    "chain = (\n",
    "    {\n",
    "     \"context\": retriever | format_docs, \n",
    "     \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "```\n",
    "\n",
    "### O que cada parte faz?\n",
    "\n",
    "#### 1ï¸âƒ£ `{\"context\": ..., \"question\": ...}`\n",
    "Cria um dicionÃ¡rio com as variÃ¡veis que o prompt precisa:\n",
    "\n",
    "- **`\"context\": retriever | format_docs`**\n",
    "  - Pega a pergunta â†’ busca no vectorstore â†’ formata os docs\n",
    "  - `retriever`: Busca os 4 chunks mais similares\n",
    "  - `|`: Passa os docs para a prÃ³xima funÃ§Ã£o\n",
    "  - `format_docs`: Junta os chunks com `\\n\\n`\n",
    "\n",
    "- **`\"question\": RunnablePassthrough()`**\n",
    "  - Passa a pergunta original sem modificar\n",
    "  - `RunnablePassthrough()` = \"deixa passar como estÃ¡\"\n",
    "\n",
    "#### 2ï¸âƒ£ `| prompt`\n",
    "- Recebe o dicionÃ¡rio `{context: ..., question: ...}`\n",
    "- Substitui os placeholders no template\n",
    "- Retorna o prompt completo preenchido\n",
    "\n",
    "#### 3ï¸âƒ£ `| llm`\n",
    "- Recebe o prompt completo\n",
    "- Gera a resposta usando o LLM\n",
    "- Retorna o texto bruto\n",
    "\n",
    "#### 4ï¸âƒ£ `| StrOutputParser()`\n",
    "- Converte a saÃ­da do LLM em string limpa\n",
    "- Remove metadados extras\n",
    "- Retorna sÃ³ o texto da resposta\n",
    "\n",
    "### ğŸ­ Fluxo Completo:\n",
    "\n",
    "```\n",
    "\"Quais sÃ£o as formaÃ§Ãµes do futebol?\"\n",
    "         â†“\n",
    "    retriever â†’ [doc1, doc2, doc3, doc4]\n",
    "         â†“\n",
    "    format_docs â†’ \"doc1\\n\\ndoc2\\n\\ndoc3\\n\\ndoc4\"\n",
    "         â†“\n",
    "    {context: \"...\", question: \"Quais sÃ£o...\"}\n",
    "         â†“\n",
    "    prompt â†’ \"Use o seguinte contexto... Pergunta: Quais sÃ£o...\"\n",
    "         â†“\n",
    "    llm â†’ \"As principais formaÃ§Ãµes sÃ£o...\"\n",
    "         â†“\n",
    "    StrOutputParser â†’ \"As principais formaÃ§Ãµes sÃ£o...\"\n",
    "```\n",
    "\n",
    "### âœ¨ Vantagens:\n",
    "\n",
    "1. **Uma linha invoca tudo:** `chain.invoke(\"pergunta\")`\n",
    "2. **ComposÃ¡vel:** Pode adicionar mais etapas facilmente\n",
    "3. **ReutilizÃ¡vel:** Salva e usa em outros projetos\n",
    "4. **Streaming:** `chain.stream()` para respostas progressivas\n",
    "5. **Async:** `await chain.ainvoke()` para execuÃ§Ã£o assÃ­ncrona\n",
    "\n",
    "Vamos criar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73971785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunÃ§Ã£o helper para formatar documentos\n",
    "def format_docs(docs):\n",
    "    \"\"\"Formata lista de documentos em string Ãºnica.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# ğŸ”— Criando a Chain RAG com LCEL\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs, \n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"ğŸ”— Chain RAG criada com sucesso!\")\n",
    "print(\"\\nğŸ“‹ Componentes da chain:\")\n",
    "print(\"  1ï¸âƒ£  Retriever (busca documentos)\")\n",
    "print(\"  2ï¸âƒ£  Format Docs (formata contexto)\")\n",
    "print(\"  3ï¸âƒ£  Prompt Template (monta prompt)\")\n",
    "print(\"  4ï¸âƒ£  LLM (gera resposta)\")\n",
    "print(\"  5ï¸âƒ£  String Parser (formata saÃ­da)\")\n",
    "print(\"\\nâœ… Pronta para uso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856ce0a",
   "metadata": {},
   "source": [
    "## ğŸš€ Passo 11: Invocar a Chain\n",
    "\n",
    "### Compare a simplicidade!\n",
    "\n",
    "**Abordagem procedural (lab 3.3):**\n",
    "```python\n",
    "# 6 linhas de cÃ³digo\n",
    "relevant_documents = vectorstore.similarity_search(query, k=4)\n",
    "context = '\\n\\n'.join(doc.page_content for doc in relevant_documents)\n",
    "question = query\n",
    "prompt_result = prompt.invoke({'context': context, 'question': question})\n",
    "response = llm.invoke(prompt_result)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "**Abordagem com Chain (agora):**\n",
    "```python\n",
    "# 1 linha de cÃ³digo!\n",
    "response = rag_chain.invoke(query)\n",
    "```\n",
    "\n",
    "âœ¨ **6 linhas â†’ 1 linha!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pergunta de teste\n",
    "query_futebol = \"Quais sÃ£o as principais formaÃ§Ãµes tÃ¡ticas do futebol?\"\n",
    "\n",
    "print(f\"â“ Pergunta: {query_futebol}\\n\")\n",
    "print(\"â³ Processando...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ğŸ¯ Uma linha invoca toda a chain!\n",
    "response = rag_chain.invoke(query_futebol)\n",
    "\n",
    "print(\"\\nğŸ¤– Resposta:\")\n",
    "print(\"=\" * 80)\n",
    "print(response)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ad1a7",
   "metadata": {},
   "source": [
    "## ğŸ” Passo 12: Inspecionar Documentos Recuperados\n",
    "\n",
    "Vamos verificar quais documentos o retriever encontrou para essa pergunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busca manual para inspeÃ§Ã£o\n",
    "relevant_docs = retriever.invoke(query_futebol)\n",
    "\n",
    "print(f\"ğŸ“š Documentos recuperados: {len(relevant_docs)}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"\\nğŸ“„ Documento {i}:\")\n",
    "    print(f\"   Fonte: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   PÃ¡gina: {doc.metadata.get('page', 'N/A')}\")\n",
    "    print(f\"   ConteÃºdo (primeiros 150 chars):\\n   {doc.page_content[:150]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0728c",
   "metadata": {},
   "source": [
    "## ğŸ§ª Passo 13: Testar com MÃºltiplas Perguntas\n",
    "\n",
    "Vamos testar a chain com diferentes tipos de perguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1789a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de perguntas de teste\n",
    "test_queries = [\n",
    "    \"Como fazer uma lasanha?\",\n",
    "    \"Qual a diferenÃ§a entre 4-4-2 e 4-3-3 no futebol?\",\n",
    "    \"Quais sÃ£o as regras do basquete?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testando mÃºltiplas perguntas...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nâ“ Pergunta {i}: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Invoca a chain\n",
    "    response = rag_chain.invoke(query)\n",
    "    \n",
    "    print(f\"ğŸ¤– Resposta:\\n{response}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d083c23",
   "metadata": {},
   "source": [
    "## ğŸŒŠ Passo 14: Streaming de Respostas (Bonus!)\n",
    "\n",
    "Uma das grandes vantagens de usar chains Ã© o suporte nativo a **streaming**!\n",
    "\n",
    "### O que Ã© Streaming?\n",
    "\n",
    "Ao invÃ©s de esperar a resposta completa, vocÃª recebe **palavra por palavra** em tempo real.\n",
    "\n",
    "**BenefÃ­cios:**\n",
    "- ğŸš€ **UX melhor:** UsuÃ¡rio vÃª progresso imediato\n",
    "- â±ï¸ **LatÃªncia percebida:** Parece mais rÃ¡pido\n",
    "- ğŸ’¬ **Chat-like:** ExperiÃªncia tipo ChatGPT\n",
    "\n",
    "**Uso:**\n",
    "```python\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com streaming\n",
    "query_stream = \"Quais sÃ£o as principais formaÃ§Ãµes tÃ¡ticas do futebol?\"\n",
    "\n",
    "print(f\"â“ Pergunta: {query_stream}\\n\")\n",
    "print(\"ğŸŒŠ Resposta (streaming):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Stream da resposta palavra por palavra\n",
    "for chunk in rag_chain.stream(query_stream):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nâœ… Streaming concluÃ­do!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8aaeb8",
   "metadata": {},
   "source": [
    "## ğŸ“Š Passo 15: AnÃ¡lise do Sistema\n",
    "\n",
    "EstatÃ­sticas sobre nosso sistema RAG com chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ff359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EstatÃ­sticas\n",
    "print(\"ğŸ“Š EstatÃ­sticas do Sistema RAG com Chains\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nğŸ“š Dados:\")\n",
    "print(f\"   Total de PDFs: {len(pdf_paths)}\")\n",
    "print(f\"   Total de pÃ¡ginas: {len(documents)}\")\n",
    "print(f\"   Total de chunks: {len(chunks)}\")\n",
    "print(f\"   ExpansÃ£o de chunking: {len(chunks)/len(documents):.1f}x\")\n",
    "\n",
    "# Tamanho dos chunks\n",
    "tamanhos = [len(c.page_content) for c in chunks]\n",
    "print(f\"\\nğŸ“ Chunks:\")\n",
    "print(f\"   Tamanho mÃ©dio: {sum(tamanhos)/len(tamanhos):.0f} caracteres\")\n",
    "print(f\"   Tamanho mÃ­nimo: {min(tamanhos)} caracteres\")\n",
    "print(f\"   Tamanho mÃ¡ximo: {max(tamanhos)} caracteres\")\n",
    "\n",
    "# Vectorstore\n",
    "print(f\"\\nğŸ—„ï¸  Vectorstore FAISS:\")\n",
    "print(f\"   DimensÃµes dos embeddings: {vectorstore.index.d}\")\n",
    "print(f\"   Total de vetores: {vectorstore.index.ntotal}\")\n",
    "\n",
    "# Chain\n",
    "print(f\"\\nğŸ”— Chain RAG:\")\n",
    "print(f\"   Componentes: 5 (retriever â†’ format â†’ prompt â†’ llm â†’ parser)\")\n",
    "print(f\"   Suporte a streaming: âœ… Sim\")\n",
    "print(f\"   Suporte a batch: âœ… Sim\")\n",
    "print(f\"   Suporte a async: âœ… Sim\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a16dd",
   "metadata": {},
   "source": [
    "## ğŸ“ Resumo: O que Aprendemos?\n",
    "\n",
    "### âœ… Conceitos Principais\n",
    "\n",
    "1. **LangChain Expression Language (LCEL)**\n",
    "   - Sintaxe de pipeline usando `|`\n",
    "   - ComposiÃ§Ã£o de componentes reutilizÃ¡veis\n",
    "   - PadrÃ£o moderno para RAG\n",
    "\n",
    "2. **Retriever**\n",
    "   - AbstraÃ§Ã£o sobre vectorstore\n",
    "   - Interface `Runnable` compatÃ­vel com chains\n",
    "   - ConfigurÃ¡vel e padronizado\n",
    "\n",
    "3. **Chain RAG**\n",
    "   - Pipeline automÃ¡tico: retrieval â†’ format â†’ prompt â†’ llm\n",
    "   - Uma linha substitui 6+ linhas de cÃ³digo procedural\n",
    "   - Suporte nativo a streaming, batch, async\n",
    "\n",
    "4. **RunnablePassthrough**\n",
    "   - Passa dados atravÃ©s da chain sem modificar\n",
    "   - Ãštil para manter variÃ¡veis originais\n",
    "\n",
    "5. **StrOutputParser**\n",
    "   - Converte saÃ­da do LLM em string limpa\n",
    "   - Remove metadados extras\n",
    "\n",
    "### ğŸ†š ComparaÃ§Ã£o: Procedural vs Chain\n",
    "\n",
    "| Aspecto | Procedural (lab 3.3) | Chain (este lab) |\n",
    "|---------|---------------------|------------------|\n",
    "| **Linhas de cÃ³digo** | ~6 linhas | 1 linha |\n",
    "| **Legibilidade** | Verboso | Conciso |\n",
    "| **Streaming** | Manual | Nativo (`stream()`) |\n",
    "| **ReutilizaÃ§Ã£o** | DifÃ­cil | FÃ¡cil (salva chain) |\n",
    "| **ManutenÃ§Ã£o** | Repetitivo | DRY (Don't Repeat) |\n",
    "| **ProduÃ§Ã£o** | NÃ£o recomendado | PadrÃ£o da indÃºstria |\n",
    "\n",
    "### ğŸ¯ Quando Usar Cada Abordagem?\n",
    "\n",
    "**Procedural (lab 3.3):**\n",
    "- âœ… Aprendizado e didÃ¡tica\n",
    "- âœ… Debugging detalhado\n",
    "- âœ… Entender cada etapa do RAG\n",
    "\n",
    "**Chain (este lab):**\n",
    "- âœ… ProduÃ§Ã£o\n",
    "- âœ… Projetos reais\n",
    "- âœ… CÃ³digo limpo e manutenÃ­vel\n",
    "- âœ… Performance e escalabilidade\n",
    "\n",
    "### ğŸš€ PrÃ³ximos Passos\n",
    "\n",
    "1. **Experimente diferentes retrievers:**\n",
    "   - `search_type=\"mmr\"` (Maximum Marginal Relevance)\n",
    "   - `search_type=\"similarity_score_threshold\"`\n",
    "\n",
    "2. **Adicione mais componentes Ã  chain:**\n",
    "   - Re-ranking de documentos\n",
    "   - Filtros de metadados\n",
    "   - Fallbacks e error handling\n",
    "\n",
    "3. **Explore recursos avanÃ§ados:**\n",
    "   - `chain.batch([query1, query2])`: Processar mÃºltiplas queries\n",
    "   - `await chain.ainvoke(query)`: ExecuÃ§Ã£o assÃ­ncrona\n",
    "   - `chain.with_config(...)`: ConfiguraÃ§Ã£o dinÃ¢mica\n",
    "\n",
    "4. **Otimize para produÃ§Ã£o:**\n",
    "   - Cache de embeddings\n",
    "   - Monitoramento e logging\n",
    "   - Rate limiting\n",
    "\n",
    "### ğŸ’¡ Dica Final\n",
    "\n",
    "**Chain = Composabilidade**\n",
    "\n",
    "```python\n",
    "# VocÃª pode criar chains complexas facilmente:\n",
    "chain = (\n",
    "    preprocessor\n",
    "    | retriever\n",
    "    | reranker\n",
    "    | prompt\n",
    "    | llm\n",
    "    | postprocessor\n",
    "    | validator\n",
    ")\n",
    "```\n",
    "\n",
    "Cada etapa Ã© um componente reutilizÃ¡vel que pode ser testado isoladamente!\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **ParabÃ©ns!** VocÃª dominou RAG com Chains usando LCEL!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "11-34-engenharia-vetorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
