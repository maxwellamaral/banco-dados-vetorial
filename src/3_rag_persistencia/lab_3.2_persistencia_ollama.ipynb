{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f31acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d4c7f5",
   "metadata": {},
   "source": [
    "## üì¶ Imports e Configura√ß√£o\n",
    "\n",
    "Bibliotecas necess√°rias:\n",
    "- `os`, `Path`, `load_dotenv`: Gerenciamento de arquivos e vari√°veis de ambiente\n",
    "- `FAISS`: Banco de dados vetorial (com suporte a salvar/carregar!)\n",
    "- `OllamaEmbeddings`: Embeddings locais via Ollama\n",
    "\n",
    "### Por que Ollama?\n",
    "\n",
    "Neste lab usamos Ollama porque:\n",
    "- ‚úÖ **100% Gratuito** - Nenhum custo de API\n",
    "- ‚úÖ **Privacidade Total** - Dados n√£o saem da sua m√°quina\n",
    "- ‚úÖ **Offline** - Funciona sem internet\n",
    "- ‚úÖ **Performance** - Roda local, sem lat√™ncia de rede\n",
    "- ‚úÖ **Modelos variados** - all-minilm, nomic-embed-text, mxbai-embed-large\n",
    "\n",
    "### Pr√©-requisitos\n",
    "\n",
    "1. **Docker Compose** deve estar rodando:\n",
    "   ```bash\n",
    "   docker-compose up -d\n",
    "   ```\n",
    "\n",
    "2. **Modelos instalados** (j√° inclu√≠dos no `Dockerfile.ollama`):\n",
    "   - `all-minilm` (embeddings - 384 dims)\n",
    "   - `nomic-embed-text` (embeddings - 768 dims)\n",
    "   - `llama3.2:1b` (LLM para gera√ß√£o)\n",
    "\n",
    "3. **Ollama acess√≠vel** em `http://ollama:11434` (dentro do Docker) ou `http://localhost:11434`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac669954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Ollama endpoint: http://localhost:11434\n",
      "‚úÖ Ollama conectado! Modelos dispon√≠veis: 6\n",
      "   - embeddinggemma:latest\n",
      "   - gpt-oss:latest\n",
      "   - llama3.2:1b\n",
      "   - all-minilm:latest\n",
      "   - mxbai-embed-large:latest\n",
      "   - nomic-embed-text:latest\n"
     ]
    }
   ],
   "source": [
    "# Configura√ß√£o do endpoint Ollama\n",
    "# Se estiver rodando fora do Docker, use: http://localhost:11434\n",
    "# Se estiver dentro do Docker (Jupyter no compose), use: http://ollama:11434\n",
    "OLLAMA_BASE_URL = 'http://localhost:11434'\n",
    "# OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')\n",
    "\n",
    "print(f'üîó Ollama endpoint: {OLLAMA_BASE_URL}')\n",
    "\n",
    "# Testar conex√£o\n",
    "try:\n",
    "    response = requests.get(f'{OLLAMA_BASE_URL}/api/tags')\n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get('models', [])\n",
    "        print(f'‚úÖ Ollama conectado! Modelos dispon√≠veis: {len(models)}')\n",
    "        for model in models:\n",
    "            print(f\"   - {model['name']}\")\n",
    "    else:\n",
    "        print('‚ö†Ô∏è  Ollama respondeu, mas com erro')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Erro ao conectar com Ollama: {e}')\n",
    "    print('   Certifique-se que o Docker Compose est√° rodando!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e63046",
   "metadata": {},
   "source": [
    "## üîê Verificando Conex√£o com Ollama\n",
    "\n",
    "Antes de come√ßar, precisamos garantir que o Ollama est√° acess√≠vel.\n",
    "\n",
    "**Troubleshooting:**\n",
    "\n",
    "Se voc√™ ver ‚ùå, verifique:\n",
    "1. Docker Compose est√° rodando? `docker-compose ps`\n",
    "2. Container Ollama est√° UP? `docker-compose logs ollama`\n",
    "3. Porta 11434 est√° exposta? Verifique o `docker-compose.yaml`\n",
    "\n",
    "**Endpoints:**\n",
    "- Dentro do Docker: `http://ollama:11434`\n",
    "- Fora do Docker: `http://localhost:11434`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cf238ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "meus_textos = [\n",
    "    \"O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.\",    # Tecnologia\n",
    "    \"Para fazer um bolo macio, bata as claras em neve.\",       # Culin√°ria\n",
    "    \"O atacante chutou a bola no √¢ngulo e foi gol.\",           # Esporte\n",
    "    \"A placa de v√≠deo RTX 4090 roda jogos em 4K.\",             # Tecnologia\n",
    "    \"Receita de lasanha √† bolonhesa com muito queijo.\"         # Culin√°ria\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c141e14d",
   "metadata": {},
   "source": [
    "## üìÑ Passo 1: Preparando os Documentos\n",
    "\n",
    "Nosso dataset de teste com 5 documentos de categorias diferentes.\n",
    "\n",
    "**Importante:** Em uma aplica√ß√£o RAG real, esses textos viriam de:\n",
    "- üìë PDFs de documenta√ß√£o\n",
    "- üåê Base de conhecimento da empresa\n",
    "- üí¨ FAQs e tickets de suporte\n",
    "- üìä Manuais t√©cnicos\n",
    "\n",
    "üí° **Conceito RAG:** Quanto mais documentos relevantes voc√™ indexar, melhor o LLM conseguir√° responder perguntas espec√≠ficas do seu dom√≠nio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "323eb8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelo de embeddings inicializado: mxbai-embed-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\AppData\\Local\\Temp\\ipykernel_46504\\4109340853.py:5: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# Escolha o modelo de embeddings\n",
    "# Op√ß√µes: 'all-minilm' (384 dims, r√°pido), 'nomic-embed-text' (768 dims, balanceado), 'mxbai-embed-large' (1024 dims, preciso)\n",
    "EMBEDDING_MODEL = 'mxbai-embed-large'\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    base_url=OLLAMA_BASE_URL\n",
    ")\n",
    "\n",
    "print(f'‚úÖ Modelo de embeddings inicializado: {EMBEDDING_MODEL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5398858c",
   "metadata": {},
   "source": [
    "## üß† Passo 2: Inicializando o Modelo de Embeddings\n",
    "\n",
    "Criamos uma inst√¢ncia do modelo de embeddings via Ollama local.\n",
    "\n",
    "**Modelo:** `nomic-embed-text` (recomendado)\n",
    "- **Dimens√µes:** 768 (excelente qualidade sem√¢ntica)\n",
    "- **Qualidade:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Estado da arte open-source\n",
    "- **Custo:** $0.00 (totalmente gratuito!)\n",
    "- **Velocidade:** ~50-100ms por request (local)\n",
    "- **Privacidade:** üîí 100% local, dados n√£o saem da m√°quina\n",
    "\n",
    "**Alternativas:**\n",
    "- `all-minilm`: 384 dims, mais r√°pido, menor qualidade\n",
    "- `mxbai-embed-large`: 1024 dims, melhor qualidade, mais lento\n",
    "\n",
    "### Como funciona?\n",
    "\n",
    "```\n",
    "Texto ‚Üí üè† Ollama Local ‚Üí Vetor [768 n√∫meros]\n",
    "```\n",
    "\n",
    "Cada texto ser√° transformado em um vetor de 768 dimens√µes que captura seu significado sem√¢ntico, **sem enviar dados para nuvem**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac4e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Criando √≠ndice FAISS... (pode levar alguns segundos)\n",
      "‚úÖ √çndice criado com sucesso!\n",
      "‚úÖ √çndice criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "print('‚è≥ Criando √≠ndice FAISS... (pode levar alguns segundos)')\n",
    "vector_store = FAISS.from_texts(meus_textos, embeddings)\n",
    "print('‚úÖ √çndice criado com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2e48d3",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Passo 3: Criando o √çndice Vetorial\n",
    "\n",
    "Transformamos os textos em vetores e criamos o √≠ndice FAISS **em mem√≥ria**.\n",
    "\n",
    "### O que acontece internamente?\n",
    "\n",
    "```text\n",
    "Para cada texto em meus_textos:\n",
    "  1. Envia para Ollama local (http://localhost:11434)\n",
    "  2. Recebe vetor de 768 dimens√µes\n",
    "  3. FAISS armazena o vetor em RAM\n",
    "\n",
    "Resultado:\n",
    "  vector_store = √çndice FAISS com 5 vetores em mem√≥ria\n",
    "```\n",
    "\n",
    "**Importante:** Neste ponto, o √≠ndice existe **apenas na RAM**! Se voc√™ fechar o programa, perde tudo.\n",
    "\n",
    "‚è±Ô∏è **Tempo:** ~0.5-1 segundo (5 requests locais ao Ollama)\n",
    "\n",
    "üí° **Vantagem Ollama:** Como √© local, √© **muito mais r√°pido** que APIs de nuvem (sem lat√™ncia de rede)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285e1c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Banco vetorial salvo com sucesso em: e:\\01-projetos\\11-work\\11.34-engenharia-vetorial\\src\\1_fundamentos\\..\\..\\data\\meu_indice_faiss_ollama\n"
     ]
    }
   ],
   "source": [
    "FAISS_PATH = Path.cwd().joinpath('..', '..', 'data', 'meu_indice_faiss_ollama')\n",
    "\n",
    "vector_store.save_local(str(FAISS_PATH))\n",
    "print(f\"‚úÖ Banco vetorial salvo com sucesso em: {FAISS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c604f4",
   "metadata": {},
   "source": [
    "## üíæ Passo 4: Salvando o √çndice no Disco (PERSIST√äNCIA!)\n",
    "\n",
    "Esta √© a **parte mais importante** para aplica√ß√µes RAG em produ√ß√£o!\n",
    "\n",
    "### O que acontece aqui?\n",
    "\n",
    "```python\n",
    "vector_store.save_local(str(FAISS_PATH))\n",
    "```\n",
    "\n",
    "**Internamente:**\n",
    "1. FAISS cria uma pasta: `data/meu_indice_faiss_ollama/`\n",
    "2. Salva 2 arquivos:\n",
    "   - `index.faiss` ‚Üí Os vetores e estrutura de √≠ndice\n",
    "   - `index.pkl` ‚Üí Metadados (textos originais, etc.)\n",
    "\n",
    "### Estrutura de arquivos criada:\n",
    "\n",
    "```text\n",
    "data/\n",
    "‚îî‚îÄ‚îÄ meu_indice_faiss_ollama/\n",
    "    ‚îú‚îÄ‚îÄ index.faiss  ‚Üê ~60KB (5 vetores √ó 768 dims √ó 4 bytes)\n",
    "    ‚îî‚îÄ‚îÄ index.pkl    ‚Üê ~2KB (textos originais)\n",
    "```\n",
    "\n",
    "### Por que isso √© importante?\n",
    "\n",
    "**Sem persist√™ncia:**\n",
    "```python\n",
    "# Toda execu√ß√£o:\n",
    "vector_store = FAISS.from_texts(textos, embeddings)\n",
    "# ‚Üë 5 requests ao Ollama + 0.5-1 segundo\n",
    "```\n",
    "\n",
    "**Com persist√™ncia:**\n",
    "```python\n",
    "# Apenas uma vez:\n",
    "vector_store.save_local(\"path\")\n",
    "\n",
    "# Depois, sempre:\n",
    "vector_store = FAISS.load_local(\"path\", embeddings)\n",
    "# ‚Üë 0 requests ao Ollama + ~10ms\n",
    "```\n",
    "\n",
    "### Benef√≠cios em produ√ß√£o\n",
    "\n",
    "| M√©trica | Sem Persist√™ncia | Com Persist√™ncia |\n",
    "|---------|------------------|------------------|\n",
    "| **Tempo de startup** | Segundos/Minutos | Milissegundos |\n",
    "| **Carga no Ollama** | Toda vez | Uma vez (na cria√ß√£o) |\n",
    "| **Escalabilidade** | Limitada | Milh√µes de docs |\n",
    "| **Confiabilidade** | Dependente do Ollama | Funciona offline |\n",
    "\n",
    "üí° **Analogia:** √â como salvar um documento do Word - voc√™ n√£o quer redigitar tudo cada vez que abrir!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a58616b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Carregando √≠ndice do disco...\n",
      "‚úÖ √çndice carregado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# CARREGAR do disco\n",
    "# O par√¢metro 'allow_dangerous_deserialization' √© necess√°rio em vers√µes recentes\n",
    "# para confirmar que voc√™ confia no arquivo que est√° carregando.\n",
    "print('‚è≥ Carregando √≠ndice do disco...')\n",
    "novo_db = FAISS.load_local(\n",
    "    str(FAISS_PATH), \n",
    "    embeddings, \n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "print('‚úÖ √çndice carregado com sucesso!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e7797e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Documentos no √≠ndice:\n",
      "  ‚Ä¢ O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.\n",
      "  ‚Ä¢ Para fazer um bolo macio, bata as claras em neve.\n",
      "  ‚Ä¢ O atacante chutou a bola no √¢ngulo e foi gol.\n",
      "  ‚Ä¢ A placa de v√≠deo RTX 4090 roda jogos em 4K.\n",
      "  ‚Ä¢ Receita de lasanha √† bolonhesa com muito queijo.\n"
     ]
    }
   ],
   "source": [
    "# Visualizar os textos carregados\n",
    "print('üìö Documentos no √≠ndice:')\n",
    "for doc_id in novo_db.index_to_docstore_id.values():\n",
    "    documento = novo_db.docstore.search(doc_id)\n",
    "    print(f'  ‚Ä¢ {documento.page_content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b85d32",
   "metadata": {},
   "source": [
    "## üìÇ Passo 5: Carregando o √çndice do Disco\n",
    "\n",
    "Agora vamos **carregar** o √≠ndice que salvamos anteriormente!\n",
    "\n",
    "### O que acontece aqui?\n",
    "\n",
    "```python\n",
    "novo_db = FAISS.load_local(str(FAISS_PATH), embeddings, allow_dangerous_deserialization=True)\n",
    "```\n",
    "\n",
    "**Internamente:**\n",
    "1. FAISS l√™ os arquivos da pasta `data/meu_indice_faiss_ollama/`\n",
    "2. Reconstr√≥i o √≠ndice em mem√≥ria (~10ms)\n",
    "3. Pronto para uso instant√¢neo!\n",
    "\n",
    "### Par√¢metros importantes\n",
    "\n",
    "**`embeddings`**: Necess√°rio para futuras queries\n",
    "- N√£o recalcula nada dos documentos existentes\n",
    "- S√≥ ser√° usado quando voc√™ buscar algo novo\n",
    "\n",
    "**`allow_dangerous_deserialization=True`**: Confirma√ß√£o de seguran√ßa\n",
    "- FAISS usa pickle para serializar\n",
    "- Pickle pode executar c√≥digo malicioso se vier de fonte n√£o confi√°vel\n",
    "- Como voc√™ mesmo criou o √≠ndice, √© seguro ‚úÖ\n",
    "\n",
    "### Fluxo completo de uma aplica√ß√£o RAG\n",
    "\n",
    "```text\n",
    "Primeira execu√ß√£o (setup):\n",
    "  1. Carregar documentos\n",
    "  2. Criar embeddings (via Ollama)\n",
    "  3. Criar √≠ndice FAISS\n",
    "  4. Salvar no disco ‚Üê Voc√™ est√° aqui!\n",
    "\n",
    "Execu√ß√µes subsequentes (produ√ß√£o):\n",
    "  5. Carregar √≠ndice do disco ‚Üê Estamos fazendo isso agora!\n",
    "  6. Receber query do usu√°rio\n",
    "  7. Buscar documentos relevantes\n",
    "  8. Enviar contexto + query para LLM\n",
    "  9. Retornar resposta\n",
    "```\n",
    "\n",
    "### Performance com Ollama\n",
    "\n",
    "| Opera√ß√£o | Tempo (5 docs) | Tempo (10k docs) | Tempo (1M docs) |\n",
    "|----------|----------------|------------------|------------------|\n",
    "| **Criar √≠ndice** | ~1s | ~15s | ~15min |\n",
    "| **Salvar** | ~50ms | ~200ms | ~5s |\n",
    "| **Carregar** | ~10ms | ~100ms | ~2s |\n",
    "| **Buscar** | <1ms | ~5ms | ~50ms |\n",
    "\n",
    "üí° **Observe:** Carregar √© **muito mais r√°pido** que criar! E com Ollama local, √© ainda mais r√°pido que APIs de nuvem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab0a9b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Buscando por: \"Sugest√£o de celular\"\n",
      "‚úÖ Encontrados 2 documentos relevantes\n",
      "‚úÖ Encontrados 2 documentos relevantes\n"
     ]
    }
   ],
   "source": [
    "# Usar normalmente\n",
    "consulta = \"Sugest√£o de celular\"\n",
    "print(f'üîç Buscando por: \"{consulta}\"')\n",
    "resultados = novo_db.similarity_search(consulta, k=2)\n",
    "print(f'‚úÖ Encontrados {len(resultados)} documentos relevantes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f6a54",
   "metadata": {},
   "source": [
    "## üîç Passo 6: Usando o √çndice Carregado\n",
    "\n",
    "Agora vamos fazer uma busca usando o √≠ndice que **carregamos do disco**!\n",
    "\n",
    "### A Query\n",
    "\n",
    "```python\n",
    "consulta = \"Sugest√£o de celular\"\n",
    "```\n",
    "\n",
    "**Desafio sem√¢ntico:**\n",
    "- A palavra \"iPhone\" n√£o aparece na query\n",
    "- A palavra \"celular\" sim\n",
    "- O modelo precisa entender que iPhone √© um celular\n",
    "\n",
    "### O que acontece internamente?\n",
    "\n",
    "1. **Query vira vetor (via Ollama local):**\n",
    "   ```text\n",
    "   \"Sugest√£o de celular\" ‚Üí üè† Ollama ‚Üí [0.15, -0.32, ..., 0.47]\n",
    "   ```\n",
    "\n",
    "2. **FAISS busca (local, instant√¢neo):**\n",
    "   ```text\n",
    "   Compara o vetor da query com os 5 vetores armazenados\n",
    "   Calcula dist√¢ncias:\n",
    "     - iPhone: 0.3 ‚Üê Mais pr√≥ximo!\n",
    "     - RTX 4090: 1.2\n",
    "     - Bolo: 2.5\n",
    "     - Gol: 2.8\n",
    "     - Lasanha: 2.6\n",
    "   ```\n",
    "\n",
    "3. **Retorna top-k (k=2):**\n",
    "   ```\n",
    "   [iPhone, RTX 4090]\n",
    "   ```\n",
    "\n",
    "### Par√¢metro k\n",
    "\n",
    "- `k=1`: Retorna apenas o mais similar\n",
    "- `k=2`: Retorna os 2 mais similares\n",
    "- `k=5`: Retorna todos (nosso dataset tem 5)\n",
    "\n",
    "üí° **Dica:** Em RAG, geralmente usa-se `k=3` ou `k=5` para dar contexto suficiente ao LLM sem sobrecarregar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446dfe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Documento mais relevante:\n",
      "   \"O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.\"\n"
     ]
    }
   ],
   "source": [
    "print('üìÑ Documento mais relevante:')\n",
    "print(f'   \"{resultados[0].page_content}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac8def",
   "metadata": {},
   "source": [
    "## üìä Passo 7: Visualizando o Resultado\n",
    "\n",
    "Vamos ver o documento mais relevante (√≠ndice 0 = primeiro resultado).\n",
    "\n",
    "**Esperado:** Deve retornar algo sobre iPhone, pois √© o documento sobre celular!\n",
    "\n",
    "### Interpretando o resultado\n",
    "\n",
    "Se voc√™ ver:\n",
    "- ‚úÖ `\"O novo iPhone 15...\"` ‚Üí Perfeito! A busca funcionou\n",
    "- ‚ùå Qualquer outro texto ‚Üí Algo deu errado (improv√°vel com nomic-embed-text)\n",
    "\n",
    "### Qualidade do modelo nomic-embed-text\n",
    "\n",
    "O modelo `nomic-embed-text` √© treinado especificamente para busca sem√¢ntica e tem **excelente desempenho** em tarefas de similaridade, compar√°vel a modelos comerciais!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd7ca9e",
   "metadata": {},
   "source": [
    "### Pr√≥ximo passo: RAG completo com Ollama\n",
    "\n",
    "Em um RAG de verdade com Ollama, voc√™ faria:\n",
    "\n",
    "```python\n",
    "# 1. Buscar documentos relevantes (j√° fizemos!)\n",
    "resultados = novo_db.similarity_search(consulta, k=3)\n",
    "\n",
    "# 2. Montar o contexto\n",
    "contexto = \"\\n\".join([doc.page_content for doc in resultados])\n",
    "\n",
    "# 3. Criar prompt para LLM\n",
    "prompt = f\"\"\"\n",
    "Baseado no seguinte contexto:\n",
    "{contexto}\n",
    "\n",
    "Responda a pergunta: {consulta}\n",
    "\"\"\"\n",
    "\n",
    "# 4. Enviar para LLM Ollama (Llama, Mistral, etc.)\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3.2:1b\", base_url=OLLAMA_BASE_URL)\n",
    "resposta = llm.invoke(prompt)\n",
    "\n",
    "print(resposta)\n",
    "# ‚Üí \"Eu recomendo o iPhone 15, que possui uma lente perisc√≥pica incr√≠vel...\"\n",
    "```\n",
    "\n",
    "üí° **Esse √© o poder do RAG:** O LLM responde com base nos **seus** documentos, n√£o apenas no conhecimento geral! E tudo **100% local e gratuito**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c925e963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fun√ß√£o RAG completa criada!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "def rag_query(pergunta, k=2, modelo='llama3.2:1b'):\n",
    "    \"\"\"\n",
    "    Realiza uma busca RAG completa com Ollama local:\n",
    "    1. Carrega o √≠ndice do disco\n",
    "    2. Busca documentos relevantes\n",
    "    3. Envia para o LLM local gerar a resposta\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Carregar √≠ndice do disco (garantindo que usamos a vers√£o persistida)\n",
    "    db = FAISS.load_local(str(FAISS_PATH), embeddings, allow_dangerous_deserialization=True)\n",
    "    \n",
    "    # 2. Buscar documentos relevantes (Retrieval)\n",
    "    docs = db.similarity_search(pergunta, k=k)\n",
    "    \n",
    "    # 3. Montar o contexto (Augmentation)\n",
    "    contexto = \"\\n\".join([f\"- {doc.page_content}\" for doc in docs])\n",
    "    \n",
    "    # 4. Criar prompt\n",
    "    prompt = f\"\"\"\n",
    "Voc√™ √© um assistente inteligente. Use APENAS o contexto abaixo para responder a pergunta do usu√°rio.\n",
    "Se a resposta n√£o estiver no contexto, diga educadamente que n√£o possui essa informa√ß√£o.\n",
    "\n",
    "Contexto:\n",
    "{contexto}\n",
    "\n",
    "Pergunta: {pergunta}\n",
    "\n",
    "Resposta:\"\"\"\n",
    "    \n",
    "    # 5. Gerar resposta (Generation) - usando Llama local\n",
    "    llm = Ollama(\n",
    "        model=modelo,  # Modelo leve e r√°pido\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    resposta = llm.invoke(prompt)\n",
    "    \n",
    "    # Retorna a resposta (texto) e os documentos usados (fonte)\n",
    "    return resposta, docs\n",
    "\n",
    "print('‚úÖ Fun√ß√£o RAG completa criada!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c00d80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Pergunta: Qual celular tem uma c√¢mera boa?\n",
      "\n",
      "‚è≥ Processando RAG local...\n",
      "\n",
      "üìù Resposta do Llama (local):\n",
      "----------------------------------------\n",
      "O iPhone‚ÄØ15 tem uma c√¢mera excelente, gra√ßas √† sua lente perisc√≥pica incr√≠vel.\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "üìö Documentos usados no contexto:\n",
      "   1. O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.\n",
      "   2. O atacante chutou a bola no √¢ngulo e foi gol.\n",
      "üìù Resposta do Llama (local):\n",
      "----------------------------------------\n",
      "O iPhone‚ÄØ15 tem uma c√¢mera excelente, gra√ßas √† sua lente perisc√≥pica incr√≠vel.\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "üìö Documentos usados no contexto:\n",
      "   1. O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.\n",
      "   2. O atacante chutou a bola no √¢ngulo e foi gol.\n"
     ]
    }
   ],
   "source": [
    "# Vamos testar nosso sistema RAG!\n",
    "pergunta = \"Qual celular tem uma c√¢mera boa?\"\n",
    "\n",
    "print(f\"ü§ñ Pergunta: {pergunta}\\n\")\n",
    "print(\"‚è≥ Processando RAG local...\\n\")\n",
    "\n",
    "resposta_final, docs_usados = rag_query(pergunta, modelo='gpt-oss:latest')\n",
    "\n",
    "print(\"üìù Resposta do Llama (local):\")\n",
    "print(\"-\" * 40)\n",
    "print(resposta_final)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n\\nüìö Documentos usados no contexto:\")\n",
    "for i, doc in enumerate(docs_usados):\n",
    "    print(f\"   {i+1}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f91d8",
   "metadata": {},
   "source": [
    "### üß† Entendendo a \"M√°gica\"\n",
    "\n",
    "Observe o que aconteceu acima:\n",
    "\n",
    "1.  **Recupera√ß√£o (Retrieval):** O sistema buscou no FAISS os trechos mais parecidos com \"c√¢mera boa\". Ele encontrou reviews de celulares (ex: \"O iPhone 15 tem uma lente perisc√≥pica incr√≠vel...\").\n",
    "2.  **Augmenta√ß√£o (Augmentation):** Pegamos esses trechos e colamos no prompt do Llama.\n",
    "3.  **Gera√ß√£o (Generation):** O Llama leu os trechos e respondeu a pergunta **baseado apenas neles**.\n",
    "\n",
    "√â como se voc√™ desse um livro para algu√©m e dissesse: \"Responda a pergunta usando APENAS este livro\".\n",
    "\n",
    "### üöÄ Vantagens do RAG com Ollama\n",
    "\n",
    "‚úÖ **100% Gratuito** - Sem custos de API  \n",
    "‚úÖ **Privacidade Total** - Dados n√£o saem da m√°quina  \n",
    "‚úÖ **Offline** - Funciona sem internet  \n",
    "‚úÖ **R√°pido** - Sem lat√™ncia de rede  \n",
    "‚úÖ **Escal√°vel** - Pode rodar em servidores pr√≥prios  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb2d7d",
   "metadata": {},
   "source": [
    "### üß™ Experimento: Perguntas fora do contexto\n",
    "\n",
    "O que acontece se perguntarmos algo que **n√£o** est√° nos documentos que indexamos?\n",
    "Vamos testar com uma pergunta sobre futebol (nossos documentos s√£o sobre tecnologia/celulares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcb19abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Pergunta: Quem ganhou a copa de 1994?\n",
      "\n",
      "üìù Resposta do Llama:\n",
      "----------------------------------------\n",
      "Desculpe, mas n√£o tenho essa informa√ß√£o no contexto fornecido.\n",
      "----------------------------------------\n",
      "\n",
      "üìö Documentos recuperados (provavelmente irrelevantes):\n",
      "   1. O atacante chutou a bola no √¢ngulo e foi gol.\n",
      "   2. Receita de lasanha √† bolonhesa com muito queijo.\n",
      "üìù Resposta do Llama:\n",
      "----------------------------------------\n",
      "Desculpe, mas n√£o tenho essa informa√ß√£o no contexto fornecido.\n",
      "----------------------------------------\n",
      "\n",
      "üìö Documentos recuperados (provavelmente irrelevantes):\n",
      "   1. O atacante chutou a bola no √¢ngulo e foi gol.\n",
      "   2. Receita de lasanha √† bolonhesa com muito queijo.\n"
     ]
    }
   ],
   "source": [
    "pergunta_fora = \"Quem ganhou a copa de 1994?\"\n",
    "\n",
    "print(f\"ü§ñ Pergunta: {pergunta_fora}\\n\")\n",
    "resposta_fora, docs_fora = rag_query(pergunta_fora, modelo='gpt-oss:latest')\n",
    "\n",
    "print(\"üìù Resposta do Llama:\")\n",
    "print(\"-\" * 40)\n",
    "print(resposta_fora)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nüìö Documentos recuperados (provavelmente irrelevantes):\")\n",
    "for i, doc in enumerate(docs_fora):\n",
    "    print(f\"   {i+1}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46474b3",
   "metadata": {},
   "source": [
    "### üí° An√°lise do Erro (ou Sucesso)\n",
    "\n",
    "Se o modelo respondeu \"N√£o sei\" ou deu uma resposta estranha, isso √© **bom**!\n",
    "Significa que ele est√° respeitando o contexto. Como n√£o indexamos nada sobre futebol, ele n√£o encontrou informa√ß√µes para responder.\n",
    "\n",
    "**Li√ß√£o:** O RAG depende 100% da qualidade da busca.\n",
    "*   **Busca Ruim** (Garbage In) ‚Üí **Resposta Ruim** (Garbage Out).\n",
    "\n",
    "Por isso, gastamos tanto tempo nos labs anteriores aprendendo sobre Embeddings e Busca Vetorial! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f7cecf",
   "metadata": {},
   "source": [
    "### üîç Analisando o Resultado\n",
    "\n",
    "Observe que o modelo n√£o apenas \"copiou\" o texto, mas **interpretou** a informa√ß√£o.\n",
    "\n",
    "- Voc√™ perguntou sobre \"c√¢mera boa\"\n",
    "- O documento falava sobre \"lente perisc√≥pica incr√≠vel\"\n",
    "- O LLM conectou os pontos e recomendou o iPhone 15!\n",
    "\n",
    "Isso √© muito mais poderoso que uma busca simples por palavras-chave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e2fcd",
   "metadata": {},
   "source": [
    "## üéì Resumo e Conceitos-Chave\n",
    "\n",
    "### O que aprendemos\n",
    "\n",
    "‚úÖ **Persist√™ncia de √≠ndices** - Salvar e carregar FAISS do disco  \n",
    "‚úÖ **Ollama local** - Embeddings e LLM 100% gratuitos e privados  \n",
    "‚úÖ **Performance** - Carregar √© 100x mais r√°pido que criar  \n",
    "‚úÖ **RAG completo** - Retrieval + Augmentation + Generation  \n",
    "‚úÖ **Privacidade** - Dados nunca saem da sua m√°quina\n",
    "\n",
    "### Fluxo completo que implementamos\n",
    "\n",
    "```text\n",
    "1. Criar embeddings (via Ollama local)\n",
    "   ‚Üì\n",
    "2. Criar √≠ndice FAISS (em RAM)\n",
    "   ‚Üì\n",
    "3. Salvar no disco (.save_local) ‚Üê PERSIST√äNCIA!\n",
    "   ‚Üì\n",
    "4. Carregar do disco (.load_local) ‚Üê REUTILIZA√á√ÉO!\n",
    "   ‚Üì\n",
    "5. Fazer buscas normalmente\n",
    "   ‚Üì\n",
    "6. Gerar respostas com Llama local\n",
    "```\n",
    "\n",
    "### Compara√ß√£o: Ollama vs APIs de Nuvem\n",
    "\n",
    "| Aspecto | Ollama Local | APIs Nuvem |\n",
    "|---------|--------------|------------|\n",
    "| **Custo** | $0.00 | $0.02-0.10 por 1M tokens |\n",
    "| **Privacidade** | ‚úÖ 100% local | ‚ùå Dados v√£o para nuvem |\n",
    "| **Lat√™ncia** | ~50-100ms | ~200-500ms |\n",
    "| **Offline** | ‚úÖ Funciona | ‚ùå Precisa internet |\n",
    "| **Escalabilidade** | Limitada por hardware | Ilimitada |\n",
    "| **Qualidade** | ‚≠ê‚≠ê‚≠ê‚≠ê Excelente | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Melhor |\n",
    "\n",
    "### Aplica√ß√µes reais de RAG com Ollama\n",
    "\n",
    "ü§ñ **Chatbots corporativos privados**\n",
    "- Indexa documenta√ß√£o interna sens√≠vel\n",
    "- LLM responde baseado nos docs da empresa\n",
    "- Dados nunca saem do servidor\n",
    "\n",
    "üìö **Assistentes de estudo offline**\n",
    "- Indexa livros, apostilas, anota√ß√µes\n",
    "- Funciona sem internet\n",
    "- Totalmente gratuito\n",
    "\n",
    "üîß **Suporte t√©cnico local**\n",
    "- Indexa manuais, FAQs, tickets antigos\n",
    "- Resposta instant√¢nea\n",
    "- Sem custos de API\n",
    "\n",
    "üè• **Assist√™ncia m√©dica privada**\n",
    "- Indexa prontu√°rios, estudos cient√≠ficos\n",
    "- 100% privado (LGPD/HIPAA compliant)\n",
    "- LLM auxilia diagn√≥sticos localmente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de69e5c4",
   "metadata": {},
   "source": [
    "### üß™ Experimentos para tentar\n",
    "\n",
    "#### Experimento 1: Testar modelos diferentes\n",
    "```python\n",
    "# Embeddings menores (mais r√°pido)\n",
    "embeddings_small = OllamaEmbeddings(\n",
    "    model='all-minilm',  # 384 dims\n",
    "    base_url=OLLAMA_BASE_URL\n",
    ")\n",
    "\n",
    "# Embeddings maiores (mais qualidade)\n",
    "embeddings_large = OllamaEmbeddings(\n",
    "    model='mxbai-embed-large',  # 1024 dims\n",
    "    base_url=OLLAMA_BASE_URL\n",
    ")\n",
    "\n",
    "# Compare a qualidade das buscas!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263cfadb",
   "metadata": {},
   "source": [
    "#### Experimento 2: LLMs diferentes\n",
    "\n",
    "```python\n",
    "# Llama menor (mais r√°pido)\n",
    "llm_fast = Ollama(model=\"llama3.2:1b\", base_url=OLLAMA_BASE_URL)\n",
    "\n",
    "# Llama maior (melhor qualidade)\n",
    "llm_quality = Ollama(model=\"llama3.2:3b\", base_url=OLLAMA_BASE_URL)\n",
    "\n",
    "# Mistral (alternativa)\n",
    "llm_mistral = Ollama(model=\"mistral:7b\", base_url=OLLAMA_BASE_URL)\n",
    "\n",
    "# Compare as respostas!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6abb2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452309ad",
   "metadata": {},
   "source": [
    "#### Experimento 3: Adicionar mais documentos\n",
    "```python\n",
    "\n",
    "novos_textos = [\n",
    "    \"Samsung Galaxy S23 tem c√¢mera de 200MP\",\n",
    "    \"Xiaomi Redmi Note 12 √© um bom custo-benef√≠cio\",\n",
    "    \"Google Pixel 8 Pro tem excelente fotografia noturna\"\n",
    "]\n",
    "\n",
    "# Criar novo √≠ndice com todos os documentos\n",
    "todos_textos = meus_textos + novos_textos\n",
    "vector_store = FAISS.from_texts(todos_textos, embeddings)\n",
    "vector_store.save_local(str(FAISS_PATH))\n",
    "\n",
    "# Agora tem 8 documentos!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066de1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b218e",
   "metadata": {},
   "source": [
    "#### Experimento 4: RAG com streaming\n",
    "```python\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "def rag_stream(pergunta, k=2):\n",
    "    \"\"\"RAG com resposta em streaming (efeito ChatGPT)\"\"\"\n",
    "    db = FAISS.load_local(str(FAISS_PATH), embeddings, allow_dangerous_deserialization=True)\n",
    "    docs = db.similarity_search(pergunta, k=k)\n",
    "    contexto = \"\\n\".join([f\"- {doc.page_content}\" for doc in docs])\n",
    "    \n",
    "    prompt = f\"\"\"Contexto:\\n{contexto}\\n\\nPergunta: {pergunta}\\n\\nResposta:\"\"\"\n",
    "    \n",
    "    llm = Ollama(model=\"llama3.2:1b\", base_url=OLLAMA_BASE_URL)\n",
    "    \n",
    "    # Streaming!\n",
    "    for chunk in llm.stream(prompt):\n",
    "        print(chunk, end='', flush=True)\n",
    "\n",
    "# Testar\n",
    "rag_stream(\"Qual √© o melhor celular?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676520e6",
   "metadata": {},
   "source": [
    "### üí° Boas pr√°ticas em produ√ß√£o com Ollama\n",
    "\n",
    "1. **Versionamento de √≠ndices**\n",
    "   ```python\n",
    "   vector_store.save_local(\"indices/v1.0.0\")\n",
    "   vector_store.save_local(\"indices/v1.1.0\")  # Ap√≥s adicionar docs\n",
    "   ```\n",
    "\n",
    "2. **Monitoramento de recursos**\n",
    "   ```python\n",
    "   import psutil\n",
    "   \n",
    "   # CPU\n",
    "   cpu_percent = psutil.cpu_percent(interval=1)\n",
    "   print(f\"CPU: {cpu_percent}%\")\n",
    "   \n",
    "   # Mem√≥ria\n",
    "   mem = psutil.virtual_memory()\n",
    "   print(f\"RAM: {mem.percent}%\")\n",
    "   ```\n",
    "\n",
    "3. **Metadados do √≠ndice**\n",
    "   ```python\n",
    "   import json\n",
    "   from datetime import datetime\n",
    "   \n",
    "   metadata = {\n",
    "       \"created_at\": datetime.now().isoformat(),\n",
    "       \"num_docs\": len(meus_textos),\n",
    "       \"embedding_model\": EMBEDDING_MODEL,\n",
    "       \"dimensions\": 768,\n",
    "       \"llm_model\": \"llama3.2:1b\"\n",
    "   }\n",
    "   \n",
    "   with open(FAISS_PATH / \"metadata.json\", \"w\") as f:\n",
    "       json.dump(metadata, f, indent=2)\n",
    "   ```\n",
    "\n",
    "4. **Cache de respostas**\n",
    "   ```python\n",
    "   from functools import lru_cache\n",
    "   \n",
    "   @lru_cache(maxsize=100)\n",
    "   def rag_query_cached(pergunta, k=2):\n",
    "       return rag_query(pergunta, k)\n",
    "   \n",
    "   # Perguntas repetidas s√£o instant√¢neas!\n",
    "   ```\n",
    "\n",
    "5. **Health check do Ollama**\n",
    "   ```python\n",
    "   def check_ollama_health():\n",
    "       try:\n",
    "           response = requests.get(f'{OLLAMA_BASE_URL}/api/tags', timeout=5)\n",
    "           return response.status_code == 200\n",
    "       except:\n",
    "           return False\n",
    "   \n",
    "   if not check_ollama_health():\n",
    "       print(\"‚ö†Ô∏è  Ollama n√£o est√° respondendo!\")\n",
    "   ```\n",
    "\n",
    "üí° **Dica final:** O RAG com Ollama √© **ideal** para aplica√ß√µes que precisam de **privacidade**, **controle de custos** e **funcionamento offline**. √â a escolha perfeita para ambientes corporativos sens√≠veis ou aplica√ß√µes que precisam rodar em edge devices!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "11-34-engenharia-vetorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
