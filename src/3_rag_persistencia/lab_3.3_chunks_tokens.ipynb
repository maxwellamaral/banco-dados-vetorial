{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d432964",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Conceitos Fundamentais\n",
    "\n",
    "### O que s√£o Tokens? üî§\n",
    "\n",
    "**Tokens** s√£o as unidades b√°sicas de processamento de um modelo de linguagem (LLM).\n",
    "\n",
    "**Analogia:** Imagine que voc√™ est√° lendo um livro em outro idioma. Voc√™ n√£o l√™ letra por letra, mas sim **palavra por palavra** (ou √†s vezes s√≠labas). Para um LLM, os tokens s√£o essas \"palavras\".\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- 1 token ‚âà 4 caracteres em portugu√™s (varia conforme o modelo)\n",
    "- Palavras comuns = 1 token (ex: \"gato\", \"casa\")\n",
    "- Palavras raras = m√∫ltiplos tokens (ex: \"extraordin√°rio\" = 3-4 tokens)\n",
    "- Espa√ßos e pontua√ß√£o tamb√©m s√£o tokens\n",
    "\n",
    "**Por que importam?**\n",
    "- LLMs t√™m limite de tokens (ex: GPT-4 = 8k, 32k ou 128k tokens)\n",
    "- APIs cobram por token processado\n",
    "- Influenciam velocidade de processamento\n",
    "\n",
    "---\n",
    "\n",
    "### O que s√£o Chunks? üì¶\n",
    "\n",
    "**Chunks** s√£o peda√ßos de texto criados para **armazenamento e busca** em bancos vetoriais.\n",
    "\n",
    "**Analogia:** Imagine que voc√™ est√° organizando uma biblioteca. Ao inv√©s de guardar livros inteiros em uma prateleira (muito grande), voc√™ divide cada livro em **cap√≠tulos** (chunks). Assim, quando algu√©m pergunta \"onde est√° a informa√ß√£o sobre fotoss√≠ntese?\", voc√™ n√£o precisa trazer o livro inteiro de biologia, apenas o cap√≠tulo relevante.\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- Definidos em **caracteres** (ex: 1000 chars) ou **tokens** (ex: 256 tokens)\n",
    "- Geralmente t√™m **overlap** (sobreposi√ß√£o) para n√£o perder contexto\n",
    "- Cada chunk vira **1 vetor** no banco vetorial\n",
    "\n",
    "**Por que importam?**\n",
    "- Buscas mais precisas (chunks menores = mais espec√≠ficos)\n",
    "- Controle de custo (menos tokens enviados ao LLM)\n",
    "- Performance do banco vetorial (chunks balanceados = busca eficiente)\n",
    "\n",
    "![Vectors chuck example](vectors_chunk_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4bf410",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç A Diferen√ßa Crucial\n",
    "\n",
    "| Aspecto | Tokens üî§ | Chunks üì¶ |\n",
    "|---------|-----------|----------|\n",
    "| **Defini√ß√£o** | Unidades de processamento do LLM | Peda√ßos de texto para busca |\n",
    "| **Criado por** | Tokenizador do modelo (ex: tiktoken) | Voc√™/desenvolvedor (splitter) |\n",
    "| **Medido em** | Quantidade de subpalavras | Caracteres ou tokens |\n",
    "| **Usado para** | Limites de contexto, custos de API | Indexa√ß√£o em banco vetorial |\n",
    "| **Exemplo** | \"extraordin√°rio\" = 3 tokens | \"1000 caracteres de um PDF\" = 1 chunk |\n",
    "| **Import√¢ncia em RAG** | Define quanto cabe no LLM | Define granularidade da busca |\n",
    "\n",
    "### üí° Exemplo Pr√°tico\n",
    "\n",
    "Imagine um artigo cient√≠fico de 10.000 caracteres:\n",
    "\n",
    "**Chunking:**\n",
    "- Voc√™ divide em 5 chunks de 2000 chars cada\n",
    "- Cada chunk vira 1 embedding no FAISS/Qdrant\n",
    "- Na busca, voc√™ recupera 2 chunks relevantes\n",
    "\n",
    "**Tokeniza√ß√£o:**\n",
    "- Os 2 chunks recuperados (4000 chars) = ~1000 tokens\n",
    "- Esses 1000 tokens s√£o enviados ao LLM como contexto\n",
    "- O LLM precisa caber isso + sua resposta no limite de tokens\n",
    "\n",
    "**Em resumo:**\n",
    "- **Chunks:** Como voc√™ organiza documentos para BUSCAR\n",
    "- **Tokens:** Como o LLM LIDA com o que voc√™ encontrou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c359ff51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ†Ô∏è Setup: Instala√ß√£o de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d3f9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Importa√ß√µes necess√°rias\n",
    "import tiktoken\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c88ef9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¢ Calculadora de Tokens\n",
    "\n",
    "Vamos criar uma fun√ß√£o para contar tokens usando `tiktoken`, a biblioteca oficial da OpenAI.\n",
    "\n",
    "Um simulador pode ser visto em https://platform.openai.com/tokenizer\n",
    "\n",
    "### üìñ O que √© tiktoken?\n",
    "\n",
    "`tiktoken` √© um tokenizador criado pela OpenAI que mostra **exatamente** como modelos como GPT-3.5 e GPT-4 quebram texto em tokens.\n",
    "\n",
    "**Encodings dispon√≠veis:**\n",
    "- `cl100k_base`: GPT-4, GPT-3.5-turbo, text-embedding-ada-002\n",
    "- `p50k_base`: Codex, text-davinci-002, text-davinci-003\n",
    "- `r50k_base`: GPT-3 (davinci, curie, babbage, ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1af8cda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Calculadora de tokens criada!\n"
     ]
    }
   ],
   "source": [
    "def calc_tokens(text: str, encoding_name: str = \"cl100k_base\") -> dict:\n",
    "    \"\"\"\n",
    "    Calcula tokens de um texto usando tiktoken.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto para tokenizar\n",
    "        encoding_name: Nome do encoding (padr√£o: cl100k_base para GPT-4)\n",
    "    \n",
    "    Returns:\n",
    "        Dict com estat√≠sticas de tokens\n",
    "    \"\"\"\n",
    "    # Carrega o encoding\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    \n",
    "    # Tokeniza\n",
    "    tokens = encoding.encode(text)\n",
    "    \n",
    "    # Calcula estat√≠sticas\n",
    "    num_chars = len(text)\n",
    "    num_tokens = len(tokens)\n",
    "    ratio = num_chars / num_tokens if num_tokens > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"texto\": text[:100] + \"...\" if len(text) > 100 else text,\n",
    "        \"caracteres\": num_chars,\n",
    "        \"tokens\": num_tokens,\n",
    "        \"chars_por_token\": round(ratio, 2),\n",
    "        \"encoding\": encoding_name,\n",
    "        \"tokens_list\": tokens[:20]  # Primeiros 20 tokens para inspe√ß√£o\n",
    "    }\n",
    "\n",
    "# Fun√ß√£o para exibir de forma bonita\n",
    "def show_token_counts(results: dict):\n",
    "    \"\"\"Exibe resultado da contagem de tokens de forma formatada.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä AN√ÅLISE DE TOKENS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nüìù Texto (preview): {results['texto']}\")\n",
    "    print(f\"\\nüìè Caracteres: {results['caracteres']:,}\")\n",
    "    print(f\"üî¢ Tokens: {results['tokens']:,}\")\n",
    "    print(f\"üìê Propor√ß√£o: {results['chars_por_token']} chars/token\")\n",
    "    print(f\"‚öôÔ∏è  Encoding: {results['encoding']}\")\n",
    "    print(f\"\\nüîç Primeiros 20 IDs de tokens: {results['tokens_list']}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Calculadora de tokens criada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1445397",
   "metadata": {},
   "source": [
    "### üß™ Teste 1: Frase Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddcd56b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä AN√ÅLISE DE TOKENS\n",
      "================================================================================\n",
      "\n",
      "üìù Texto (preview): Ol√°, meu nome √© Jo√£o e eu estudo banco de dados vetoriais!\n",
      "\n",
      "üìè Caracteres: 58\n",
      "üî¢ Tokens: 19\n",
      "üìê Propor√ß√£o: 3.05 chars/token\n",
      "‚öôÔ∏è  Encoding: cl100k_base\n",
      "\n",
      "üîç Primeiros 20 IDs de tokens: [43819, 1995, 11, 56309, 17567, 4046, 11186, 3496, 384, 15925, 1826, 7835, 53565, 409, 29045, 24195, 11015, 285, 0]\n",
      "================================================================================\n",
      "\n",
      "üí° INSIGHT:\n",
      "   Uma frase de 58 caracteres = 19 tokens\n",
      "   Propor√ß√£o: ~3.05 caracteres por token\n"
     ]
    }
   ],
   "source": [
    "# Teste com frase simples\n",
    "text = \"Ol√°, meu nome √© Jo√£o e eu estudo banco de dados vetoriais!\"\n",
    "\n",
    "results = calc_tokens(text)\n",
    "show_token_counts(results)\n",
    "\n",
    "# Insight\n",
    "print(\"üí° INSIGHT:\")\n",
    "print(f\"   Uma frase de {results['caracteres']} caracteres = {results['tokens']} tokens\")\n",
    "print(f\"   Propor√ß√£o: ~{results['chars_por_token']} caracteres por token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd30420",
   "metadata": {},
   "source": [
    "### üß™ Teste 2: Texto T√©cnico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d40baa",
   "metadata": {},
   "source": [
    "**Observa√ß√£o**: Os tokenizadores gen√©ricos como o `cl100k_base` da OpenAI n√£o s√£o otimizados para portugu√™s, resultando em mais tokens necess√°rios para representar o mesmo texto. O BERTugues, ao remover caracteres irrelevantes para portugu√™s, consegue reduzir significativamente o tamanho da representa√ß√£o, economizando no processamento e custos de API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80208ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com texto t√©cnico (t√≠pico de RAG)\n",
    "text_pt = \"\"\"\n",
    "Retrieval-Augmented Generation (RAG) √© uma arquitetura que combina recupera√ß√£o de informa√ß√µes \n",
    "com gera√ß√£o de linguagem natural. O sistema funciona em duas etapas: primeiro, um retriever \n",
    "busca documentos relevantes em um banco vetorial usando embeddings sem√¢nticos; depois, um \n",
    "Large Language Model (LLM) utiliza esses documentos como contexto para gerar respostas precisas \n",
    "e fundamentadas. A vantagem principal do RAG √© a capacidade de acessar conhecimento externo sem \n",
    "necessidade de fine-tuning do modelo.\n",
    "\"\"\"\n",
    "\n",
    "text_en = \"\"\"\n",
    "Retrieval-Augmented Generation (RAG) is an architecture that combines information retrieval\n",
    "with natural language generation. The system operates in two stages: first, a retriever\n",
    "searches for relevant documents in a vector database using semantic embeddings; then, a\n",
    "Large Language Model (LLM) uses these documents as context to generate accurate and\n",
    "grounded responses. The main advantage of RAG is the ability to access external knowledge\n",
    "without the need for fine-tuning the model.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76c44aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä AN√ÅLISE DE TOKENS\n",
      "================================================================================\n",
      "\n",
      "üìù Texto (preview): \n",
      "Retrieval-Augmented Generation (RAG) is an architecture that combines information retrieval\n",
      "with na...\n",
      "\n",
      "üìè Caracteres: 487\n",
      "üî¢ Tokens: 99\n",
      "üìê Propor√ß√£o: 4.92 chars/token\n",
      "‚öôÔ∏è  Encoding: cl100k_base\n",
      "\n",
      "üîç Primeiros 20 IDs de tokens: [198, 12289, 7379, 838, 62735, 28078, 24367, 320, 49, 1929, 8, 374, 459, 18112, 430, 33511, 2038, 57470, 198, 4291]\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä AN√ÅLISE DE TOKENS\n",
      "================================================================================\n",
      "\n",
      "üìù Texto (preview): \n",
      "Retrieval-Augmented Generation (RAG) √© uma arquitetura que combina recupera√ß√£o de informa√ß√µes \n",
      "com ...\n",
      "\n",
      "üìè Caracteres: 512\n",
      "üî¢ Tokens: 125\n",
      "üìê Propor√ß√£o: 4.1 chars/token\n",
      "‚öôÔ∏è  Encoding: cl100k_base\n",
      "\n",
      "üîç Primeiros 20 IDs de tokens: [198, 12289, 7379, 838, 62735, 28078, 24367, 320, 49, 1929, 8, 4046, 10832, 802, 32637, 295, 5808, 1744, 3698, 2259]\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenization_result_pt = calc_tokens(text_pt)\n",
    "tokenization_result_en = calc_tokens(text_en)\n",
    "show_token_counts(tokenization_result_en)\n",
    "show_token_counts(tokenization_result_pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d5999fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ COMPARA√á√ÉO DE ENCODINGS:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Encoding",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Usado em",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Tokens",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Chars/Token",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "0916303a-4762-4d76-b6d7-e20680aa544e",
       "rows": [
        [
         "0",
         "cl100k_base",
         "GPT-4",
         "99",
         "4.92"
        ],
        [
         "1",
         "p50k_base",
         "GPT-3",
         "104",
         "4.68"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Encoding</th>\n",
       "      <th>Usado em</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Chars/Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cl100k_base</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>99</td>\n",
       "      <td>4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p50k_base</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>104</td>\n",
       "      <td>4.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Encoding Usado em  Tokens  Chars/Token\n",
       "0  cl100k_base    GPT-4      99         4.92\n",
       "1    p50k_base    GPT-3     104         4.68"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° INSIGHT:\n",
      "   Diferentes modelos tokenizam de forma diferente!\n",
      "   Sempre use o encoding correspondente ao seu modelo.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compara√ß√£o com diferentes encodings\n",
    "print(\"\\nüî¨ COMPARA√á√ÉO DE ENCODINGS:\\n\")\n",
    "encodings = [\n",
    "    \"cl100k_base\",  # Usado na fam√≠lia GPT‚Äë3.5 e GPT‚Äë4 (por exemplo, gpt‚Äë3.5‚Äëturbo e gpt‚Äë4)\n",
    "    \"p50k_base\",    # Foi usado em modelos GPT‚Äë3 mais novos (e em Codex), com vocabul√°rio de 50k tokens.\n",
    "\n",
    "    ]\n",
    "encoding_comparison = []\n",
    "\n",
    "for enc in encodings:\n",
    "    res = calc_tokens(text_en, enc)\n",
    "    encoding_comparison.append({\n",
    "        \"Encoding\": enc,\n",
    "        \"Usado em\": \"GPT-4\" if enc == \"cl100k_base\" else \"GPT-3\",\n",
    "        \"Tokens\": res['tokens'],\n",
    "        \"Chars/Token\": res['chars_por_token']\n",
    "    })\n",
    "\n",
    "df_encodings = pd.DataFrame(encoding_comparison)\n",
    "display(df_encodings)\n",
    "\n",
    "print(\"\\nüí° INSIGHT:\")\n",
    "print(\"   Diferentes modelos tokenizam de forma diferente!\")\n",
    "print(\"   Sempre use o encoding correspondente ao seu modelo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8606608",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Calculadora de Chunks\n",
    "\n",
    "Agora vamos criar uma calculadora de chunks que mostra como diferentes configura√ß√µes afetam a divis√£o do texto.\n",
    "\n",
    "### Par√¢metros Principais do Chunking\n",
    "\n",
    "1. **chunk_size**: Tamanho m√°ximo de cada chunk (em caracteres)\n",
    "2. **chunk_overlap**: Quantos caracteres de sobreposi√ß√£o entre chunks consecutivos\n",
    "3. **separators**: Ordem de prioridade para quebrar o texto (ex: par√°grafos ‚Üí linhas ‚Üí palavras)\n",
    "\n",
    "**Por que overlap √© importante?**\n",
    "- Evita cortar frases/conceitos importantes no meio\n",
    "- Mant√©m contexto entre chunks adjacentes\n",
    "- Geralmente usa-se 10-20% do chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96315e60",
   "metadata": {},
   "source": [
    "**Como funciona os separadores**\n",
    "\n",
    "Os **separadores** definem a **ordem de prioridade** de onde o texto pode ser quebrado ao criar chunks. √â uma lista hier√°rquica que o `RecursiveCharacterTextSplitter` usa para dividir o texto de forma inteligente.\n",
    "\n",
    "O algoritmo tenta **primeiro** usar o separador mais \"natural\" e, se n√£o conseguir respeitar o `chunk_size`, vai descendo na hierarquia:\n",
    "\n",
    "```python\n",
    "separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbcde21",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Ordem de tentativa:**\n",
    "\n",
    "1. **`\\n\\n`** (duas quebras de linha) ‚Üí Quebra por **par√°grafos**\n",
    "   - Mais natural, mant√©m ideias completas juntas\n",
    "   - Ideal para textos estruturados\n",
    "\n",
    "2. **`\\n`** (uma quebra de linha) ‚Üí Quebra por **linhas**\n",
    "   - Se par√°grafos forem muito grandes\n",
    "   - √ötil para listas, c√≥digo, etc.\n",
    "\n",
    "3. **` `** (espa√ßo) ‚Üí Quebra por **palavras**\n",
    "   - Se linhas forem muito longas\n",
    "   - Evita cortar palavras no meio\n",
    "\n",
    "4. **`\"\"`** (vazio) ‚Üí Quebra em **qualquer caractere**\n",
    "   - √öltimo recurso\n",
    "   - Quando nem palavras cabem (muito raro)\n",
    "\n",
    "**üí° Exemplo pr√°tico:**\n",
    "\n",
    "Imagine este texto:\n",
    "\n",
    "```text\n",
    "Par√°grafo 1: Este √© um texto longo sobre RAG.\\n\n",
    "Aqui continua a explica√ß√£o sobre retrieval.\\n\\n\n",
    "\n",
    "Par√°grafo 2: Agora vou falar de embeddings.\\n\n",
    "Embeddings s√£o representa√ß√µes vetoriais.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9744a6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Com `chunk_size=100` e `separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]`:\n",
    "\n",
    "- **1¬™ tentativa:** Quebrar em `\\n\\n` ‚Üí Se par√°grafo 1 tiver < 100 chars, fica inteiro no chunk\n",
    "- **Se n√£o couber:** Quebra em `\\n` ‚Üí Divide por linhas dentro do par√°grafo\n",
    "- **Se ainda n√£o couber:** Quebra em ` ` ‚Üí Divide por palavras\n",
    "- **√öltimo caso:** Quebra caractere por caractere\n",
    "\n",
    "‚úÖ Por que isso √© importante?\n",
    "\n",
    "Mant√©m a **coer√™ncia sem√¢ntica** dos chunks:\n",
    "- ‚úÖ \"A fotoss√≠ntese √© um processo...\" (chunk completo)\n",
    "- ‚ùå \"A fotoss√≠n...\" (cortado no meio - p√©ssimo para embeddings!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "810acc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Calculadora de chunks criada!\n"
     ]
    }
   ],
   "source": [
    "def calc_chunks(text: str, \n",
    "                chunk_size: int = 1000, \n",
    "                chunk_overlap: int = 200,\n",
    "                separators: list = []) -> dict:\n",
    "    \"\"\"\n",
    "    Calcula chunks de um texto usando RecursiveCharacterTextSplitter.\n",
    "    \n",
    "    Args:\n",
    "        texto: Texto para dividir\n",
    "        chunk_size: Tamanho m√°ximo de cada chunk\n",
    "        chunk_overlap: Sobreposi√ß√£o entre chunks\n",
    "        separators: Lista de separadores (padr√£o: par√°grafos, linhas, espa√ßos)\n",
    "    \n",
    "    Returns:\n",
    "        Dict com estat√≠sticas de chunking\n",
    "    \"\"\"\n",
    "    if not separators:\n",
    "        separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    \n",
    "    # Cria o splitter\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=separators\n",
    "    )\n",
    "    \n",
    "    # Divide o texto\n",
    "    chunks = splitter.split_text(text)\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    chunk_sizes = [len(c) for c in chunks]\n",
    "    \n",
    "    return {\n",
    "        \"texto_original\": text,\n",
    "        \"chunks\": chunks,\n",
    "        \"num_chunks\": len(chunks),\n",
    "        \"chunk_size_config\": chunk_size,\n",
    "        \"chunk_overlap_config\": chunk_overlap,\n",
    "        \"tamanho_medio\": sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0,\n",
    "        \"tamanho_min\": min(chunk_sizes) if chunk_sizes else 0,\n",
    "        \"tamanho_max\": max(chunk_sizes) if chunk_sizes else 0,\n",
    "        \"chars_totais\": len(text),\n",
    "        \"separators\": separators\n",
    "    }\n",
    "\n",
    "def show_chunk_statistics(results: dict, display_content: bool = True):\n",
    "    \"\"\"Exibe resultado da divis√£o em chunks de forma formatada.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üì¶ AN√ÅLISE DE CHUNKS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è  CONFIGURA√á√ÉO:\")\n",
    "    print(f\"   Chunk Size: {results['chunk_size_config']} chars\")\n",
    "    print(f\"   Chunk Overlap: {results['chunk_overlap_config']} chars ({results['chunk_overlap_config']/results['chunk_size_config']*100:.0f}%)\")\n",
    "    print(f\"   Separators: {results['separators']}\")\n",
    "    \n",
    "    print(f\"\\nüìä ESTAT√çSTICAS:\")\n",
    "    print(f\"   Caracteres totais: {results['chars_totais']:,}\")\n",
    "    print(f\"   N√∫mero de chunks: {results['num_chunks']}\")\n",
    "    print(f\"   Tamanho m√©dio: {results['tamanho_medio']:.0f} chars\")\n",
    "    print(f\"   Tamanho m√≠nimo: {results['tamanho_min']} chars\")\n",
    "    print(f\"   Tamanho m√°ximo: {results['tamanho_max']} chars\")\n",
    "    \n",
    "    if display_content:\n",
    "        print(f\"\\nüìù CONTE√öDO DOS CHUNKS:\\n\")\n",
    "        for i, chunk in enumerate(results['chunks'], 1):\n",
    "            print(f\"\\n{'‚îÄ'*80}\")\n",
    "            print(f\"Chunk {i}/{results['num_chunks']} ({len(chunk)} chars)\")\n",
    "            print(f\"{'‚îÄ'*80}\")\n",
    "            preview = chunk[:300] + \"...\" if len(chunk) > 300 else chunk\n",
    "            print(preview)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Calculadora de chunks criada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4352c3",
   "metadata": {},
   "source": [
    "### üß™ Teste 3: Texto M√©dio com Diferentes Configura√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d42e647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Texto de exemplo: 1571 caracteres\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üì¶ AN√ÅLISE DE CHUNKS\n",
      "================================================================================\n",
      "\n",
      "‚öôÔ∏è  CONFIGURA√á√ÉO:\n",
      "   Chunk Size: 500 chars\n",
      "   Chunk Overlap: 100 chars (20%)\n",
      "   Separators: ['\\n\\n', '\\n', ' ', '']\n",
      "\n",
      "üìä ESTAT√çSTICAS:\n",
      "   Caracteres totais: 1,571\n",
      "   N√∫mero de chunks: 5\n",
      "   Tamanho m√©dio: 332 chars\n",
      "   Tamanho m√≠nimo: 299 chars\n",
      "   Tamanho m√°ximo: 359 chars\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Texto de exemplo (simulando um artigo)\n",
    "article = \"\"\"\n",
    "Introdu√ß√£o aos Bancos de Dados Vetoriais\n",
    "\n",
    "Os bancos de dados vetoriais s√£o uma tecnologia emergente crucial para aplica√ß√µes de intelig√™ncia \n",
    "artificial moderna. Eles armazenam dados como vetores de alta dimens√£o (embeddings) e permitem \n",
    "buscas por similaridade sem√¢ntica.\n",
    "\n",
    "Principais Caracter√≠sticas\n",
    "\n",
    "Um embedding √© uma representa√ß√£o num√©rica de dados (texto, imagens, √°udio) em um espa√ßo vetorial. \n",
    "Por exemplo, a frase \"gato\" pode ser representada como um vetor de 768 dimens√µes. Frases com \n",
    "significados similares ter√£o vetores pr√≥ximos no espa√ßo vetorial.\n",
    "\n",
    "Aplica√ß√µes Pr√°ticas\n",
    "\n",
    "Sistemas RAG (Retrieval-Augmented Generation) s√£o a aplica√ß√£o mais popular. Eles combinam busca \n",
    "vetorial com modelos de linguagem para criar assistentes que respondem com base em documentos \n",
    "espec√≠ficos. Outras aplica√ß√µes incluem sistemas de recomenda√ß√£o, busca sem√¢ntica em e-commerce, \n",
    "e detec√ß√£o de anomalias.\n",
    "\n",
    "Tecnologias Dispon√≠veis\n",
    "\n",
    "As principais solu√ß√µes incluem FAISS (Facebook AI), Qdrant, Pinecone, Weaviate e Milvus. FAISS \n",
    "√© ideal para prototipagem local, enquanto Qdrant oferece recursos de produ√ß√£o com API REST completa. \n",
    "Pinecone √© uma solu√ß√£o gerenciada na nuvem, e Weaviate oferece suporte nativo a GraphQL.\n",
    "\n",
    "Desafios e Considera√ß√µes\n",
    "\n",
    "O principal desafio √© o chunking adequado dos documentos. Chunks muito grandes perdem precis√£o na \n",
    "busca, enquanto chunks muito pequenos perdem contexto. O overlap entre chunks ajuda a manter a \n",
    "coer√™ncia sem√¢ntica. Outra considera√ß√£o importante √© a escolha do modelo de embeddings, que deve \n",
    "ser consistente entre indexa√ß√£o e busca.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"üìÑ Texto de exemplo: {len(article)} caracteres\\n\")\n",
    "\n",
    "# Teste com configura√ß√£o padr√£o\n",
    "chunked_article_results = calc_chunks(article, chunk_size=500, chunk_overlap=100)\n",
    "show_chunk_statistics(chunked_article_results, display_content=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d9516e",
   "metadata": {},
   "source": [
    "### üß™ Teste 4: Compara√ß√£o de Diferentes Chunk Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "775c743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ COMPARA√á√ÉO DE CONFIGURA√á√ïES DE CHUNKING\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Chunk Size",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Overlap",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Overlap %",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "N¬∫ Chunks",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Tamanho M√©dio",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Tamanho Min",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Tamanho Max",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ddcc8e92-5ec0-4b20-9446-79ceaf496a30",
       "rows": [
        [
         "0",
         "200",
         "40",
         "20%",
         "15",
         "103 chars",
         "19",
         "196"
        ],
        [
         "1",
         "500",
         "100",
         "20%",
         "5",
         "332 chars",
         "299",
         "359"
        ],
        [
         "2",
         "1000",
         "200",
         "20%",
         "2",
         "796 chars",
         "672",
         "920"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk Size</th>\n",
       "      <th>Overlap</th>\n",
       "      <th>Overlap %</th>\n",
       "      <th>N¬∫ Chunks</th>\n",
       "      <th>Tamanho M√©dio</th>\n",
       "      <th>Tamanho Min</th>\n",
       "      <th>Tamanho Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "      <td>20%</td>\n",
       "      <td>15</td>\n",
       "      <td>103 chars</td>\n",
       "      <td>19</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>20%</td>\n",
       "      <td>5</td>\n",
       "      <td>332 chars</td>\n",
       "      <td>299</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>200</td>\n",
       "      <td>20%</td>\n",
       "      <td>2</td>\n",
       "      <td>796 chars</td>\n",
       "      <td>672</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chunk Size  Overlap Overlap %  N¬∫ Chunks Tamanho M√©dio  Tamanho Min  \\\n",
       "0         200       40       20%         15     103 chars           19   \n",
       "1         500      100       20%          5     332 chars          299   \n",
       "2        1000      200       20%          2     796 chars          672   \n",
       "\n",
       "   Tamanho Max  \n",
       "0          196  \n",
       "1          359  \n",
       "2          920  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° INSIGHTS:\n",
      "   - Chunks menores = mais chunks = busca mais precisa (mas menos contexto)\n",
      "   - Chunks maiores = menos chunks = mais contexto (mas busca menos precisa)\n",
      "   - Overlap de 20% (do chunk_size) √© um bom equil√≠brio\n",
      "   - Para produ√ß√£o RAG, chunk_size entre 500-1000 chars √© comum\n"
     ]
    }
   ],
   "source": [
    "# Testar diferentes tamanhos de chunk\n",
    "configs = [\n",
    "    {\"chunk_size\": 200, \"chunk_overlap\": 40},   # Chunks pequenos\n",
    "    {\"chunk_size\": 500, \"chunk_overlap\": 100},  # Chunks m√©dios\n",
    "    {\"chunk_size\": 1000, \"chunk_overlap\": 200}, # Chunks grandes\n",
    "]\n",
    "\n",
    "print(\"\\nüî¨ COMPARA√á√ÉO DE CONFIGURA√á√ïES DE CHUNKING\\n\")\n",
    "\n",
    "chunking_comparison_results = []\n",
    "\n",
    "for config in configs:\n",
    "    res = calc_chunks(article, **config)\n",
    "    chunking_comparison_results.append({\n",
    "        \"Chunk Size\": config['chunk_size'],\n",
    "        \"Overlap\": config['chunk_overlap'],\n",
    "        \"Overlap %\": f\"{config['chunk_overlap']/config['chunk_size']*100:.0f}%\",\n",
    "        \"N¬∫ Chunks\": res['num_chunks'],\n",
    "        \"Tamanho M√©dio\": f\"{res['tamanho_medio']:.0f} chars\",\n",
    "        \"Tamanho Min\": res['tamanho_min'],\n",
    "        \"Tamanho Max\": res['tamanho_max']\n",
    "    })\n",
    "\n",
    "df_chunks = pd.DataFrame(chunking_comparison_results)\n",
    "display(df_chunks)\n",
    "\n",
    "print(\"\\nüí° INSIGHTS:\")\n",
    "print(\"   - Chunks menores = mais chunks = busca mais precisa (mas menos contexto)\")\n",
    "print(\"   - Chunks maiores = menos chunks = mais contexto (mas busca menos precisa)\")\n",
    "print(\"   - Overlap de 20% (do chunk_size) √© um bom equil√≠brio\")\n",
    "print(\"   - Para produ√ß√£o RAG, chunk_size entre 500-1000 chars √© comum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662493e",
   "metadata": {},
   "source": [
    "## Import√¢ncia do Tamanho do Chunk\n",
    "\n",
    "O tamanho do chunk √© **crucial** porque afeta diretamente tr√™s aspectos do sistema RAG:\n",
    "\n",
    "### 1. Precis√£o da Busca Sem√¢ntica\n",
    "\n",
    "**Chunks pequenos (200-400 chars):**\n",
    "- ‚úÖ Alta precis√£o: retornam informa√ß√£o muito espec√≠fica\n",
    "- ‚ùå Pouco contexto: podem perder o \"quadro geral\"\n",
    "- üìç Uso: FAQs, dicion√°rios, documenta√ß√£o de APIs\n",
    "\n",
    "**Chunks grandes (1500-2000 chars):**\n",
    "- ‚úÖ Muito contexto: capturam conceitos completos\n",
    "- ‚ùå Baixa precis√£o: diluem a relev√¢ncia sem√¢ntica\n",
    "- üìç Uso: narrativas, an√°lises longas\n",
    "\n",
    "### 2. Custo e Performance\n",
    "\n",
    "- **Custo de API:** Mais chunks recuperados (k alto) com chunks grandes = mais tokens = mais caro\n",
    "- **Lat√™ncia:** Chunks grandes demoram mais para gerar embeddings e retornar\n",
    "- **Armazenamento:** Cada chunk = 1 vetor no banco (chunks pequenos = mais vetores)\n",
    "\n",
    "### 3. Qualidade das Respostas do LLM\n",
    "\n",
    "- **Context window:** LLMs t√™m limite de tokens (ex: GPT-4 = 8k, 128k)\n",
    "- **Chunk muito grande:** Pode estourar o limite mesmo com k=2 ou k=3\n",
    "- **Chunk muito pequeno:** LLM recebe informa√ß√£o fragmentada, gera respostas incompletas\n",
    "\n",
    "---\n",
    "\n",
    "## Tamanhos Padr√£o da Ind√∫stria\n",
    "\n",
    "### Configura√ß√µes mais comuns (2024-2025)\n",
    "\n",
    "\n",
    "| Aplica√ß√£o | Chunk Size (chars) | Chunk Size (tokens) | Overlap | k | Justificativa |\n",
    "|-----------|-------------------|---------------------|---------|---|---------------|\n",
    "| **RAG Gen√©rico (Baseline)** | **1600-2000** | **400-512** | **10-20%** | **4-5** | Sweet spot recomendado; equil√≠brio ideal entre contexto e precis√£o |\n",
    "| **FAQ/Suporte** | **800-1200** | **200-400** | **10-15%** | **4-6** | Respostas isoladas e completas; quest√µes espec√≠ficas |\n",
    "| **Chatbots Empresariais** | **1600-2400** | **400-600** | **15-20%** | **4-5** | Contexto mais amplo para respostas complexas; n√£o t√£o pequeno quanto FAQ |\n",
    "| **Documenta√ß√£o T√©cnica** | **2000-2800** | **500-700** | **20-25%** | **3-4** | Inclui exemplos, par√¢metros e contexto completo de m√©todos API |\n",
    "| **An√°lise de Documentos** | **2400-3600** | **600-900** | **20-25%** | **2-4** | Contexto profundo para an√°lise hol√≠stica; busca sem√¢ntica |\n",
    "| **Academic Papers** | **2800-4800** | **700-1200** | **30%+** | **2-3** | Constru√ß√£o incremental de conceitos; preserva√ß√£o de figuras e tabelas |\n",
    "| **Legal/Compliance** | **3200-5000** | **800-1250** | **25-30%** | **2-3** | Precis√£o jur√≠dica; mant√©m cl√°usulas e refer√™ncias legais completas |\n",
    "| **Code Search/Repositories** | **1200-2000** | **300-512** | **10-15%** | **3-5** | Fun√ß√µes e classes completas; respeita limites de estrutura de c√≥digo |\n",
    "| **News/Blog Posts** | **1600-2400** | **400-600** | **15-20%** | **4-5** | Estrutura editorial j√° otimizada para leitura; par√°grafos naturais |\n",
    "\n",
    "#### Notas Importantes\n",
    "\n",
    "- **Sweet Spot**: **256-512 tokens** √© o intervalo recomendado pela maioria dos especialistas como ponto de partida para experimenta√ß√£o\n",
    "- **Overlap**: recomend√°vel **10-20% de overlap** para a maioria dos casos, aumentando para **25-30%** em documentos legais e acad√™micos para preservar contexto.\n",
    "- **Par√¢metro k** (chunks recuperados): sugere-se **k=5** como padr√£o em muitos estudos de RAG, com varia√ß√£o de **2-6** conforme a aplica√ß√£o. Valores mais baixos (2-3) para documentos densos (legal, academic); valores mais altos (4-6) para consultas gerais\n",
    "- **Estrat√©gia de Chunking**: os valores acima assumem **recursive/semantic chunking** (padr√£o recomendado para 80% das aplica√ß√µes), n√£o simple size-based chunking.\n",
    "\n",
    "#### Refer√™ncias da Ind√∫stria\n",
    "\n",
    "- https://www.pinecone.io/learn/rag-best-practices/#chunk-size\n",
    "- https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025\n",
    "- https://arxiv.org/html/2508.02435v1\n",
    "- https://www.sciencedirect.com/science/article/pii/S0920548925000248\n",
    "\n",
    "### Convers√£o Aproximada\n",
    "\n",
    "**Regra pr√°tica:** \n",
    "- **1 token ‚âà 4 caracteres** (portugu√™s)\n",
    "- **1 token ‚âà 4 caracteres** (ingl√™s)\n",
    "- **Chunk de 1000 chars ‚âà 250 tokens**\n",
    "- **Chunk de 1800 chars ‚âà 450 tokens** (baseline 2024-2025)\n",
    "- **Chunk de 2000 chars ‚âà 512 tokens** (limite superior baseline)\n",
    "\n",
    "## Recomenda√ß√£o da Ind√∫stria (Consenso 2024-2025)\n",
    "\n",
    "### **Sweet Spot Universal:**\n",
    "\n",
    "```python\n",
    "chunk_size = 1800        # caracteres (baseline: 1600-2000)\n",
    "chunk_overlap = 300      # ~17% do chunk_size (range: 10-20%)\n",
    "k = 4                    # chunks recuperados (range: 4-5)\n",
    "chunk_overlap = 200      # 20% do chunk_size\n",
    "```\n",
    "\n",
    "\n",
    "**üìä Evolu√ß√£o das Recomenda√ß√µes:**\n",
    "- **2024-2025:** 1600-2000 chars (sweet spot: 1800)\n",
    "- **2022-2023:** 800-1000 chars era o padr√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115fa6ec",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Por qu√™?**\n",
    "1. **1800 chars ‚âà 450 tokens/chunk** ‚Üí Equil√≠brio ideal entre contexto e precis√£o\n",
    "2. **k=4 chunks = ~1800 tokens** ‚Üí Contexto rico sem estourar limites de modelos modernos\n",
    "3. **Overlap de 17% (300 chars)** ‚Üí Preserva contexto entre chunks sem duplica√ß√£o excessiva\n",
    "4. **Flex√≠vel:** Funciona bem para 70-80% dos casos de uso gen√©ricos\n",
    "5. **Compat√≠vel:** Aproveitam context windows maiores dos LLMs modernos (GPT-4: 128k, Claude-3: 200k)\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Como Escolher o Tamanho Ideal?\n",
    "\n",
    "### M√©todo Emp√≠rico (recomendado para produ√ß√£o):\n",
    "1. **Comece com 1800 chars, overlap 300** (baseline 2024-2025)\n",
    "2. **Teste com 20-50 perguntas reais** do seu dom√≠nio\n",
    "3. **Me√ßa m√©tricas:**\n",
    "   - Relev√¢ncia das respostas (1-5)\n",
    "   - Taxa de \"n√£o sei\" (se LLM n√£o acha informa√ß√£o)\n",
    "   - Lat√™ncia de resposta\n",
    "4. **Ajuste iterativamente:**\n",
    "   - Respostas vagas/imprecisas? ‚Üí Reduza chunk_size (1200-1400)\n",
    "   - Respostas fragmentadas? ‚Üí Aumente chunk_size (2200-2400)\n",
    "   - Custo alto? ‚Üí Reduza k (de 4 para 3) ou chunk_size\n",
    "   - Performance lenta? ‚Üí Reduza chunk_size ou k\n",
    "\n",
    "### üß™ Exemplo de Teste A/B:\n",
    "\n",
    "```python\n",
    "\n",
    "test_configs = [\n",
    "    {\"chunk_size\": 1200, \"overlap\": 200, \"k\": 5},   # Precis√£o (FAQ/Suporte)\n",
    "    {\"chunk_size\": 1800, \"overlap\": 300, \"k\": 4},   # Baseline 2024-2025\n",
    "    {\"chunk_size\": 2400, \"overlap\": 400, \"k\": 3},   # Contexto (Docs T√©cnicos)\n",
    "    {\"chunk_size\": 1000, \"overlap\": 200, \"k\": 4},  # Padr√£o\n",
    "    {\"chunk_size\": 1500, \"overlap\": 300, \"k\": 3},  # Contexto\n",
    "]\n",
    "```\n",
    "\n",
    "- Teste com perguntas reais do seu dom√≠nio\n",
    "- Me√ßa qualidade, custo e lat√™ncia\n",
    "- Compare com m√©tricas de baseline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65624c6b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Erros Comuns\n",
    "\n",
    "‚ùå **\"Vou usar 5000 chars para ter mais contexto!\"**\n",
    "‚Üí Resultado: Busca imprecisa, retorna chunks irrelevantes\n",
    "\n",
    "‚ùå **\"Vou usar 100 chars para ter precis√£o m√°xima!\"**\n",
    "‚Üí Resultado: Respostas fragmentadas, LLM n√£o entende contexto\n",
    "\n",
    "‚ùå **\"Vou usar k=10 para garantir que tenho tudo!\"**\n",
    "‚Üí Resultado: Custo alto, informa√ß√£o irrelevante dilui a relevante\n",
    "\n",
    "‚úÖ **Abordagem correta:**\n",
    "- Comece com padr√£o da ind√∫stria (1800/300/k=4)\n",
    "- Me√ßa resultados com dados reais\n",
    "- Ajuste baseado em m√©tricas objetivas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080ad31",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Calculadora Combinada: Tokens + Chunks\n",
    "\n",
    "Agora vamos combinar as duas an√°lises para entender o impacto completo no sistema RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eefacbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Calculadora combinada criada!\n"
     ]
    }
   ],
   "source": [
    "def perform_rag_analysis(text: str,\n",
    "                        chunk_size: int = 1000,\n",
    "                        chunk_overlap: int = 200,\n",
    "                        num_chunks_retrieved: int = 4,\n",
    "                        encoding: str = \"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    An√°lise completa simulando um pipeline RAG:\n",
    "    1. Divide o texto em chunks\n",
    "    2. Calcula tokens por chunk\n",
    "    3. Simula recupera√ß√£o de k chunks\n",
    "    4. Calcula tokens totais que ser√£o enviados ao LLM\n",
    "    \n",
    "    Args:\n",
    "        texto: Documento original\n",
    "        chunk_size: Tamanho dos chunks\n",
    "        chunk_overlap: Overlap entre chunks\n",
    "        k_recuperados: Quantos chunks ser√£o recuperados na busca\n",
    "        encoding: Encoding do tiktoken\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ AN√ÅLISE COMPLETA RAG: CHUNKS + TOKENS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. An√°lise do documento original\n",
    "    print(\"\\nüìÑ DOCUMENTO ORIGINAL:\")\n",
    "    tokens_original = calc_tokens(text, encoding)\n",
    "    print(f\"   Caracteres: {tokens_original['caracteres']:,}\")\n",
    "    print(f\"   Tokens: {tokens_original['tokens']:,}\")\n",
    "    \n",
    "    # 2. Chunking\n",
    "    print(\"\\n‚úÇÔ∏è  CHUNKING:\")\n",
    "    chunks_result = calc_chunks(text, chunk_size, chunk_overlap)\n",
    "    print(f\"   Configura√ß√£o: {chunk_size} chars, overlap {chunk_overlap} ({chunk_overlap/chunk_size*100:.0f}%)\")\n",
    "    print(f\"   Chunks criados: {chunks_result['num_chunks']}\")\n",
    "    print(f\"   Tamanho m√©dio: {chunks_result['tamanho_medio']:.0f} chars/chunk\")\n",
    "    \n",
    "    # 3. Tokens por chunk\n",
    "    print(\"\\nüî¢ TOKENS POR CHUNK:\")\n",
    "    tokens_por_chunk = []\n",
    "    for i, chunk in enumerate(chunks_result['chunks'], 1):\n",
    "        tokens_chunk = calc_tokens(chunk, encoding)\n",
    "        tokens_por_chunk.append({\n",
    "            \"Chunk\": i,\n",
    "            \"Caracteres\": len(chunk),\n",
    "            \"Tokens\": tokens_chunk['tokens']\n",
    "        })\n",
    "    \n",
    "    df_tokens_chunks = pd.DataFrame(tokens_por_chunk)\n",
    "    display(df_tokens_chunks)\n",
    "    \n",
    "    # 4. Simula√ß√£o de recupera√ß√£o (RAG)\n",
    "    print(f\"\\nüîç SIMULA√á√ÉO DE BUSCA (k={num_chunks_retrieved}):\")\n",
    "    chunks_recuperados = min(num_chunks_retrieved, chunks_result['num_chunks'])\n",
    "    tokens_recuperados = sum(tokens_por_chunk[i]['Tokens'] for i in range(chunks_recuperados))\n",
    "    chars_recuperados = sum(tokens_por_chunk[i]['Caracteres'] for i in range(chunks_recuperados))\n",
    "    \n",
    "    print(f\"   Chunks recuperados: {chunks_recuperados}\")\n",
    "    print(f\"   Caracteres enviados ao LLM: {chars_recuperados:,}\")\n",
    "    print(f\"   Tokens enviados ao LLM: {tokens_recuperados:,}\")\n",
    "    \n",
    "    # 5. An√°lise de custo/efici√™ncia\n",
    "    print(\"\\nüí∞ AN√ÅLISE DE EFICI√äNCIA:\")\n",
    "    reducao_chars = (1 - chars_recuperados / tokens_original['caracteres']) * 100\n",
    "    reducao_tokens = (1 - tokens_recuperados / tokens_original['tokens']) * 100\n",
    "    \n",
    "    print(f\"   Redu√ß√£o de caracteres: {reducao_chars:.1f}%\")\n",
    "    print(f\"   Redu√ß√£o de tokens: {reducao_tokens:.1f}%\")\n",
    "    print(f\"   Economia: Voc√™ est√° enviando apenas {100-reducao_tokens:.1f}% do documento!\")\n",
    "    \n",
    "    # 6. Limites de contexto\n",
    "    print(\"\\nüö¶ VERIFICA√á√ÉO DE LIMITES:\")\n",
    "    limites = {\n",
    "        \"GPT-3.5-turbo\": 4096,\n",
    "        \"GPT-4\": 8192,\n",
    "        \"GPT-4-turbo\": 128000,\n",
    "        \"Claude-3\": 200000\n",
    "    }\n",
    "    \n",
    "    # Reservar 500 tokens para resposta\n",
    "    tokens_com_resposta = tokens_recuperados + 500\n",
    "    \n",
    "    for modelo, limite in limites.items():\n",
    "        percentual = (tokens_com_resposta / limite) * 100\n",
    "        status = \"‚úÖ\" if percentual < 80 else \"‚ö†Ô∏è\" if percentual < 100 else \"‚ùå\"\n",
    "        print(f\"   {status} {modelo} ({limite:,} tokens): {percentual:.1f}% usado\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"chunks_result\": chunks_result,\n",
    "        \"tokens_original\": tokens_original,\n",
    "        \"tokens_recuperados\": tokens_recuperados,\n",
    "        \"chars_recuperados\": chars_recuperados\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Calculadora combinada criada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f01273b",
   "metadata": {},
   "source": [
    "### üß™ Teste 5: An√°lise Completa do Artigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f2e7502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéØ AN√ÅLISE COMPLETA RAG: CHUNKS + TOKENS\n",
      "================================================================================\n",
      "\n",
      "üìÑ DOCUMENTO ORIGINAL:\n",
      "   Caracteres: 1,571\n",
      "   Tokens: 418\n",
      "\n",
      "‚úÇÔ∏è  CHUNKING:\n",
      "   Configura√ß√£o: 500 chars, overlap 100 (20%)\n",
      "   Chunks criados: 5\n",
      "   Tamanho m√©dio: 332 chars/chunk\n",
      "\n",
      "üî¢ TOKENS POR CHUNK:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Chunk",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Caracteres",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Tokens",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "8914e830-6397-4fa0-86b4-174cbccff4da",
       "rows": [
        [
         "0",
         "1",
         "299",
         "79"
        ],
        [
         "1",
         "2",
         "307",
         "81"
        ],
        [
         "2",
         "3",
         "359",
         "94"
        ],
        [
         "3",
         "4",
         "337",
         "99"
        ],
        [
         "4",
         "5",
         "359",
         "88"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk</th>\n",
       "      <th>Caracteres</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>299</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>359</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>337</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>359</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chunk  Caracteres  Tokens\n",
       "0      1         299      79\n",
       "1      2         307      81\n",
       "2      3         359      94\n",
       "3      4         337      99\n",
       "4      5         359      88"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç SIMULA√á√ÉO DE BUSCA (k=3):\n",
      "   Chunks recuperados: 3\n",
      "   Caracteres enviados ao LLM: 965\n",
      "   Tokens enviados ao LLM: 254\n",
      "\n",
      "üí∞ AN√ÅLISE DE EFICI√äNCIA:\n",
      "   Redu√ß√£o de caracteres: 38.6%\n",
      "   Redu√ß√£o de tokens: 39.2%\n",
      "   Economia: Voc√™ est√° enviando apenas 60.8% do documento!\n",
      "\n",
      "üö¶ VERIFICA√á√ÉO DE LIMITES:\n",
      "   ‚úÖ GPT-3.5-turbo (4,096 tokens): 18.4% usado\n",
      "   ‚úÖ GPT-4 (8,192 tokens): 9.2% usado\n",
      "   ‚úÖ GPT-4-turbo (128,000 tokens): 0.6% usado\n",
      "   ‚úÖ Claude-3 (200,000 tokens): 0.4% usado\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An√°lise completa com configura√ß√£o t√≠pica de RAG\n",
    "complete_analysis_result = perform_rag_analysis(\n",
    "    text=article,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    num_chunks_retrieved=3,\n",
    "    encoding=\"cl100k_base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570cf4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üáßüá∑ Teste 6: Tokeniza√ß√£o Otimizada para Portugu√™s\n",
    "\n",
    "Vamos comparar a tokeniza√ß√£o do `tiktoken` (otimizado para ingl√™s) com o **BERTugues** (otimizado para portugu√™s).\n",
    "\n",
    "### üìñ O que √© BERTugues?\n",
    "\n",
    "BERTugues √© uma vers√£o do BERT treinada especificamente para portugu√™s brasileiro. Sua principal vantagem √© a **efici√™ncia na tokeniza√ß√£o** de textos em portugu√™s, resultando em menos tokens para representar o mesmo conte√∫do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a5f4923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bibliotecas para BERTugues importadas!\n"
     ]
    }
   ],
   "source": [
    "# Importa√ß√µes para tokeniza√ß√£o com BERTugues\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "print(\"‚úÖ Bibliotecas para BERTugues importadas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "927e0866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Carregando BERTugues (pode levar alguns segundos)...\n",
      "\n",
      "‚úÖ BERTugues carregado com sucesso!\n",
      "üìä Vocabul√°rio: 30,522 tokens\n"
     ]
    }
   ],
   "source": [
    "# Carregar o tokenizador e o modelo BERTugues\n",
    "print(\"üîÑ Carregando BERTugues (pode levar alguns segundos)...\\n\")\n",
    "\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\n",
    "    \"ricardoz/BERTugues-base-portuguese-cased\", \n",
    "    do_lower_case=False\n",
    ")\n",
    "\n",
    "model_bert = BertModel.from_pretrained(\n",
    "    \"ricardoz/BERTugues-base-portuguese-cased\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ BERTugues carregado com sucesso!\")\n",
    "print(f\"üìä Vocabul√°rio: {tokenizer_bert.vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6967970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä AN√ÅLISE DE TOKENS - BERTugues\n",
      "================================================================================\n",
      "\n",
      "üìù Texto (preview): \n",
      "Retrieval-Augmented Generation (RAG) √© uma arquitetura que combina recupera√ß√£o de informa√ß√µes \n",
      "com ...\n",
      "\n",
      "üìè Caracteres: 512\n",
      "üî¢ Tokens: 105\n",
      "üìê Propor√ß√£o: 4.88 chars/token\n",
      "‚öôÔ∏è  Encoding: BERTugues\n",
      "üì¶ Tamanho do vocabul√°rio: 30,522\n",
      "üßÆ Shape dos embeddings: torch.Size([1, 107, 768])\n",
      "\n",
      "üîç Primeiros 20 tokens:\n",
      "   ['Ret', '##rie', '##val', '-', 'Aug']\n",
      "   ['##mente', '##d', 'Generation', '(', 'RA']\n",
      "   ['##G', ')', '√©', 'uma', 'arquitetura']\n",
      "   ['que', 'combina', 'recupera√ß√£o', 'de', 'informa√ß√µes']\n",
      "================================================================================\n",
      "\n",
      "üî¨ COMPARA√á√ÉO: BERTugues vs tiktoken (cl100k_base)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Modelo",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Otimizado para",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Tokens",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Chars/Token",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Economia",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0834ca36-9d59-4bb1-a0be-bc6e82569531",
       "rows": [
        [
         "0",
         "tiktoken (GPT-4)",
         "Ingl√™s",
         "125",
         "4.1",
         "baseline"
        ],
        [
         "1",
         "BERTugues",
         "Portugu√™s",
         "105",
         "4.88",
         "16.0%"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Otimizado para</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Chars/Token</th>\n",
       "      <th>Economia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiktoken (GPT-4)</td>\n",
       "      <td>Ingl√™s</td>\n",
       "      <td>125</td>\n",
       "      <td>4.10</td>\n",
       "      <td>baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BERTugues</td>\n",
       "      <td>Portugu√™s</td>\n",
       "      <td>105</td>\n",
       "      <td>4.88</td>\n",
       "      <td>16.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Modelo Otimizado para  Tokens  Chars/Token  Economia\n",
       "0  tiktoken (GPT-4)         Ingl√™s     125         4.10  baseline\n",
       "1         BERTugues      Portugu√™s     105         4.88     16.0%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° INSIGHTS:\n",
      "   - BERTugues usa 105 tokens vs 125 do tiktoken\n",
      "   - Redu√ß√£o de 16.0% no n√∫mero de tokens\n",
      "   - Cada token do BERTugues representa ~4.88 caracteres\n",
      "   - Para textos em portugu√™s, BERTugues √© mais eficiente!\n",
      "   - Economia direta em custo de API e processamento\n"
     ]
    }
   ],
   "source": [
    "def calc_tokens_bertugues(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calcula tokens usando BERTugues (otimizado para portugu√™s).\n",
    "    \n",
    "    Args:\n",
    "        text: Texto para tokenizar\n",
    "    \n",
    "    Returns:\n",
    "        Dict com estat√≠sticas de tokens e embeddings\n",
    "    \"\"\"\n",
    "    # Tokenizar\n",
    "    tokens = tokenizer_bert.tokenize(text)\n",
    "    input_ids = tokenizer_bert.encode(text, return_tensors='pt')\n",
    "    \n",
    "    # Gerar embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(input_ids)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    # Calcular estat√≠sticas\n",
    "    num_chars = len(text)\n",
    "    num_tokens = len(tokens)\n",
    "    ratio = num_chars / num_tokens if num_tokens > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"texto\": text[:100] + \"...\" if len(text) > 100 else text,\n",
    "        \"caracteres\": num_chars,\n",
    "        \"tokens\": num_tokens,\n",
    "        \"chars_por_token\": round(ratio, 2),\n",
    "        \"encoding\": \"BERTugues\",\n",
    "        \"tokens_list\": tokens[:20],  # Primeiros 20 tokens\n",
    "        \"embedding_shape\": embeddings.shape,\n",
    "        \"vocab_size\": tokenizer_bert.vocab_size\n",
    "    }\n",
    "\n",
    "# Teste com o mesmo texto t√©cnico usado anteriormente\n",
    "resultado_bert = calc_tokens_bertugues(text_pt)\n",
    "\n",
    "# Exibir usando formato similar aos testes anteriores\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä AN√ÅLISE DE TOKENS - BERTugues\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìù Texto (preview): {resultado_bert['texto']}\")\n",
    "print(f\"\\nüìè Caracteres: {resultado_bert['caracteres']:,}\")\n",
    "print(f\"üî¢ Tokens: {resultado_bert['tokens']:,}\")\n",
    "print(f\"üìê Propor√ß√£o: {resultado_bert['chars_por_token']} chars/token\")\n",
    "print(f\"‚öôÔ∏è  Encoding: {resultado_bert['encoding']}\")\n",
    "print(f\"üì¶ Tamanho do vocabul√°rio: {resultado_bert['vocab_size']:,}\")\n",
    "print(f\"üßÆ Shape dos embeddings: {resultado_bert['embedding_shape']}\")\n",
    "print(f\"\\nüîç Primeiros 20 tokens:\")\n",
    "for i in range(0, min(20, len(resultado_bert['tokens_list'])), 5):\n",
    "    tokens_linha = resultado_bert['tokens_list'][i:i+5]\n",
    "    print(f\"   {tokens_linha}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Compara√ß√£o com tiktoken\n",
    "print(\"üî¨ COMPARA√á√ÉO: BERTugues vs tiktoken (cl100k_base)\\n\")\n",
    "\n",
    "comparacao_modelos = pd.DataFrame([\n",
    "    {\n",
    "        \"Modelo\": \"tiktoken (GPT-4)\",\n",
    "        \"Otimizado para\": \"Ingl√™s\",\n",
    "        \"Tokens\": tokenization_result_pt['tokens'],\n",
    "        \"Chars/Token\": tokenization_result_pt['chars_por_token'],\n",
    "        \"Economia\": \"baseline\"\n",
    "    },\n",
    "    {\n",
    "        \"Modelo\": \"BERTugues\",\n",
    "        \"Otimizado para\": \"Portugu√™s\",\n",
    "        \"Tokens\": resultado_bert['tokens'],\n",
    "        \"Chars/Token\": resultado_bert['chars_por_token'],\n",
    "        \"Economia\": f\"{((tokenization_result_pt['tokens'] - resultado_bert['tokens']) / tokenization_result_pt['tokens'] * 100):.1f}%\"\n",
    "    }\n",
    "])\n",
    "\n",
    "display(comparacao_modelos)\n",
    "\n",
    "print(\"\\nüí° INSIGHTS:\")\n",
    "print(f\"   - BERTugues usa {resultado_bert['tokens']} tokens vs {tokenization_result_pt['tokens']} do tiktoken\")\n",
    "print(f\"   - Redu√ß√£o de {((tokenization_result_pt['tokens'] - resultado_bert['tokens']) / tokenization_result_pt['tokens'] * 100):.1f}% no n√∫mero de tokens\")\n",
    "print(f\"   - Cada token do BERTugues representa ~{resultado_bert['chars_por_token']} caracteres\")\n",
    "print(\"   - Para textos em portugu√™s, BERTugues √© mais eficiente!\")\n",
    "print(\"   - Economia direta em custo de API e processamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ce3c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Exerc√≠cios Pr√°ticos\n",
    "\n",
    "Agora √© sua vez! Use as calculadoras acima para explorar diferentes cen√°rios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f10cf",
   "metadata": {},
   "source": [
    "### üìù Exerc√≠cio 1: Otimiza√ß√£o de Chunks\n",
    "\n",
    "**Desafio:** Voc√™ tem um documento de 5000 caracteres e quer recuperar 4 chunks. Qual configura√ß√£o de `chunk_size` e `chunk_overlap` resulta em:\n",
    "- Aproximadamente 6-8 chunks totais?\n",
    "- Tokens enviados ao LLM < 1000?\n",
    "\n",
    "**Instru√ß√µes:**\n",
    "1. Crie um texto de exemplo de ~5000 chars\n",
    "2. Teste diferentes configura√ß√µes\n",
    "3. Use `analise_completa_rag()` para verificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8c82de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEU C√ìDIGO AQUI\n",
    "# Dica: Comece com chunk_size=800, chunk_overlap=160\n",
    "\n",
    "# texto_exercicio_1 = \"...\" * 100  # crie um texto de ~5000 chars\n",
    "# analise_completa_rag(texto_exercicio_1, chunk_size=???, chunk_overlap=???, k_recuperados=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c185eeb",
   "metadata": {},
   "source": [
    "### üìù Exerc√≠cio 2: An√°lise de Custo\n",
    "\n",
    "**Contexto:** Voc√™ tem 100 PDFs de 50 p√°ginas cada (m√©dia de 3000 chars/p√°gina).\n",
    "\n",
    "**Desafio:** Compare duas estrat√©gias:\n",
    "- **Estrat√©gia A:** chunk_size=500, k=5\n",
    "- **Estrat√©gia B:** chunk_size=1000, k=3\n",
    "\n",
    "Qual estrat√©gia envia menos tokens ao LLM em m√©dia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71568b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEU C√ìDIGO AQUI\n",
    "# Dica: Simule uma p√°gina t√≠pica e calcule tokens para cada estrat√©gia\n",
    "\n",
    "# pagina_tipica = \"...\" * 50  # ~3000 chars\n",
    "# estrategia_a = analise_completa_rag(pagina_tipica, chunk_size=500, k_recuperados=5)\n",
    "# estrategia_b = analise_completa_rag(pagina_tipica, chunk_size=1000, k_recuperados=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb37083",
   "metadata": {},
   "source": [
    "### üìù Exerc√≠cio 3: Detec√ß√£o de Problemas\n",
    "\n",
    "**Cen√°rio:** Um colega configurou um sistema RAG assim:\n",
    "- chunk_size=5000\n",
    "- chunk_overlap=0\n",
    "- k=1\n",
    "\n",
    "**Desafio:** Identifique 3 problemas com essa configura√ß√£o e sugira melhorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73ab0b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEU C√ìDIGO AQUI\n",
    "# Teste a configura√ß√£o problem√°tica e compare com uma melhor\n",
    "\n",
    "# resultado_ruim = analise_completa_rag(artigo, chunk_size=5000, chunk_overlap=0, k_recuperados=1)\n",
    "# resultado_bom = analise_completa_rag(artigo, chunk_size=???, chunk_overlap=???, k_recuperados=???)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8102eb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclus√£o\n",
    "\n",
    "### O que aprendemos:\n",
    "\n",
    "1. **Tokens** s√£o unidades de processamento do LLM\n",
    "   - Determinam custos e limites de contexto\n",
    "   - Variam conforme o modelo/encoding\n",
    "\n",
    "2. **Chunks** s√£o unidades de armazenamento e busca\n",
    "   - Determinam precis√£o da recupera√ß√£o\n",
    "   - Voc√™ controla o tamanho e overlap\n",
    "\n",
    "3. **A rela√ß√£o entre eles:**\n",
    "   ```\n",
    "   Chunks (sua escolha) ‚Üí Busca ‚Üí Tokens (para o LLM)\n",
    "   ```\n",
    "\n",
    "4. **Otimiza√ß√£o √© balanceamento:**\n",
    "   - Chunks menores = busca precisa, mas menos contexto\n",
    "   - Chunks maiores = mais contexto, mas busca vaga\n",
    "   - Tokens recuperados devem caber no limite do modelo\n",
    "\n",
    "### üöÄ Pr√≥ximos Passos:\n",
    "\n",
    "1. Aplique essas calculadoras nos seus pr√≥prios documentos\n",
    "2. Experimente diferentes configura√ß√µes e me√ßa qualidade das respostas\n",
    "3. Documente suas melhores configura√ß√µes para cada tipo de documento\n",
    "4. Compartilhe descobertas com a turma!\n",
    "\n",
    "---\n",
    "\n",
    "**Boa pr√°tica em RAG! üéì**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "11-34-engenharia-vetorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
