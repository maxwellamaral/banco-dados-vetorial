{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b325f0f",
   "metadata": {},
   "source": [
    "# üîç Laborat√≥rio: Busca Sem√¢ntica Local com Ollama\n",
    "\n",
    "## O que vamos aprender?\n",
    "\n",
    "Neste notebook, voc√™ vai construir um **sistema de busca inteligente** que entende o **significado** das palavras, n√£o apenas correspond√™ncias exatas de texto.\n",
    "\n",
    "### Diferen√ßa entre Busca Tradicional vs. Busca Sem√¢ntica\n",
    "\n",
    "**Busca Tradicional (palavras-chave):**\n",
    "```text\n",
    "Query: \"hardware celular\"\n",
    "Resultado: S√≥ encontra documentos com as palavras exatas \"hardware\" e \"celular\"\n",
    "```\n",
    "\n",
    "**Busca Sem√¢ntica (significado):**\n",
    "```text\n",
    "Query: \"hardware celular\"\n",
    "Resultado: Encontra \"iPhone\", \"smartphone\", \"processador mobile\" \n",
    "           mesmo sem as palavras exatas!\n",
    "```\n",
    "\n",
    "### Como isso funciona?\n",
    "\n",
    "Usando **bibliotecas reais** de produ√ß√£o que empresas usam no dia a dia:\n",
    "- **LangChain** ‚Üí Framework para construir apps com IA\n",
    "- **Ollama** ‚Üí Roda modelos de IA localmente (gr√°tis!)\n",
    "- **FAISS** ‚Üí Banco de dados vetorial ultrarr√°pido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b34f0e4",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Ferramentas que Vamos Usar\n",
    "\n",
    "### 1. LangChain ü¶úüîó\n",
    "**O que √©:** Framework Python que simplifica a cria√ß√£o de aplica√ß√µes com IA  \n",
    "**Por que usar:** Em vez de escrever centenas de linhas de c√≥digo, usamos algumas linhas  \n",
    "**Analogia:** √â como usar um framework web (Django/Flask) em vez de programar HTTP do zero\n",
    "\n",
    "### 2. Ollama ü¶ô\n",
    "**O que √©:** Plataforma para rodar modelos de IA **localmente** no seu computador  \n",
    "**Por que usar:** \n",
    "- ‚úÖ Gratuito (sem pagar por API)\n",
    "- ‚úÖ Privado (seus dados n√£o v√£o para a nuvem)\n",
    "- ‚úÖ R√°pido (sem lat√™ncia de internet)\n",
    "\n",
    "**Modelos de embedding dispon√≠veis:**\n",
    "- `all-minilm`: 384 dimens√µes, r√°pido e leve\n",
    "- `nomic-embed-text`: 768 dimens√µes, balanceado\n",
    "- `mxbai-embed-large`: 1024 dimens√µes, mais preciso\n",
    "- `embeddinggemma`: 768 dimens√µes, otimizado para compreens√£o de texto em portugu√™s\n",
    "\n",
    "### 3. FAISS üöÄ\n",
    "**O que √©:** Biblioteca do Facebook/Meta para busca vetorial eficiente  \n",
    "**Por que usar:** Consegue buscar em **milh√µes de vetores** em milissegundos  \n",
    "**Analogia:** √â como um √≠ndice de banco de dados, mas para vetores matem√°ticos\n",
    "\n",
    "**Como funciona:**\n",
    "```text\n",
    "Texto ‚Üí [0.2, -0.5, 0.8, ...] ‚Üê Vetor com 384/768/1024 n√∫meros\n",
    "         ‚Üì\n",
    "      Armazenado no FAISS\n",
    "         ‚Üì\n",
    "Query ‚Üí [0.3, -0.4, 0.7, ...] ‚Üê Tamb√©m vira vetor\n",
    "         ‚Üì\n",
    "      FAISS compara dist√¢ncias\n",
    "         ‚Üì\n",
    "    Retorna os mais pr√≥ximos!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618018e7",
   "metadata": {},
   "source": [
    "## üéØ Cen√°rio do Experimento\n",
    "\n",
    "Vamos criar um **\"mini Google\"** com documentos de 3 categorias completamente diferentes:\n",
    "\n",
    "| Categoria | Documentos |\n",
    "|-----------|------------|\n",
    "| üñ•Ô∏è **Tecnologia** | iPhone 15, RTX 4090 |\n",
    "| üç∞ **Culin√°ria** | Receita de bolo, Lasanha |\n",
    "| ‚öΩ **Esportes** | Gol de futebol |\n",
    "\n",
    "### O Desafio\n",
    "\n",
    "Faremos uma pergunta que **n√£o cont√©m palavras exatas** dos documentos:\n",
    "\n",
    "**Pergunta:** \"Quero sugest√µes de hardware para computador ou celular\"\n",
    "\n",
    "**Expectativa:**\n",
    "- ‚úÖ Deve retornar: iPhone e RTX 4090 (s√£o hardware!)\n",
    "- ‚ùå N√ÉO deve retornar: Bolo, Lasanha, Gol (n√£o s√£o hardware)\n",
    "\n",
    "### Por que isso √© dif√≠cil?\n",
    "\n",
    "Um sistema de busca tradicional (Ctrl+F) falharia porque:\n",
    "- A palavra \"iPhone\" n√£o aparece na pergunta\n",
    "- A palavra \"RTX 4090\" n√£o aparece na pergunta\n",
    "- Mas nossa IA precisa **entender** que ambos s√£o hardware!\n",
    "\n",
    "üí° **Isso √© Intelig√™ncia Artificial em a√ß√£o!** O modelo aprendeu que \"iPhone = celular = hardware\" e \"RTX 4090 = placa de v√≠deo = hardware\" durante seu treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e005c1dc",
   "metadata": {},
   "source": [
    "## üîå Imports e Configura√ß√£o Inicial\n",
    "\n",
    "Importando as bibliotecas que vamos usar:\n",
    "\n",
    "- `os`: Para ler vari√°veis de ambiente\n",
    "- `requests`: Para verificar se o Ollama est√° rodando\n",
    "- `Path` e `load_dotenv`: Para carregar configura√ß√µes do arquivo `.env`\n",
    "- `FAISS`: O banco de dados vetorial\n",
    "- `OllamaEmbeddings`: O modelo que transforma texto em vetores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5da2c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Se preferir usar Ollama Embeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d96b7b2",
   "metadata": {},
   "source": [
    "## üîê Carregando Vari√°veis de Ambiente\n",
    "\n",
    "O arquivo `.env` cont√©m configura√ß√µes como:\n",
    "- URL do Ollama (`OLLAMA_API_URL`)\n",
    "- Chaves de API (se necess√°rio)\n",
    "\n",
    "**Por que usar `.env`?**\n",
    "- ‚úÖ N√£o expor senhas no c√≥digo\n",
    "- ‚úÖ F√°cil mudar entre ambientes (dev/prod)\n",
    "- ‚úÖ Compartilhar c√≥digo sem vazar credenciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf197ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé .env carregado -> E:\\01-projetos\\11-work\\11.34-engenharia-vetorial\\.env\n"
     ]
    }
   ],
   "source": [
    "# 2) Configura√ß√£o e carregamento do .env (simplificado)\n",
    "env_path = Path.cwd().joinpath('..', '..', '.env').resolve()\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f'üîé .env carregado -> {env_path.resolve()}')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  .env n√£o encontrado. Defina as vari√°veis de ambiente manualmente.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0113a6",
   "metadata": {},
   "source": [
    "## üè• Health Check do Ollama\n",
    "\n",
    "Antes de usar o Ollama, precisamos verificar se ele est√° rodando corretamente.\n",
    "\n",
    "**O que esta c√©lula faz:**\n",
    "1. Tenta conectar na URL do Ollama (padr√£o: `http://localhost:11434`)\n",
    "2. Lista todos os modelos dispon√≠veis\n",
    "3. Mostra o tamanho de cada modelo em GB\n",
    "\n",
    "**Poss√≠veis URLs:**\n",
    "- `http://localhost:11434` ‚Üí Se Ollama est√° rodando no seu computador\n",
    "- `http://ollama:11434` ‚Üí Se Ollama est√° rodando em Docker Compose\n",
    "\n",
    "üí° **Dica:** Se voc√™ ver ‚ùå, o Ollama n√£o est√° rodando. Inicie-o com `ollama serve` no terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc61b89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama est√° online! Modelos dispon√≠veis: 6\n",
      "   - embeddinggemma:latest (0.58 GB)\n",
      "   - gpt-oss:latest (12.85 GB)\n",
      "   - llama3.2:1b (1.23 GB)\n",
      "   - all-minilm:latest (0.04 GB)\n",
      "   - mxbai-embed-large:latest (0.62 GB)\n",
      "   - nomic-embed-text:latest (0.26 GB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configura√ß√£o do Ollama (ajuste conforme necess√°rio)\n",
    "# - Dentro do Docker: http://ollama:11434\n",
    "# - Fora do Docker: http://localhost:11434\n",
    "# OLLAMA_API_URL = os.getenv('OLLAMA_API_URL', 'http://localhost:11434')\n",
    "OLLAMA_API_URL = 'http://localhost:11434'\n",
    "\n",
    "# Verificar se Ollama est√° online\n",
    "def check_ollama_health() -> bool:\n",
    "    try:\n",
    "        response = requests.get(f'{OLLAMA_API_URL}/api/tags', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            print(f'‚úÖ Ollama est√° online! Modelos dispon√≠veis: {len(models)}')\n",
    "            for model in models:\n",
    "                name = model.get('name', 'unknown')\n",
    "                size = model.get('size', 0) / (1024**3)  # Convert to GB\n",
    "                print(f'   - {name} ({size:.2f} GB)')\n",
    "            return True\n",
    "        else:\n",
    "            print(f'‚ùå Ollama retornou status {response.status_code}')\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Erro ao conectar com Ollama: {e}')\n",
    "        return False\n",
    "\n",
    "# Testar conex√£o\n",
    "check_ollama_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbaf26e",
   "metadata": {},
   "source": [
    "## üìÑ Passo 1: Preparando os Documentos\n",
    "\n",
    "Aqui temos nosso **dataset de teste** ‚Äî 5 documentos de categorias bem distintas.\n",
    "\n",
    "**Por que categorias diferentes?**\n",
    "Queremos testar se a IA consegue distinguir contextos. Um bom modelo de embedding deve:\n",
    "- Colocar \"iPhone\" e \"RTX 4090\" pr√≥ximos (ambos s√£o tecnologia/hardware)\n",
    "- Colocar \"bolo\" e \"lasanha\" pr√≥ximos (ambos s√£o culin√°ria)\n",
    "- Manter \"gol\" distante de receitas e hardware (esporte √© outro contexto)\n",
    "\n",
    "### Estrutura dos Dados\n",
    "\n",
    "```python\n",
    "meus_textos = [\n",
    "    \"Documento sobre tecnologia...\",  # Categoria: Tech\n",
    "    \"Documento sobre culin√°ria...\",   # Categoria: Food\n",
    "    \"Documento sobre esporte...\",     # Categoria: Sport\n",
    "]\n",
    "```\n",
    "\n",
    "üí° **Conceito importante:** Esses textos s√£o chamados de **corpus** (conjunto de documentos que queremos buscar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da63cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meus_textos = [\n",
    "    \"O novo iPhone 15 tem uma lente perisc√≥pica incr√≠vel.\",    # Tecnologia\n",
    "    \"Para fazer um bolo macio, bata as claras em neve.\",       # Culin√°ria\n",
    "    \"O atacante chutou a bola no √¢ngulo e foi gol.\",           # Esporte\n",
    "    \"A placa de v√≠deo RTX 4090 roda jogos em 4K.\",             # Tecnologia\n",
    "    \"Receita de lasanha √† bolonhesa com muito queijo.\"         # Culin√°ria\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aecab4d",
   "metadata": {},
   "source": [
    "## üß† Passo 2: Inicializando o Modelo de Embeddings\n",
    "\n",
    "Este √© o **\"c√©rebro\"** que transforma texto em vetores num√©ricos!\n",
    "\n",
    "### O que s√£o Embeddings?\n",
    "\n",
    "**Embedding** = Representa√ß√£o num√©rica de um texto\n",
    "\n",
    "Exemplo:\n",
    "```text\n",
    "Texto: \"gato\"\n",
    "Embedding: [0.12, -0.34, 0.56, 0.89, ..., 0.23]\n",
    "           ‚Üë vetor com 384 n√∫meros (all-minilm)\n",
    "```\n",
    "\n",
    "### Como funciona a magia?\n",
    "\n",
    "1. O modelo foi **treinado** em milh√µes de textos\n",
    "2. Aprendeu que \"gato\" e \"felino\" t√™m significados parecidos\n",
    "3. Ent√£o gera vetores **pr√≥ximos** para palavras com significados similares\n",
    "\n",
    "**Visualiza√ß√£o conceitual:**\n",
    "```text\n",
    "Espa√ßo vetorial (imagine em 3D, mas √© 384D!)\n",
    "\n",
    "  gato ‚Ä¢ ‚Üê pr√≥ximo de ‚Üí ‚Ä¢ felino\n",
    "  \n",
    "  carro ‚Ä¢ ‚Üê DISTANTE de ‚Üí ‚Ä¢ felino\n",
    "```\n",
    "\n",
    "üí° **Nota:** Todos esses modelos rodam **localmente** e s√£o **gratuitos**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30f41cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se preferir usar Ollama Embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    # model=\"mxbai-embed-large\",\n",
    "    # model=\"nomic-embed-text\",\n",
    "    model=\"all-minilm\",\n",
    "    base_url=OLLAMA_API_URL,\n",
    "    # Em vez de headers, use client kwargs (se necess√°rio para auth)\n",
    "    client_kwargs={\"headers\": {\"Authorization\": \"Bearer YOUR_TOKEN\"}} if os.getenv(\"OLLAMA_API_KEY\") else {},\n",
    "    # opcional: validar se o modelo existe localmente\n",
    "    validate_model_on_init=False,\n",
    "    # opcional: tempo de keep-alive\n",
    "    keep_alive=5 * 60,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce7ba2f",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Passo 3: Criando o Banco de Dados Vetorial (Indexa√ß√£o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "231e1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_texts(meus_textos, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846679db",
   "metadata": {},
   "source": [
    "Configura√ß√£o conclu√≠da! ‚úÖ  \n",
    "Agora vamos usar nosso sistema de busca sem√¢ntica.\n",
    "\n",
    "**O que fizemos at√© agora:**\n",
    "1. ‚úÖ Instalamos as bibliotecas\n",
    "2. ‚úÖ Conectamos ao Ollama\n",
    "3. ‚úÖ Criamos um modelo de embeddings\n",
    "4. ‚úÖ Indexamos nossos documentos no FAISS\n",
    "\n",
    "**Pr√≥ximo passo:** Fazer perguntas e ver a IA encontrar as respostas! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910e95e",
   "metadata": {},
   "source": [
    "## üîç Passo 4: Definindo a Pergunta (Query)\n",
    "\n",
    "Aqui est√° o **teste de fogo** para nosso sistema!\n",
    "\n",
    "### A Pergunta\n",
    "\n",
    "```python\n",
    "\"Quero sugest√µes de hardware para computador ou celular\"\n",
    "```\n",
    "\n",
    "### Por que esta pergunta √© desafiadora?\n",
    "\n",
    "**Palavras que N√ÉO aparecem na pergunta:**\n",
    "- ‚ùå \"iPhone\"\n",
    "- ‚ùå \"RTX\"\n",
    "- ‚ùå \"4090\"\n",
    "- ‚ùå \"placa de v√≠deo\"\n",
    "- ‚ùå \"lente perisc√≥pica\"\n",
    "\n",
    "**Palavras que SIM aparecem:**\n",
    "- ‚úÖ \"hardware\"\n",
    "- ‚úÖ \"computador\"\n",
    "- ‚úÖ \"celular\"\n",
    "\n",
    "### O Desafio\n",
    "\n",
    "Um sistema de busca tradicional (Ctrl+F) **falharia** porque:\n",
    "```text\n",
    "Busca por \"hardware\" ‚Üí N√£o encontra nada (palavra n√£o est√° nos docs)\n",
    "Busca por \"celular\" ‚Üí N√£o encontra \"iPhone\" (palavra diferente)\n",
    "Busca por \"computador\" ‚Üí N√£o encontra \"RTX 4090\" (palavra diferente)\n",
    "```\n",
    "\n",
    "### A Intelig√™ncia Artificial\n",
    "\n",
    "Nossa IA vai **entender** que:\n",
    "```text\n",
    "\"celular\" ‚âà \"iPhone\" ‚âà \"smartphone\"\n",
    "\"computador\" ‚âà \"PC\" ‚âà \"placa de v√≠deo\"\n",
    "\"hardware\" ‚âà \"equipamento\" ‚âà \"dispositivo\"\n",
    "```\n",
    "\n",
    "üí° **Isso √© aprendizado sem√¢ntico!** O modelo foi treinado para entender rela√ß√µes entre conceitos, n√£o apenas correspond√™ncia exata de strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "946d3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Quero sugest√µes de hardware para computador ou celular\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb8374",
   "metadata": {},
   "source": [
    "## üéØ Passo 5: Executando a Busca Sem√¢ntica\n",
    "\n",
    "Agora vamos **executar a busca** e ver os resultados!\n",
    "\n",
    "### O que esperar?\n",
    "\n",
    "‚úÖ **Esperado:** iPhone e RTX 4090 (hardware!)  \n",
    "‚ùå **N√£o esperado:** Bolo, Lasanha, Gol (n√£o s√£o hardware)\n",
    "\n",
    "Vamos ver se a IA acerta! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2110a5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: 'Quero sugest√µes de hardware para computador ou celular'\n",
      "\n",
      "--- Documentos Encontrados ---\n",
      "1. A placa de v√≠deo RTX 4090 roda jogos em 4K.\n",
      "2. Receita de lasanha √† bolonhesa com muito queijo.\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"Pergunta: '{query}'\\n\")\n",
    "print(\"--- Documentos Encontrados ---\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"{i+1}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac762e53",
   "metadata": {},
   "source": [
    "## üßæ Interpretando os Scores (L2 vs Cosseno)\n",
    "\n",
    "√â comum ficar em d√∫vida sobre o que o `score` retornado por `similarity_search_with_score` significa. Depende do √≠ndice usado no FAISS:\n",
    "\n",
    "- Se o √≠ndice utiliza **dist√¢ncia L2** (ex.: `IndexFlatL2`): **score = dist√¢ncia** ‚Üí *menor √© melhor*\n",
    "- Se o √≠ndice utiliza **Inner Product** com vetores normalizados (equivalente √† similaridade cosseno): **score = similaridade** ‚Üí *maior √© melhor*\n",
    "\n",
    "Abaixo h√° um exemplo execut√°vel para inspecionar scores no seu `vector_store`. Rode-o no seu ambiente e veja como interpretar os n√∫meros retornados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdc191b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: 'Quero sugest√µes de hardware para computador ou celular'\n",
      "--- Documentos com score ---\n",
      "Score: 1.211371 | Texto: A placa de v√≠deo RTX 4090 roda jogos em 4K.\n",
      "Score: 1.428187 | Texto: Receita de lasanha √† bolonhesa com muito queijo.\n",
      "Interpreta√ß√£o: se o √≠ndice for L2 -> menor melhor; se for inner-product/cosseno -> maior melhor.\n"
     ]
    }
   ],
   "source": [
    "# Exemplo: imprimir scores para a mesma `query` usada acima\n",
    "resultados = vector_store.similarity_search_with_score(query, k=2)\n",
    "print(f\"Pergunta: '{query}'\\n--- Documentos com score ---\")\n",
    "for doc, score in resultados:\n",
    "    print(f\"Score: {score:.6f} | Texto: {doc.page_content}\")\n",
    "\n",
    "print(\"Interpreta√ß√£o: se o √≠ndice for L2 -> menor melhor; se for inner-product/cosseno -> maior melhor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94620ce4",
   "metadata": {},
   "source": [
    "## üìä An√°lise dos Resultados\n",
    "\n",
    "### O que observar?\n",
    "\n",
    "Quando voc√™ executar a c√©lula acima, observe:\n",
    "\n",
    "1. **Os documentos retornados s√£o relevantes?**\n",
    "   - ‚úÖ Se retornou iPhone e RTX 4090: O modelo entendeu!\n",
    "   - ‚ùå Se retornou receitas ou gol: O modelo falhou\n",
    "\n",
    "2. **A ordem faz sentido?**\n",
    "   - O primeiro resultado deve ser o mais relevante\n",
    "   - O segundo resultado deve ser um pouco menos relevante\n",
    "\n",
    "### Por que pode falhar?\n",
    "\n",
    "Modelos diferentes podem interpretar de formas diferentes:\n",
    "- `all-minilm` √© mais r√°pido, mas menos preciso\n",
    "- `nomic-embed-text` pode ter vi√©s de treinamento\n",
    "- `mxbai-embed-large` geralmente √© mais confi√°vel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230eb46",
   "metadata": {},
   "source": [
    "\n",
    "### üß™ Experimentos para Tentar\n",
    "\n",
    "#### Experimento 1: Mudar o modelo\n",
    "```python\n",
    "# Tente trocar na c√©lula de inicializa√ß√£o:\n",
    "model=\"nomic-embed-text\"  # ou \"mxbai-embed-large\", \"embeddinggemma\" \n",
    "\n",
    "# E recriar o vector_store:\n",
    "vector_store = FAISS.from_texts(meus_textos, embeddings)\n",
    "\n",
    "# E refazer a busca\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb25768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905344b",
   "metadata": {},
   "source": [
    "#### Experimento 2: Testar outras queries\n",
    "```python\n",
    "# Query espec√≠fica\n",
    "query = \"receitas de massas italianas\"\n",
    "# Deve retornar: lasanha\n",
    "\n",
    "# Query amb√≠gua\n",
    "query = \"como melhorar performance\"\n",
    "# Vai retornar hardware ou esporte? ü§î\n",
    "\n",
    "# Query fora do dom√≠nio\n",
    "query = \"viagens para a Europa\"\n",
    "# Vai retornar o que est√° \"menos distante\" (mas nada relevante)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd3972",
   "metadata": {},
   "source": [
    "\n",
    "#### Experimento 3: Ajustar k\n",
    "\n",
    "```python\n",
    "# Ver mais resultados\n",
    "resultados = vector_store.similarity_search(query, k=5)\n",
    "# Agora voc√™ v√™ TODOS os 5 documentos ordenados por relev√¢ncia\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ff63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b499929",
   "metadata": {},
   "source": [
    "\n",
    "#### Experimento 4: Ver os scores\n",
    "```python\n",
    "# Usar similarity_search_with_score para ver as dist√¢ncias/similaridades\n",
    "resultados = vector_store.similarity_search_with_score(query, k=2)\n",
    "for doc, score in resultados:\n",
    "    print(f\"Score: {score:.6f} | Texto: {doc.page_content}\")\n",
    "\n",
    "# Interpreta√ß√£o: depende do √≠ndice ‚Äî se for L2 (dist√¢ncia) menor = melhor; se for inner-product/cosseno maior = melhor\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a316ba",
   "metadata": {},
   "source": [
    "\n",
    "### üí° Conceitos-chave para lembrar\n",
    "\n",
    "1. **Embedding transforma texto em vetor** (lista de n√∫meros)\n",
    "2. **FAISS armazena vetores** e faz buscas r√°pidas\n",
    "3. **Similaridade = proximidade no espa√ßo vetorial**\n",
    "4. **Modelos diferentes = interpreta√ß√µes diferentes**\n",
    "5. **Sempre teste com dados do SEU dom√≠nio!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "11-34-engenharia-vetorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
