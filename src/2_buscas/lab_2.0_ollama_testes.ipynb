{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Lab 2.0: Embeddings com Ollama (Local)\n",
    "\n",
    "**Objetivo**: Entender como gerar embeddings localmente usando Ollama.\n",
    "\n",
    "**Comparativo**:\n",
    "- âŒ **Antes (Lab 1.2)**: DependÃªncia de APIs externas (OpenAI, Google)\n",
    "- âœ… **Agora (Lab 2.0)**: Modelo local com Ollama, sem custos de API, controle total\n",
    "\n",
    "**Hardware recomendado**:\n",
    "- GPU com 4GB+ VRAM (NVIDIA ideal para melhor performance)\n",
    "- CPU fallback: Funciona perfeitamente, apenas mais lento\n",
    "\n",
    "**Stack Docker Compose:**\n",
    "- Ollama na porta 11434\n",
    "- Modelos disponÃ­veis: nomic-embed-text, mxbai-embed-large, all-minilm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Setup e ConfiguraÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports principais OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Carregamento de variÃ¡veis de ambiente\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Para requisiÃ§Ãµes HTTP (Ollama Ã© OpenAI-compatible)\n",
    "import requests\n",
    "from requests.exceptions import RequestException, ConnectionError, Timeout\n",
    "\n",
    "# Logging\n",
    "from loguru import logger\n",
    "import logging\n",
    "\n",
    "# Configurar logger\n",
    "logger.remove()\n",
    "logger.add(lambda msg: print(msg, end=''), format=\"<level>{level: <8}</level> | {message}\")\n",
    "\n",
    "print(\"âœ… Imports principais OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_env",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  | âš ï¸  .env nÃ£o encontrado em: /home/jovyan/.env\n",
      "INFO     | Ollama URL: http://ollama:11434\n",
      "INFO     | Modelo padrÃ£o: nomic-embed-text\n",
      "INFO     | Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Carregar variÃ¡veis de ambiente\n",
    "env_path = Path('/home/jovyan').joinpath('.env').resolve()\n",
    "if not env_path.exists():\n",
    "    env_path = Path.cwd().joinpath('.env').resolve()\n",
    "\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    logger.info(f\"âœ… Carregou .env de: {env_path}\")\n",
    "else:\n",
    "    logger.warning(f\"âš ï¸  .env nÃ£o encontrado em: {env_path}\")\n",
    "\n",
    "# VariÃ¡veis de configuraÃ§Ã£o\n",
    "OLLAMA_API_URL = os.getenv('OLLAMA_API_URL', 'http://ollama:11434')\n",
    "OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'nomic-embed-text')\n",
    "DATA_PATH = Path(os.getenv('DATA_PATH', '/home/jovyan/work/data'))\n",
    "BATCH_SIZE = int(os.getenv('BATCH_SIZE', 32))\n",
    "\n",
    "logger.info(f\"Ollama URL: {OLLAMA_API_URL}\")\n",
    "logger.info(f\"Modelo padrÃ£o: {OLLAMA_MODEL}\")\n",
    "logger.info(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Criar diretÃ³rio de dados se nÃ£o existir\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "health_check",
   "metadata": {},
   "source": [
    "## 2. Health Check: Verificar ConexÃ£o com Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "check_ollama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     | ğŸ”„ Verificando Ollama em http://ollama:11434...\n",
      "SUCCESS  | âœ… Ollama estÃ¡ online! Modelos disponÃ­veis: 3\n",
      "INFO     |    â€¢ all-minilm:latest (23M)\n",
      "INFO     |    â€¢ mxbai-embed-large:latest (334M)\n",
      "INFO     |    â€¢ nomic-embed-text:latest (137M)\n"
     ]
    }
   ],
   "source": [
    "def health_check_ollama(api_url: str, timeout: int = 5) -> bool:\n",
    "    \"\"\"\n",
    "    Verifica se Ollama estÃ¡ rodando e acessÃ­vel.\n",
    "    \n",
    "    Tenta 3 vezes com backoff exponencial antes de desistir.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"ğŸ”„ Verificando Ollama em {api_url}...\")\n",
    "        \n",
    "        for tentativa in range(1, 4):\n",
    "            try:\n",
    "                response = requests.get(\n",
    "                    f\"{api_url}/api/tags\",\n",
    "                    timeout=timeout\n",
    "                )\n",
    "                if response.status_code == 200:\n",
    "                    models = response.json()\n",
    "                    model_count = len(models.get('models', []))\n",
    "                    logger.success(f\"âœ… Ollama estÃ¡ online! Modelos disponÃ­veis: {model_count}\")\n",
    "                    if model_count > 0:\n",
    "                        for model in models['models']:\n",
    "                            logger.info(f\"   â€¢ {model['name']} ({model['details']['parameter_size']})\")\n",
    "                    return True\n",
    "            except (ConnectionError, Timeout) as e:\n",
    "                if tentativa < 3:\n",
    "                    espera = 2 ** tentativa  # backoff: 2, 4, 8 segundos\n",
    "                    logger.warning(f\"âŒ Tentativa {tentativa}/3 falhou. Esperando {espera}s...\")\n",
    "                    time.sleep(espera)\n",
    "                else:\n",
    "                    logger.error(f\"âŒ Ollama nÃ£o acessÃ­vel apÃ³s 3 tentativas\")\n",
    "                    return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Erro ao verificar Ollama: {e}\")\n",
    "        return False\n",
    "\n",
    "# Executar health check\n",
    "ollama_online = health_check_ollama(OLLAMA_API_URL)\n",
    "\n",
    "if not ollama_online:\n",
    "    logger.error(f\"\"\"\n",
    "    âŒ Ollama nÃ£o estÃ¡ acessÃ­vel!\n",
    "    \n",
    "    Para iniciar Ollama via Docker Compose:\n",
    "    1. docker-compose up -d\n",
    "    2. Verifique a URL: {OLLAMA_API_URL}\n",
    "    3. Baixe modelos: docker exec ollama-embeddings ollama pull nomic-embed-text\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding_function",
   "metadata": {},
   "source": [
    "## 3. FunÃ§Ã£o de Embeddings com Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "embed_function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS  | âœ… OllamaEmbedder instanciado\n"
     ]
    }
   ],
   "source": [
    "class OllamaEmbedder:\n",
    "    \"\"\"\n",
    "    Wrapper para gerar embeddings via Ollama.\n",
    "    \n",
    "    CaracterÃ­sticas:\n",
    "    - Usa API nativa do Ollama\n",
    "    - Suporta mÃºltiplos modelos (nomic-embed-text, mxbai-embed-large, all-minilm)\n",
    "    - Suporta batch processing\n",
    "    - Retorna embedding + metadata (dimensionalidade, tempo)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        api_url: str,\n",
    "        model: str = 'nomic-embed-text',\n",
    "        timeout: int = 30,\n",
    "        batch_size: int = 32\n",
    "    ):\n",
    "        self.api_url = api_url\n",
    "        self.model = model\n",
    "        self.timeout = timeout\n",
    "        self.batch_size = batch_size\n",
    "        self.total_tokens = 0\n",
    "        \n",
    "    def embed_query(self, text: str, return_metadata: bool = False) -> Tuple[List[float], Optional[Dict]]:\n",
    "        \"\"\"\n",
    "        Gera embedding para um texto Ãºnico.\n",
    "        \n",
    "        Args:\n",
    "            text: Texto para embeddar\n",
    "            return_metadata: Retornar info de latÃªncia e dimensÃ£o\n",
    "            \n",
    "        Returns:\n",
    "            (embedding, metadata) onde metadata Ã© {'dim': int, 'latency_ms': float}\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            payload = {\n",
    "                'model': self.model,\n",
    "                'input': text\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{self.api_url}/api/embed\",\n",
    "                json=payload,\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                logger.error(f\"API error: {response.text}\")\n",
    "                return None, None\n",
    "            \n",
    "            data = response.json()\n",
    "            embedding = data['embeddings'][0]\n",
    "            \n",
    "            # Metadata\n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            metadata = {\n",
    "                'dim': len(embedding),\n",
    "                'latency_ms': latency_ms,\n",
    "                'model': self.model\n",
    "            }\n",
    "            \n",
    "            if return_metadata:\n",
    "                return embedding, metadata\n",
    "            return embedding, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro ao gerar embedding: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def embed_batch(self, texts: List[str], return_metadata: bool = False) -> Tuple[List[List[float]], Optional[List[Dict]]]:\n",
    "        \"\"\"\n",
    "        Gera embeddings para mÃºltiplos textos (com batch processing).\n",
    "        \n",
    "        Args:\n",
    "            texts: Lista de textos\n",
    "            return_metadata: Retornar info de latÃªncia\n",
    "            \n",
    "        Returns:\n",
    "            ([embeddings], [metadata])\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        metadata_list = []\n",
    "        \n",
    "        total_batches = (len(texts) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch = texts[i:i + self.batch_size]\n",
    "            batch_num = (i // self.batch_size) + 1\n",
    "            \n",
    "            logger.info(f\"ğŸ”„ Processando batch {batch_num}/{total_batches}...\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Ollama nÃ£o suporta batch nativamente, processa um por vez\n",
    "                for text in batch:\n",
    "                    payload = {\n",
    "                        'model': self.model,\n",
    "                        'input': text\n",
    "                    }\n",
    "                    \n",
    "                    response = requests.post(\n",
    "                        f\"{self.api_url}/api/embed\",\n",
    "                        json=payload,\n",
    "                        timeout=self.timeout\n",
    "                    )\n",
    "                    \n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        embeddings.append(data['embeddings'][0])\n",
    "                    else:\n",
    "                        logger.error(f\"Erro no batch {batch_num}: {response.text}\")\n",
    "                \n",
    "                latency_ms = (time.time() - start_time) * 1000\n",
    "                metadata_list.append({\n",
    "                    'batch_num': batch_num,\n",
    "                    'texts_in_batch': len(batch),\n",
    "                    'latency_ms': latency_ms,\n",
    "                    'avg_latency_per_text': latency_ms / len(batch)\n",
    "                })\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Erro ao processar batch {batch_num}: {e}\")\n",
    "        \n",
    "        if return_metadata:\n",
    "            return embeddings, metadata_list\n",
    "        return embeddings, None\n",
    "\n",
    "embedder = OllamaEmbedder(\n",
    "    api_url=OLLAMA_API_URL,\n",
    "    model=OLLAMA_MODEL,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "logger.success(\"âœ… OllamaEmbedder instanciado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_embedding",
   "metadata": {},
   "source": [
    "## 4. Teste: Gerar um Embedding Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "test_single",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     | ğŸ§ª Gerando embeddings de teste...\n",
      "SUCCESS  | âœ… Texto 1: O gato Ã© um animal domÃ©stico...\n",
      "   DimensÃ£o: 768 | LatÃªncia: 2134.34ms\n",
      "SUCCESS  | âœ… Texto 1: O gato Ã© um animal domÃ©stico...\n",
      "   DimensÃ£o: 768 | LatÃªncia: 2134.34ms\n",
      "SUCCESS  | âœ… Texto 2: O gato Ã© um felino de estimaÃ§Ã£o...\n",
      "   DimensÃ£o: 768 | LatÃªncia: 4343.85ms\n",
      "SUCCESS  | âœ… Texto 2: O gato Ã© um felino de estimaÃ§Ã£o...\n",
      "   DimensÃ£o: 768 | LatÃªncia: 4343.85ms\n",
      "SUCCESS  | âœ… Texto 3: A programaÃ§Ã£o Ã© importante para engenhei...\n",
      "   DimensÃ£o: 768 | LatÃªncia: 1436.15ms\n",
      "SUCCESS  | \n",
      "âœ… Todos os 3 embeddings gerados com sucesso!\n",
      "SUCCESS  | âœ… Texto 3: A programaÃ§Ã£o Ã© importante para engenhei...\n",
      "   DimensÃ£o: 768 | LatÃªncia: 1436.15ms\n",
      "SUCCESS  | \n",
      "âœ… Todos os 3 embeddings gerados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Teste com textos de exemplo (mesmos do Lab 1.2!)\n",
    "test_texts = [\n",
    "    'O gato Ã© um animal domÃ©stico',\n",
    "    'O gato Ã© um felino de estimaÃ§Ã£o',\n",
    "    'A programaÃ§Ã£o Ã© importante para engenheiros de software'\n",
    "]\n",
    "\n",
    "logger.info(\"ğŸ§ª Gerando embeddings de teste...\")\n",
    "\n",
    "embeddings = []\n",
    "for idx, text in enumerate(test_texts, 1):\n",
    "    emb, meta = embedder.embed_query(text, return_metadata=True)\n",
    "    if emb:\n",
    "        embeddings.append(emb)\n",
    "        logger.success(\n",
    "            f\"âœ… Texto {idx}: {text[:40]}...\\n\"\n",
    "            f\"   DimensÃ£o: {meta['dim']} | LatÃªncia: {meta['latency_ms']:.2f}ms\"\n",
    "        )\n",
    "    else:\n",
    "        logger.error(f\"âŒ Falha ao processar texto {idx}\")\n",
    "\n",
    "if len(embeddings) == len(test_texts):\n",
    "    logger.success(f\"\\nâœ… Todos os {len(embeddings)} embeddings gerados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similarity_calc",
   "metadata": {},
   "source": [
    "## 5. Verificar Similaridade (igual ao Lab 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cosine_sim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     | \n",
      "    ğŸ“Š Similaridades de Cosseno (Ollama Local):\n",
      "    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "    Texto 1 vs Texto 2 (esperado: alto): 0.8278\n",
      "    Texto 1 vs Texto 3 (esperado: baixo): 0.5894\n",
      "    DiferenÃ§a:                            0.2384\n",
      "    \n",
      "SUCCESS  | âœ… Modelo conseguiu distinguir conceitos!\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(v1: List[float], v2: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calcula similaridade de cosseno entre dois vetores.\n",
    "    \"\"\"\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "\n",
    "# Calcular similaridades\n",
    "if len(embeddings) >= 3:\n",
    "    sim_1_2 = cosine_similarity(embeddings[0], embeddings[1])\n",
    "    sim_1_3 = cosine_similarity(embeddings[0], embeddings[2])\n",
    "    \n",
    "    logger.info(\n",
    "        f\"\"\"\n",
    "    ğŸ“Š Similaridades de Cosseno (Ollama Local):\n",
    "    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "    Texto 1 vs Texto 2 (esperado: alto): {sim_1_2:.4f}\n",
    "    Texto 1 vs Texto 3 (esperado: baixo): {sim_1_3:.4f}\n",
    "    DiferenÃ§a:                            {(sim_1_2 - sim_1_3):.4f}\n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    if sim_1_2 > sim_1_3:\n",
    "        logger.success(\"âœ… Modelo conseguiu distinguir conceitos!\")\n",
    "    else:\n",
    "        logger.warning(\"âš ï¸  Similaridades nÃ£o diferenciam bem os conceitos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 6. Resumo: Lab 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "summary_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "    â•‘           LAB 2.0: EMBEDDINGS COM OLLAMA                    â•‘\n",
      "    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "    â•‘                                                            â•‘\n",
      "    â•‘ âœ… ENTREGÃVEIS NESTE LAB:                                 â•‘\n",
      "    â•‘                                                            â•‘\n",
      "    â•‘ 1. ConexÃ£o com Ollama (Docker Compose)                    â•‘\n",
      "    â•‘ 2. Classe OllamaEmbedder (single + batch processing)      â•‘\n",
      "    â•‘ 3. GeraÃ§Ã£o de embeddings 100% LOCAL (sem APIs remotas)    â•‘\n",
      "    â•‘ 4. CÃ¡lculo de latÃªncia e dimensionalidade                 â•‘\n",
      "    â•‘ 5. ComparaÃ§Ã£o semÃ¢ntica (cosine similarity)               â•‘\n",
      "    â•‘ 6. Suporte a GPU NVIDIA e CPU                              â•‘\n",
      "    â•‘                                                            â•‘\n",
      "    â•‘ ğŸ“Š PRÃ“XIMOS PASSOS:                                       â•‘\n",
      "    â•‘                                                            â•‘\n",
      "    â•‘ â†’ Lab 2.1: IntegraÃ§Ã£o Ollama + Qdrant                     â•‘\n",
      "    â•‘ â†’ Lab 2.2: RAG (Retrieval Augmented Generation)          â•‘\n",
      "    â•‘                                                            â•‘\n",
      "    â•‘ ğŸ’¡ PONTOS PEDAGÃ“GICOS:                                    â•‘\n",
      "    â•‘                                                            â•‘\n",
      "    â•‘ â€¢ Zero custo de API (apÃ³s setup inicial)                  â•‘\n",
      "    â•‘ â€¢ MÃºltiplos modelos disponÃ­veis (nomic, mxbai, all-minilm) â•‘\n",
      "    â•‘ â€¢ Performance excelente com GPU NVIDIA                    â•‘\n",
      "    â•‘ â€¢ Dados nÃ£o saem de casa (compliance/LGPD)                â•‘\n",
      "    â•‘ â€¢ Ollama mais moderno e ativo que LM Studio          â•‘\n",
      "    â•‘                                                            â•‘\n",
      "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "    \n",
      "SUCCESS  | \n",
      "âœ… Lab 2.0 completo! Avance para Lab 2.1\n"
     ]
    }
   ],
   "source": [
    "summary = f\"\"\"\n",
    "    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "    â•‘           LAB 2.0: EMBEDDINGS COM OLLAMA                    â•‘\n",
    "    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "    â•‘                                                            â•‘\n",
    "    â•‘ âœ… ENTREGÃVEIS NESTE LAB:                                 â•‘\n",
    "    â•‘                                                            â•‘\n",
    "    â•‘ 1. ConexÃ£o com Ollama (Docker Compose)                    â•‘\n",
    "    â•‘ 2. Classe OllamaEmbedder (single + batch processing)      â•‘\n",
    "    â•‘ 3. GeraÃ§Ã£o de embeddings 100% LOCAL (sem APIs remotas)    â•‘\n",
    "    â•‘ 4. CÃ¡lculo de latÃªncia e dimensionalidade                 â•‘\n",
    "    â•‘ 5. ComparaÃ§Ã£o semÃ¢ntica (cosine similarity)               â•‘\n",
    "    â•‘ 6. Suporte a GPU NVIDIA e CPU                              â•‘\n",
    "    â•‘                                                            â•‘\n",
    "    â•‘ ğŸ“Š PRÃ“XIMOS PASSOS:                                       â•‘\n",
    "    â•‘                                                            â•‘\n",
    "    â•‘ â†’ Lab 2.1: IntegraÃ§Ã£o Ollama + Qdrant                     â•‘\n",
    "    â•‘ â†’ Lab 2.2: RAG (Retrieval Augmented Generation)          â•‘\n",
    "    â•‘                                                            â•‘\n",
    "    â•‘ ğŸ’¡ PONTOS PEDAGÃ“GICOS:                                    â•‘\n",
    "    â•‘                                                            â•‘\n",
    "    â•‘ â€¢ Zero custo de API (apÃ³s setup inicial)                  â•‘\n",
    "    â•‘ â€¢ MÃºltiplos modelos disponÃ­veis (nomic, mxbai, all-minilm) â•‘\n",
    "    â•‘ â€¢ Performance excelente com GPU NVIDIA                    â•‘\n",
    "    â•‘ â€¢ Dados nÃ£o saem de casa (compliance/LGPD)                â•‘\n",
    "    â•‘ â€¢ Ollama mais moderno e ativo que LM Studio          â•‘\n",
    "    â•‘                                                            â•‘\n",
    "    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "    \"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "logger.success(\"\\nâœ… Lab 2.0 completo! Avance para Lab 2.1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
