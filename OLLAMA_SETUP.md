# üöÄ Setup do Ollama com Modelos Pr√©-instalados

## üì¶ Modelos Inclu√≠dos

A imagem customizada `ollama-embeddings-custom:latest` vem com os seguintes modelos de embeddings pr√©-instalados:

| Modelo | Dimens√µes | Tamanho | Uso Recomendado |
|--------|-----------|---------|-----------------|
| **all-minilm:latest** | 384 | ~23 MB | Mobile, edge, prototipagem r√°pida |
| **nomic-embed-text:latest** | 768 | ~274 MB | RAG, busca sem√¢ntica geral |
| **mxbai-embed-large:latest** | 1024 | ~670 MB | M√°xima qualidade, produ√ß√£o |

**Tamanho total dos modelos:** ~967 MB

## üî® Build da Imagem

### Primeira vez (build inicial):

```bash
# Com GPU (NVIDIA)
docker-compose build ollama

# Ou CPU-only
docker-compose -f docker-compose-cpu.yaml build ollama
```

**‚è±Ô∏è Tempo estimado de build:**
- Download da imagem base: ~2-3 min
- Download dos modelos: ~5-10 min (depende da internet)
- **Total: ~10-15 minutos**

### Verificar se a imagem foi criada:

```bash
docker images | grep ollama-embeddings-custom
```

Deve retornar:
```
ollama-embeddings-custom   latest   abc123def456   5 minutes ago   2.1GB
```

## üöÄ Iniciar Servi√ßos

### Stack completa (GPU):

```bash
docker-compose up -d
```

### Stack CPU-only:

```bash
docker-compose -f docker-compose-cpu.yaml up -d
```

### Verificar se Ollama est√° rodando:

```bash
docker-compose logs ollama
```

Deve mostrar:
```
‚úÖ Ollama est√° online!
üìã Modelos dispon√≠veis:
- all-minilm:latest
- nomic-embed-text:latest
- mxbai-embed-large:latest
```

## üß™ Testar Modelos

### Via linha de comando:

```bash
# Listar modelos instalados
docker exec ollama-embeddings ollama list

# Testar embedding com all-minilm
docker exec ollama-embeddings ollama run all-minilm "Hello world"

# Testar embedding com nomic-embed-text
docker exec ollama-embeddings ollama run nomic-embed-text "Hello world"

# Testar embedding com mxbai-embed-large
docker exec ollama-embeddings ollama run mxbai-embed-large "Hello world"
```

### Via API (curl):

```bash
# Health check
curl http://localhost:11434/api/tags

# Gerar embedding com nomic-embed-text
curl http://localhost:11434/api/embed -d '{
  "model": "nomic-embed-text",
  "input": "O gato √© um animal dom√©stico"
}'
```

### Via Python (notebook):

```python
import requests

OLLAMA_API_URL = "http://localhost:11434"

# Verificar modelos dispon√≠veis
response = requests.get(f"{OLLAMA_API_URL}/api/tags")
print(response.json())

# Gerar embedding
response = requests.post(f"{OLLAMA_API_URL}/api/embed", json={
    "model": "nomic-embed-text",
    "input": "O gato √© um animal dom√©stico"
})
embedding = response.json()["embeddings"][0]
print(f"Dimens√µes: {len(embedding)}")
print(f"Primeiros 5 valores: {embedding[:5]}")
```

## üîÑ Atualizar Modelos

Se novos modelos forem lan√ßados, voc√™ pode adicion√°-los sem rebuild:

```bash
# Acessar container
docker exec -it ollama-embeddings bash

# Dentro do container, baixar novo modelo
ollama pull <nome-do-modelo>

# Sair
exit
```

Para tornar permanente, adicione o modelo no `Dockerfile.ollama` e fa√ßa rebuild.

## üêõ Troubleshooting

### Problema: "Model not found"

**Causa:** Modelos n√£o foram baixados durante o build.

**Solu√ß√£o:**
```bash
# Rebuild for√ßado
docker-compose build --no-cache ollama
docker-compose up -d ollama
```

### Problema: Build muito lento

**Causa:** Download dos modelos (~1GB) leva tempo.

**Solu√ß√£o:** Use cache do Docker. Se j√° fez build antes, o Docker reutiliza as layers.

### Problema: Erro de mem√≥ria durante build

**Causa:** Docker sem recursos suficientes.

**Solu√ß√£o:** Aumente mem√≥ria do Docker:
- Docker Desktop ‚Üí Settings ‚Üí Resources ‚Üí Memory ‚Üí 8GB+

### Problema: Modelo n√£o responde

**Causa:** Ollama pode estar sobrecarregado ou travado.

**Solu√ß√£o:**
```bash
# Reiniciar servi√ßo
docker-compose restart ollama

# Ver logs
docker-compose logs -f ollama
```

## üìä Compara√ß√£o: Build vs Pull Manual

### Op√ß√£o 1: Build (atual)
‚úÖ Modelos prontos imediatamente  
‚úÖ Desenvolvimento/curso mais r√°pido  
‚úÖ Reprodut√≠vel  
‚ùå Build inicial demorado (~15 min)  
‚ùå Imagem maior (~2.1GB)

### Op√ß√£o 2: Pull manual (alternativa)
‚úÖ Build instant√¢neo  
‚úÖ Imagem menor (~500MB)  
‚ùå Precisa baixar modelos manualmente  
‚ùå Estudantes podem esquecer de baixar  
‚ùå Primeiro uso mais lento

**Recomenda√ß√£o:** Use a **Op√ß√£o 1 (build)** para ambiente de curso/educacional.

## üéì Uso nos Notebooks

Os notebooks do curso j√° est√£o configurados para usar estes modelos:

- `lab_1.3_comparativos_ollama.ipynb` - Compara os 3 modelos
- `lab_1.5_buscas_local.ipynb` - Usa nomic-embed-text
- `lab_1.6_buscas_local_comparativo.ipynb` - Compara todos

Basta executar as c√©lulas e os modelos estar√£o dispon√≠veis! üöÄ

## üìö Recursos

- **Ollama Docs:** https://github.com/ollama/ollama/tree/main/docs
- **Model Library:** https://ollama.ai/library
- **API Reference:** https://github.com/ollama/ollama/blob/main/docs/api.md
